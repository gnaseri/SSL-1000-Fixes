# SSL-1000-Fixes

Aimed at supporting the systematic study and refinement of **self-supervised learning workflows**, this compilation includes over 1,000 **LLM-generated** potential issues and suggested next steps. It is shared for use by researchers following their own review and judgment, and to invite expert feedback and evaluation.

**Note:** This list was generated by an LLM with human supervision but has not been fully reviewed by domain experts after generation. It is intended to spark ideas and highlight possible directions to explore. **Please critically evaluate any items you plan to experiment with, and consider consulting domain experts before incorporating them into your work.**

---


## 1. Representation Rank is Low

**Symptoms:**
- Low covariance matrix rank or eigenvalue entropy.

**Suggested Next Steps:**
- Increase the variance loss coefficient (e.g., in VICReg).
- Increase the embedding dimension (if too narrow).
- Increase batch size to capture more variation.
- Use stronger augmentations to diversify views.
- Add residual connections to encourage diversity in output.

---

## 2. Learned Kernels Are Noisy or Dead

**Symptoms:**
- Kernels look random or many are close to zero.

**Suggested Next Steps:**
- Lower the learning rate or add gradient clipping.
- Apply weight decay or L1 regularization on conv weights.
- Inspect data normalization — ensure it’s not too aggressive.
- Add initialization schemes like Kaiming or orthogonal init.

---

## 3. Embeddings Collapse or Are Too Similar

**Symptoms:**
- Cosine similarity between features too high, low variance.

**Suggested Next Steps:**
- Add or strengthen variance regularization.
- Introduce weak asymmetry (e.g., stop gradient, predictor MLP).
- Delay EMA (Exponential Moving Average) updates if using target networks.
- Increase prediction horizon to reduce trivial matching.

---

## 4. Early Overfitting or Poor Generalization

**Symptoms:**
- Training loss drops, but validation loss rises quickly.

**Suggested Next Steps:**
- Add dropout, augmentations, or weight decay.
- Check if the model is memorizing shortcuts (like pixel positions).
- Use early stopping or learning rate warmup.
- Reduce model capacity or simplify the task.

---

## 5. Feature Norms Are Highly Uneven

**Symptoms:**
- Some features dominate (e.g., 2–3 features have high norms).

**Suggested Next Steps:**
- Normalize features before loss computation.
- Strengthen covariance decorrelation loss.
- Use BatchNorm or LayerNorm in the projector or encoder.
- Consider whitening transforms in analysis.

---

## 6. Covariance Matrix is Highly Correlated

**Symptoms:**
- High off-diagonal energy, many features redundant.

**Suggested Next Steps:**
- Increase covariance loss coefficient (e.g., VICReg’s nu).
- Reduce use of overlapping augmentations that induce similarity.
- Increase depth or non-linearity in encoder/projector.

---

## 7. Eigenvalues Decay Too Quickly

**Symptoms:**
- Only top 5–10 components explain most variance.

**Suggested Next Steps:**
- Increase batch diversity or apply feature decorrelation.
- Try orthogonality constraints (e.g., QR loss or mutual information maximization).
- Apply contrastive loss with negative samples.

---

## 8. Prediction Errors Are Localized or Systematic

**Symptoms:**
- Specific regions or classes consistently fail.

**Suggested Next Steps:**
- Analyze dataset imbalance or sample quality.
- Add local receptive field diversity (e.g., dilated convolutions).
- Train with curriculum or targeted augmentation on weak regions.

---

## 9. Latent Space Visualization Shows Poor Separation

**Symptoms:**
- t-SNE or PCA shows all points clumped together.

**Suggested Next Steps:**
- Improve augmentation diversity to avoid trivial alignment.
- Add auxiliary tasks (e.g., rotation prediction or jigsaw).
- Increase embedding dimensionality or training time.

---

## 10. Gradient Norms Are Exploding or Vanishing

**Symptoms:**
- Training becomes unstable, NaNs in loss, or gradients ~0.

**Suggested Next Steps:**
- Use gradient clipping (e.g., clip value or clip norm).
- Check for ReLU dead units — try LeakyReLU or GELU.
- Switch to Adam or RMSprop with proper eps setting.
- Use smaller learning rate or mixed-precision training with GradScaler.

---

## 11. Loss Plateaus Early

**Symptoms:**
- Loss stops improving in early epochs.

**Suggested Next Steps:**
- Use a learning rate scheduler (StepLR, CosineAnnealing, etc.).
- Add warmup steps at the start.
- Increase model capacity (width or depth).
- Check for data leakage or label noise if applicable.

---

## 12. Very Low Output Variance

**Symptoms:**
- All output features are near zero or constant across samples.

**Suggested Next Steps:**
- Initialize final layer biases to small non-zero values.
- Use LayerNorm in the output head.
- Regularize output to have unit variance.
- Inspect input normalization and activation function saturation.

---

## 13. Poor Transfer to Downstream Task

**Symptoms:**
- Learned representations perform poorly on supervised tasks.

**Suggested Next Steps:**
- Introduce task-aware pretext signals.
- Add semantic consistency loss if applicable.
- Use larger datasets or pretrained backbones.
- Fine-tune with a small supervised head to guide representation.

---

## 14. Slow Training

**Symptoms:**
- Training epochs take too long.

**Suggested Next Steps:**
- Optimize data loading (e.g., increase num_workers, use .pin_memory()).
- Use mixed precision or channels_last format.
- Profile with torch.profiler or nvprof to locate bottlenecks.
- Chunk long sequences if you’re doing sequential modeling.

---

## 15. BatchNorm or LayerNorm Acting Erratically

**Symptoms:**
- Normalization hurts performance or destabilizes training.

**Suggested Next Steps:**
- Try GroupNorm or InstanceNorm instead.
- Replace with no normalization and tune learning rate carefully.
- If using DDP: ensure BatchNorm is synchronized (SyncBatchNorm).

---

## 16. Representations Are Nearly Identical Across Augmented Views

**Symptoms:**
- High cosine similarity between outputs from different views.

**Suggested Next Steps:**
- Add stochastic layers (e.g., dropout) before the projector.
- Increase augmentation intensity (e.g., stronger color jitter, masking).
- Add temporal or spatial shift if applicable (e.g., for video or sequence data).

---

## 17. Checkpoint Sizes Are Too Large

**Symptoms:**
- Disk fills up quickly or model is hard to share.

**Suggested Next Steps:**
- Save only model weights, not full optimizer states.
- Use state_dict() filtering to exclude unneeded buffers.
- Save in float16 if inference is not affected.

---

## 18. Embedding Dimensions Used Unevenly

**Symptoms:**
- Some dimensions are always close to 0, others dominate.

**Suggested Next Steps:**
- Add uniformity loss or entropy maximization on features.
- Apply decorrelation or orthogonality constraints.
- Use learned positional encoding or non-linear projectors.

---

## 19. EMA (Exponential Moving Average) Target Doesn’t Help

**Symptoms:**
- EMA-targeted representations don’t stabilize or improve.

**Suggested Next Steps:**
- Reduce the EMA decay at early epochs (e.g., start at 0.9 and anneal to 0.99).
- Re-initialize EMA with online model every few epochs (experimental).
- Try using fixed encoder instead of EMA to test isolation.

---

## 20. Visualization Metrics Don’t Change Over Epochs

**Symptoms:**
- PCA, cosine similarity, rank, or eigenvalue plots are flat.

**Suggested Next Steps:**
- Increase the training duration.
- Visualize per-class metrics or region-specific patterns.
- Log metrics after more granular intervals (e.g., every few batches).
- Inspect if loss is still changing — maybe the model is learning but not diversifying representations.

---

## 21. Feature Activations Are Sparse or Mostly Zero

**Symptoms:**
- Most features in intermediate layers are zero.

**Suggested Next Steps:**
- Replace ReLU with GELU, ELU, or Swish to allow negative flows.
- Add BatchNorm/LayerNorm before nonlinearity to stabilize activation spread.
- Lower the weight decay, which might be shrinking weights excessively.

---

## 22. Learned Representations Are Overly Sensitive to Noise

**Symptoms:**
- Small input perturbations drastically change output.

**Suggested Next Steps:**
- Add consistency loss (e.g., L2 between outputs of noisy inputs).
- Use adversarial training or noise augmentations.
- Try input denoising autoencoder or robust contrastive objectives.

---

## 23. Training Is Stochastic or Unstable Across Runs

**Symptoms:**
- Large performance variation between seeds.

**Suggested Next Steps:**
- Set all seeds (torch, numpy, random) and enable `torch.backends.cudnn.deterministic = True`.
- Use larger batch sizes or gradient accumulation.
- Tune EMA decay and optimizer momentum for stability.

---

## 24. Model Predicts Only the Majority Class or Background

**Symptoms:**
- Class imbalance in predictions.

**Suggested Next Steps:**
- Use class-balanced sampling or weighted loss functions.
- Over-sample minority classes or under-sample majority ones.
- Add focal loss or label smoothing to improve calibration.

---

## 25. Model Performs Well Quantitatively but Fails Visually

**Symptoms:**
- High accuracy but poor qualitative results (e.g., in reconstructions or predictions).

**Suggested Next Steps:**
- Add perceptual loss or structural similarity index (SSIM).
- Visualize feature maps or intermediate outputs to detect bottlenecks.
- Investigate if model is overfitting shortcut features.

---

## 26. High Feature Redundancy Across Time or Space

**Symptoms:**
- Features don’t evolve across spatial/temporal positions.

**Suggested Next Steps:**
- Use multi-scale features or dilated convolutions.
- Encourage diversity with contrastive prediction across time or space.
- Add shuffle-and-predict or jigsaw tasks to break local patterns.

---

## 27. Learned Embeddings Show Bias (e.g., class, texture, position)

**Symptoms:**
- Embeddings cluster by confounding factors.

**Suggested Next Steps:**
- Add domain adversarial training to suppress bias signals.
- Use class-conditioned augmentation or stratified sampling.
- Regularize with mutual information minimization between embeddings and bias.

---

## 28. Final Layer Activations Are Saturated

**Symptoms:**
- Activations stuck near ±1 (e.g., tanh or sigmoid saturation).

**Suggested Next Steps:**
- Use identity or linear activation in last layer for representation learning.
- Normalize final layer outputs before loss (e.g., L2 norm for cosine similarity).
- Replace sigmoid/tanh with scaled versions or avoid them altogether.

---

## 29. Latent Space Changes Are Too Abrupt Between Epochs

**Symptoms:**
- Drastic shift in embedding space across epochs.

**Suggested Next Steps:**
- Reduce learning rate or use cosine annealing.
- Stabilize training with EMA targets or slower scheduler decay.
- Visualize trajectory of embeddings to identify jump points.

---

## 30. Too Many Metrics Make it Hard to Interpret

**Symptoms:**
- Overwhelming WandB or CSV logs with unclear insight.

**Suggested Next Steps:**
- Group metrics into categories (e.g., loss/, rep/, kernels/).
- Plot composite metrics: entropy, rank, norm, diversity.
- Use EMA smoothing or epoch-wise summaries.

---

## 31. Feature Norms Grow Over Time

**Symptoms:**
- Feature magnitudes increase steadily, possibly causing instability.

**Suggested Next Steps:**
- Apply feature normalization (e.g., L2) before computing loss.
- Add a feature norm penalty to encourage compactness.
- Use BatchNorm or LayerNorm in deeper layers.

---

## 32. Learned Representations Are Too Task-Specific

**Symptoms:**
- Representations don’t transfer well or generalize poorly to new domains.

**Suggested Next Steps:**
- Use multi-task learning or domain generalization objectives.
- Apply domain-specific augmentations during pretraining.
- Encourage invariance to domain-specific features with auxiliary losses.

---

## 33. Loss Has High Variance Across Batches

**Symptoms:**
- Spiky loss curve within epochs.

**Suggested Next Steps:**
- Increase batch size or enable gradient accumulation.
- Use gradient clipping or smoother optimizers (e.g., RMSprop).
- Check for label noise, outliers, or data corruption.

---

## 34. Model Learns Trivial Identity Mapping

**Symptoms:**
- Autoencoder or predictor learns to copy input.

**Suggested Next Steps:**
- Increase prediction difficulty (e.g., masked inputs, farther temporal targets).
- Add bottlenecks (e.g., smaller latent size, dropout).
- Use contrastive or predictive coding objectives.

---

## 35. Cosine Similarities Between Embeddings Are All ~1

**Symptoms:**
- Representations collapse toward the same direction.

**Suggested Next Steps:**
- Introduce decorrelation loss (as in VICReg, Barlow Twins).
- Verify augmentations are not too weak (causing alignment to dominate).
- Add predictor asymmetry or stop-gradient operations.

---

## 36. Augmentations Make Model Too Invariant

**Symptoms:**
- Loss increases or performance drops after aggressive augmentation.

**Suggested Next Steps:**
- Tune augmentation intensity based on dataset.
- Use augmentation mixing (e.g., random select from strong/weak).
- Include augmentation-specific loss components (e.g., patch-wise consistency).

---

## 37. Temporal Representations Are Not Predictive

**Symptoms:**
- Predictive performance on future states is poor.

**Suggested Next Steps:**
- Add positional encoding or temporal convolutions.
- Use causal attention or masked convolutions for temporal structure.
- Predict multiple future steps instead of just one.

---

## 38. Spatial Attention Maps Are Too Uniform

**Symptoms:**
- No distinct focus regions in visual attention models.

**Suggested Next Steps:**
- Add attention diversity loss.
- Reduce model’s global pooling dependence.
- Use local self-attention or hierarchical attention blocks.

---

## 39. Embeddings Fail to Capture Class Separability

**Symptoms:**
- No clustering in embedding space despite supervised signals.

**Suggested Next Steps:**
- Add contrastive loss or triplet loss during supervised phase.
- Use center loss or ArcFace-style margins to pull apart classes.
- Visualize with t-SNE or UMAP to better interpret overlaps.

---

## 40. Embedding Space Is Not Smooth

**Symptoms:**
- Small changes in input lead to large jumps in embedding.

**Suggested Next Steps:**
- Apply Lipschitz regularization or Jacobian norm penalty.
- Use interpolation consistency training (e.g., MixUp, CutMix).
- Add a manifold smoothing loss between nearby embeddings.

---

## 41. Training Works but Validation Always Lags Behind

**Symptoms:**
- Validation performance never catches up to training.

**Suggested Next Steps:**
- Increase data augmentation or use test-time augmentation.
- Inspect for distribution shift between train/val datasets.
- Try label smoothing or confidence calibration losses.

---

## 42. Model Too Slow During Inference

**Symptoms:**
- Good training speed but slow forward pass at test time.

**Suggested Next Steps:**
- Replace inefficient layers (e.g., use depthwise separable convs).
- Use TorchScript, ONNX, or TensorRT to optimize inference.
- Prune or quantize model using PyTorch Quantization Toolkit.

---

## 43. Early Epochs Improve, Then Suddenly Diverge

**Symptoms:**
- Model trains initially then becomes unstable.

**Suggested Next Steps:**
- Reduce learning rate decay speed.
- Enable gradient norm tracking — add alerts for spikes.
- Monitor activation magnitudes to catch exploding outputs.

---

## 44. Loss Curve Shows Sudden Jumps or Drops

**Symptoms:**
- Sharp spikes in training/validation loss.

**Suggested Next Steps:**
- Check for batch corruption or augmentation bugs.
- Add gradient clipping to prevent numerical overflow.
- Re-initialize suspicious layers if only one head is unstable.

---

## 45. PCA Shows Linear Patterns, Not Clusters

**Symptoms:**
- Projected embeddings lie on a line or plane.

**Suggested Next Steps:**
- Increase embedding dimensionality or batch size.
- Add non-linearities or residual connections in the projector.
- Use multi-view losses (e.g., more than 2 views in contrastive learning).

---

## 46. Learned Filters Are Identical or Repeating

**Symptoms:**
- Several convolution filters are near-duplicates.

**Suggested Next Steps:**
- Use group convolutions or increase kernel diversity via dropout.
- Enforce low-rank decomposition or sparsity constraints.

---

## 47. Positional Encoding Seems Ineffective

**Symptoms:**
- Model ignores order or position in sequences.

**Suggested Next Steps:**
- Switch from sinusoidal to learned positional embeddings.
- Use relative position encoding (as in Transformer-XL).
- Add position prediction tasks to reinforce order awareness.

---

## 48. Embedding Norms Vary Greatly Across Batches

**Symptoms:**
- Some batches have much higher norms than others.

**Suggested Next Steps:**
- Add batch-wise norm regularization.
- Normalize at the feature level, not the batch level, before projection.
- Log norm histogram across multiple batches and average.

---

## 49. Feature Collapse Only Happens in One Branch

**Symptoms:**
- One encoder or projection path dominates.

**Suggested Next Steps:**
- Tune EMA decay or learning rates independently for branches.
- Add balance loss between online and target representations.
- Ensure gradient is not leaking through stop-gradient nodes.

---

## 50. Model Is Too Sensitive to Hyperparameters

**Symptoms:**
- Tiny hyperparameter changes cause big performance shifts.

**Suggested Next Steps:**
- Add regularization to reduce sensitivity.
- Use hyperparameter sweeps with logging (e.g., Optuna + WandB).
- Start with robust optimizers (AdamW, Lookahead, Lion).

---

## 51. Features Are Disentangled but Not Predictive

**Symptoms:**
- Latent dimensions are cleanly separated but don’t correlate with useful downstream variables.

**Suggested Next Steps:**
- Incorporate supervised fine-tuning on downstream tasks.
- Add mutual information maximization between features and labels.
- Apply causal feature selection to determine which representations matter.

---

## 52. Training Becomes Slower Over Time

**Symptoms:**
- Epoch time increases as training progresses.

**Suggested Next Steps:**
- Investigate data loader memory leaks or GPU fragmentation.
- Restart the job periodically or clear CUDA cache manually.
- Profile with torch.profiler to see if backward pass is bloating.

---

## 53. Final Layer Has Very High or Low Weight Norm

**Symptoms:**
- Final linear/projection layer absorbs too much or too little weight magnitude.

**Suggested Next Steps:**
- Apply weight norm penalty or L2 normalization on output.
- Consider weight normalization or spectral normalization.
- Use layer scaling (small init gain) to control output range.

---

## 54. Representations Work Only With Linear Probes

**Symptoms:**
- Linear classifiers perform well but non-linear downstream tasks fail.

**Suggested Next Steps:**
- Enrich pretraining with nonlinear objectives or auxiliary heads.
- Try representation fusion across layers (e.g., skip connections).
- Add nonlinear probes during analysis to reveal hidden structure.

---

## 55. Model Learns Shortcut Artifacts (e.g., borders, color)

**Symptoms:**
- Model overfits visual or statistical artifacts instead of semantic signals.

**Suggested Next Steps:**
- Use background substitution or style augmentation.
- Add patch mixing or region swapping.
- Train with adversarial examples to suppress shortcut reliance.

---

## 56. Highly Entangled Representations

**Symptoms:**
- Each dimension mixes multiple factors (e.g., pitch + rhythm in audio).

**Suggested Next Steps:**
- Apply factorized loss (e.g., predict rhythm separately).
- Use β-VAE, infoGAN, or total correlation minimization.
- Introduce structured bottlenecks to force separation.

---

## 57. Strong Representations but Weak Predictions

**Symptoms:**
- Intermediate features are rich, but final outputs are weak.

**Suggested Next Steps:**
- Inspect predictor head capacity — may need more depth or nonlinearity.
- Use intermediate supervision (deep supervision).

---

## 58. Embeddings Are Invariant to Too Many Transformations

**Symptoms:**
- Model ignores differences that are actually important (e.g., time shift, expression, class).

**Suggested Next Steps:**
- Refine augmentations: make only task-irrelevant ones invariant.
- Use contrastive pairs with task-relevant variation.
- Add equivariance constraints instead of full invariance.

---

## 59. Loss Improves but Evaluation Metric Stalls

**Symptoms:**
- Loss function keeps decreasing, but downstream metrics (e.g., accuracy, F1, NMI) do not.

**Suggested Next Steps:**
- Double-check that loss correlates with the true objective.
- Use evaluation metrics as auxiliary loss terms.
- Replace MSE with contrastive or ranking losses that better reflect task needs.

---

## 60. Model Struggles With Out-of-Distribution (OOD) Inputs

**Symptoms:**
- Good performance on in-distribution data but fails on minor shifts.

**Suggested Next Steps:**
- Train with domain randomization or style transfer.
- Use Outlier Exposure or self-supervised OOD detection.
- Apply Bayesian or ensemble methods to estimate uncertainty.

---

## 61. Model Quickly Memorizes Training Set

**Symptoms:**
- Training accuracy reaches 100% rapidly, validation stagnates.

**Suggested Next Steps:**
- Increase data augmentation and randomization.
- Apply label smoothing to reduce overconfidence.
- Add dropout, cutout, or mixup to regularize learning.

---

## 62. Model Fails to Learn From Rare Examples

**Symptoms:**
- Minority class or edge-case inputs are consistently mispredicted.

**Suggested Next Steps:**
- Apply oversampling or loss reweighting (e.g., focal loss).
- Use curriculum learning to emphasize rare samples over time.
- Introduce contrastive pairs between rare and common instances.

---

## 63. Model’s Confidence Is Always High

**Symptoms:**
- Predicts with near-certain confidence, even when wrong.

**Suggested Next Steps:**
- Add confidence penalty or entropy maximization regularizer.
- Apply temperature scaling or Bayesian uncertainty estimation.
- Train with label noise or adversarial samples to soften confidence.

---

## 64. Representations Have High Mutual Information With Augmentations

**Symptoms:**
- Embeddings leak augmentation type (e.g., rotation, color jitter).

**Suggested Next Steps:**
- Add augmentation-invariance loss (e.g., contrastive across augment types).
- Mask augmentation identity from predictor.
- Include augmentation prediction task and subtract gradients (as in adversarial multitask setups).

---

## 65. Projection Head Is Bottlenecking Learning

**Symptoms:**
- Increasing projection head depth reduces performance.

**Suggested Next Steps:**
- Try skip connections across projection layers.
- Reduce bottleneck dimension only after sufficient training.
- Move regularization from projection head to encoder directly.

---

## 66. Layer Outputs Are Identical Across Samples

**Symptoms:**
- Intermediate activations are static for different inputs.

**Suggested Next Steps:**
- Use batch-dependent normalization (BatchNorm or GroupNorm).
- Replace static convolutions with dynamic or conditional layers.
- Check for frozen parameters or misinitialized layers.

---

## 67. Kernel Weights Are All Small in Magnitude

**Symptoms:**
- Kernels converge to very low norm, model becomes underpowered.

**Suggested Next Steps:**
- Add weight magnitude regularization (maximize norm).
- Remove or reduce L2 regularization if too aggressive.
- Track gradient norms per layer to ensure flow is active.

---

## 68. Representation Similarity Across Batches Is Too High

**Symptoms:**
- Features learned from different batches are overly aligned.

**Suggested Next Steps:**
- Add batch decorrelation loss.
- Use batch shuffling to reduce alignment bias.
- Compare representations across augment sets, not just batch samples.

---

## 69. Final Output Layer Overfits the Loss Function

**Symptoms:**
- Very low loss, but meaningless or misleading predictions.

**Suggested Next Steps:**
- Replace final linear layer with nonlinear or gated alternatives.
- Regularize final layer weights (e.g., spectral norm).
- Detach loss target partially (e.g., stop gradient halfway through).

---

## 70. Self-Supervised Learning Works Only on Specific Data Formats

**Symptoms:**
- SSL succeeds on spectrograms but fails on raw waveforms, etc.

**Suggested Next Steps:**
- Use multi-view SSL across formats (e.g., CQT + waveform).
- Add domain adapters to bridge modalities.
- Train with cross-modal prediction or co-training tasks.

---

## 71. Symmetric Architectures Cause Collapse

**Symptoms:**
- Identical encoder/predictor/projector in both branches leads to collapse in SSL.

**Suggested Next Steps:**
- Introduce asymmetry via:
  - Extra MLP layer in one branch
  - Predictor module in only the online branch
  - EMA update in one branch only (BYOL-style)
- Add stop-gradient on the target branch.

---

## 72. Projector Output Has No Meaningful Structure

**Symptoms:**
- Projector output is unstructured even if encoder is rich.

**Suggested Next Steps:**
- Visualize encoder vs projector PCA — if projector collapses, simplify or remove layers.
- Use skip connection from encoder to projector output.
- Regularize projector with whitened contrastive losses or decorrelation terms.

---

## 73. Model Learns Only Global Features

**Symptoms:**
- Model ignores local textures or positional signals.

**Suggested Next Steps:**
- Add local prediction tasks (e.g., patch-level contrastive loss).
- Use convolutions or transformer tokens with localized attention.
- Predict patch order, rotation, or center-surround relations.

---

## 74. Loss Gradually Increases Instead of Decreasing

**Symptoms:**
- Loss slowly rises during training (e.g., with large batch sizes).

**Suggested Next Steps:**
- Decrease learning rate or switch to adaptive optimizers like AdamW.
- Try LayerScale or zero-initialized residuals to stabilize.
- Warmup learning rate more slowly or for longer.

---

## 75. All Embedding Norms Converge to the Same Value

**Symptoms:**
- Features saturate to same L2 norm across batch.

**Suggested Next Steps:**
- Add variance loss to push norm diversity (e.g., VICReg style).
- Apply norm-aware augmentation (perturb inputs by feature direction).
- Disable normalization layers temporarily to test effect.

---

## 76. Self-Supervised Model Ignores Temporal Order

**Symptoms:**
- Sequence models don't distinguish between orderings.

**Suggested Next Steps:**
- Add permutation prediction task.
- Use relative position encoding (instead of absolute).
- Predict future representations, not just contrast.

---

## 77. Attention Maps Are Uniform or Random

**Symptoms:**
- Self-attention fails to focus on meaningful regions.

**Suggested Next Steps:**
- Add auxiliary attention supervision if labels available (e.g., saliency).
- Use local/global attention hybrids (e.g., Swin, deformable attention).
- Regularize attention maps with entropy loss or diversity loss across heads.

---

## 78. Output Depends Heavily on Positional Encoding

**Symptoms:**
- Removing positional encodings drastically drops performance.

**Suggested Next Steps:**
- Use learned relative positions instead of absolute.
- Introduce data augmentations that shuffle position to test robustness.
- Add position prediction as an auxiliary task.

---

## 79. Loss Oscillates in Later Epochs

**Symptoms:**
- Noisy up-down loss in late training.

**Suggested Next Steps:**
- Try lowering the learning rate or switch to a smoother optimizer.
- Use Poly decay or Cosine Annealing with restarts.
- Check for numerical instability or NaNs in gradients.

---

## 80. First Few Layers Are Frozen in Learning

**Symptoms:**
- Weights of early layers barely change.

**Suggested Next Steps:**
- Add layer-wise learning rates — increase early layer LR.
- Add skip connections to force gradient flow.
- Inspect input normalization or weight scale mismatch.

---

## 81. Embeddings Are Nearly Binary (Hard Clustering)

**Symptoms:**
- Output features are ~0 or ~1, like hard assignment.

**Suggested Next Steps:**
- Add entropy maximization or softmax temperature tuning.
- Introduce noise in the representation (e.g., Gaussian noise).
- Reduce use of sharpening functions (e.g., hard Gumbel-softmax).

---

## 82. Representations Vary Too Much Across Augmentations

**Symptoms:**
- Representations for the same image under augmentations are too dissimilar.

**Suggested Next Steps:**
- Add alignment loss (L2/cosine between augmented views).
- Reduce augmentation strength (especially when starting training).
- Use temporal averaging or EMA smoothing across augmentations.

---

## 83. Cosine Similarity Between Batches Is Too Low

**Symptoms:**
- Batches of features don’t align at all — model is unstructured.

**Suggested Next Steps:**
- Reduce model capacity to prevent random exploration.
- Add soft clustering loss or global contrastive anchors.
- Inspect for data shuffling bugs causing semantic mismatch.

---

## 84. Model Stops Responding to New Data

**Symptoms:**
- Introducing new data doesn’t affect loss or features.

**Suggested Next Steps:**
- Check for batch normalization stats — may have saturated.
- Increase learning rate temporarily to overcome flat region.
- Reset optimizer state or remove stale EMA tracking.

---

## 85. KL Divergence or InfoNCE Loss is Always Near Zero

**Symptoms:**
- Representation space is uniform or trivially aligned.

**Suggested Next Steps:**
- Reduce use of strong priors (e.g., strong regularization or layer norm).
- Add negative sampling diversity (e.g., memory banks or cross-batch).
- Add temperature tuning in softmax.

---

## 86. Model Performance Degrades After Pretraining

**Symptoms:**
- Transfer performance is worse than random init.

**Suggested Next Steps:**
- Pretrain with task-relevant augmentations or sampling.
- Check if encoder is overfitted to self-supervised proxy task.
- Try partial freezing or reinitializing the final layers during finetuning.

---

## 87. BatchNorm Statistics Drift Wildly Between Epochs

**Symptoms:**
- Sudden jumps in feature norms or accuracy.

**Suggested Next Steps:**
- Replace with GroupNorm or LayerNorm for more stability.
- Switch to SyncBatchNorm in multi-GPU setups.
- Reduce batch size but accumulate stats across steps manually.

---

## 88. PCA Shows Embedding Collapse Only in One Class

**Symptoms:**
- One class has low-variance collapsed representations.

**Suggested Next Steps:**
- Apply class-conditional VC regularization (only penalize collapse inside each class).
- Introduce class-aware augmentations.
- Visualize feature spread per class and adjust sampling accordingly.

---

## 89. Loss Minimizes Too Fast in First Epoch

**Symptoms:**
- Sudden drop in loss without generalization gain.

**Suggested Next Steps:**
- Increase warmup steps for optimizer.
- Try cyclical learning rate to recover from bad convergence basin.
- Track gradient norms across layers — signs of early saturation.

---

## 90. Gradients Are Nonzero but Do Not Change Parameters

**Symptoms:**
- Model reports active gradients, but weights stay almost unchanged.

**Suggested Next Steps:**
- Check for gradient clipping too tight (e.g., clip_value=0.1).
- Inspect optimizer momentum or weight decay conflicts.
- Log parameter delta norms per update to confirm actual change.

---
## 91. Same Features Are Activated Across All Classes

**Symptoms:**
- Activations in final or intermediate layers are class-agnostic.

**Suggested Next Steps:**
- Introduce class-specific loss components (e.g., center loss, ArcFace).
- Add conditional projection heads.
- Visualize per-class activation heatmaps to detect overlaps.

---

## 92. Model Fails After Moving to Mixed Precision

**Symptoms:**
- Loss explodes or underflows when using AMP.

**Suggested Next Steps:**
- Inspect for instabilities in small-scale activations.
- Use `GradScaler()` properly and log scale overflow events.
- Temporarily disable AMP on sensitive layers (e.g., batch norm).

---

## 93. Outputs Saturate at Specific Values (e.g., 0 or 1)

**Symptoms:**
- Model output distribution collapses to a small range.

**Suggested Next Steps:**
- Remove final activation function if unnecessary (e.g., sigmoid before contrastive loss).
- Check for label encoding mismatch (e.g., float vs int targets).
- Clip or rescale labels if they cause saturation in early loss curves.

---

## 94. Per-layer Gradient Norms Are Inverted (later layers have larger gradients)

**Symptoms:**
- Downstream layers receive stronger updates than upstream.

**Suggested Next Steps:**
- Use grad norm balancing or gradient scaling hooks.
- Add skip connections to improve gradient flow.
- Consider progressive layer unfreezing if using transfer learning.

---

## 95. Self-Supervised Representations Are Too Fine-Grained

**Symptoms:**
- Model overfocuses on textures or local details, underperforms on generalization.

**Suggested Next Steps:**
- Apply style randomization or texture-blind augmentations.
- Include global context prediction tasks.
- Penalize local redundancy using contrastive patch-to-global loss.

---

## 96. Early Layers Dominate Kernel Norms

**Symptoms:**
- Weight energy concentrates in first 1–2 conv layers.

**Suggested Next Steps:**
- Normalize all kernel norms per layer.
- Add layer-wise weight regularization to flatten energy profile.
- Visualize kernel statistics to track norm growth over time.

---

## 97. Weight Norms Grow Without Bound

**Symptoms:**
- Unchecked weight magnitude increases even with good validation metrics.

**Suggested Next Steps:**
- Use weight decay, trust region methods, or adaptive optimizers.
- Add explicit norm constraint loss (e.g., L2 penalty > target threshold).
- Normalize weights before output projection.

---

## 98. Similar Input Samples Lead to Divergent Outputs

**Symptoms:**
- Small variations in inputs cause inconsistent predictions.

**Suggested Next Steps:**
- Add consistency regularization (e.g., Mean Teacher or R-Drop).
- Apply feature noise injection and penalize output divergence.
- Check if input preprocessing is unstable (e.g., resizing artifacts).

---

## 99. Feature Channels Are Imbalanced

**Symptoms:**
- Some feature channels are always high or dead across all data.

**Suggested Next Steps:**
- Visualize channel-wise statistics (mean, std, activation frequency).
- Apply channel dropout or learnable channel gating.
- Add channel-wise decorrelation loss (e.g., as in Barlow Twins).

---

## 100. Augmentation Diversity Saturation Reduces Learning Signal

**Symptoms:**
- Weak or overly similar augmentations cause different views of the same input to produce nearly identical features too early in training, reducing learning signal.

**Suggested Next Steps:**
- Increase augmentation diversity or strength (e.g., mix strong+weak views).
- Use view-based dropout or masking (ViewDrop, PatchDrop) to disrupt easy alignment.
- Add a variance constraint or contrastive regularization across views to maintain diversity.

---

## 101. Model Learns Slowly Despite Good Architecture

**Symptoms:**
- Loss decreases very slowly even with powerful architectures.

**Suggested Next Steps:**
- Warm up with easier proxy tasks (e.g., autoencoding or next-patch prediction).
- Apply layer-wise adaptive learning rates (e.g., LARS or Ranger).
- Check that gradients are not being silently clipped or nan-ed.

---

## 102. Training Collapses When Switching Dataset

**Symptoms:**
- Model works on one dataset but diverges or collapses on another.

**Suggested Next Steps:**
- Check if input normalization statistics need adjusting.
- Log label distributions, length distributions, or input ranges.
- Pretrain on the new dataset using a simplified task, then fine-tune.

---

## 103. Representations Are Not Smooth in Latent Space

**Symptoms:**
- Interpolating between embeddings leads to unnatural outputs.

**Suggested Next Steps:**
- Use variational bottlenecks (VAE-style).
- Add latent interpolation consistency loss.
- Train with interpolation-based contrastive learning (e.g., MixCo, InterCLR).

---

## 104. Training Requires Frequent Restarts

**Symptoms:**
- Model gets stuck in poor minima unless manually restarted.

**Suggested Next Steps:**
- Use cyclical learning rates or random restarts as part of training.
- Consider sharpness-aware minimization (SAM) to flatten loss landscape.
- Add noise injection to weights or activations during training.

---

## 105. Feature Collapse Only Happens in Subgroups

**Symptoms:**
- Certain classes, time segments, or data sources show low-rank features.

**Suggested Next Steps:**
- Apply group-wise VC regularization (e.g., per class, per domain).
- Visualize feature diversity conditioned on subgroup identity.
- Use mixture-of-experts or domain-specific branches.

---

## 106. Latent Variables Show Strong Mutual Redundancy

**Symptoms:**
- High mutual information between feature dimensions.

**Suggested Next Steps:**
- Apply total correlation regularization (e.g., in β-TCVAE).
- Use PCA whitening or Barlow Twins-style decorrelation loss.
- Introduce structured sparsity to push diversity across features.

---

## 107. Encoder Layers Are Bypassed

**Symptoms:**
- Final features ignore early-layer representations.

**Suggested Next Steps:**
- Add skip connections and supervise intermediate outputs.
- Penalize activation redundancy across layers.
- Replace deep linear chains with multi-scale feature fusion.

---

## 108. Noise Sensitivity Appears Only During Inference

**Symptoms:**
- Model behaves robustly during training but is brittle at test time.

**Suggested Next Steps:**
- Add test-time augmentation (e.g., augment-and-aggregate).
- Perform Monte Carlo dropout or ensembling during inference.
- Train with distributional robustness objectives.

---

## 109. Positional Encodings Dominate Early Training

**Symptoms:**
- Position information overwhelms content features.

**Suggested Next Steps:**
- Decay position encodings over time (e.g., annealed strength).
- Switch to relative position encoding.
- Add auxiliary tasks that rely solely on content, not position.

---

## 110. Model Can’t Differentiate Easy from Hard Samples

**Symptoms:**
- Accuracy is uniform across all sample types (easy/hard, frequent/rare).

**Suggested Next Steps:**
- Add difficulty-aware losses (e.g., focal loss, margin-based reweighting).
- Use self-paced learning or curriculum learning.
- Label easy vs. hard examples with a confidence head.

---

## 111. Model Learns Dataset-Specific Artifacts

**Symptoms:**
- Performs well on training set, but fails on slightly restructured data (e.g., shifted, rescaled, renamed classes).

**Suggested Next Steps:**
- Augment with synthetic transformations or adversarial corruption.
- Apply domain randomization (e.g., color, blur, resolution).
- Add artifact suppression loss (e.g., decorrelate background).

---

## 112. Embedding Similarity Is Not Transitive

**Symptoms:**
- A ≈ B and B ≈ C, but A ≠ C in latent space.

**Suggested Next Steps:**
- Apply triplet loss or relational contrastive loss to enforce transitivity.
- Introduce semantic anchors (e.g., prototype centers).
- Try contrastive clustering frameworks (e.g., SwAV, DeepCluster).

---

## 113. Model Improves Accuracy but Forgets Earlier Examples

**Symptoms:**
- Accuracy goes up overall but drops on previously mastered samples.

**Suggested Next Steps:**
- Use Elastic Weight Consolidation (EWC) or LwF to retain past knowledge.
- Add memory replay buffer for prior batch samples.
- Introduce importance-weighted regularization on older activations.

---

## 114. Features Are Over-Aligned Across Classes

**Symptoms:**
- Representations for different classes fall into same regions.

**Suggested Next Steps:**
- Add class-conditional contrastive terms.
- Apply Supervised Contrastive Learning (SupCon).
- Add margin penalties between class centers.

---

## 115. Model Fails to Learn Abstract Patterns

**Symptoms:**
- Learns pixel-level or token-level structure, but not higher-level concepts.

**Suggested Next Steps:**
- Add hierarchical prediction objectives (e.g., segment-level prediction).
- Use multi-level pooling and cross-scale attention.
- Introduce task-aware augmentation (e.g., mask shapes, object removal).

---

## 116. Latent Space Is Not Interpretable

**Symptoms:**
- Difficult to associate any axis or region with meaningful behavior.

**Suggested Next Steps:**
- Apply disentanglement constraints (e.g., β-VAE, InfoGAN).
- Add axis alignment loss (match specific dimensions to interpretable features).
- Use linear probing or mutual information diagnostics on each axis.

---

## 117. Contrastive Loss Doesn’t Improve After Few Epochs

**Symptoms:**
- InfoNCE or triplet loss stagnates early.

**Suggested Next Steps:**
- Increase batch size or introduce cross-batch memory.
- Add hard negative mining or false positive suppression.
- Anneal temperature parameter to sharpen contrast.

---

## 118. Some Augmentations Hurt More Than Help

**Symptoms:**
- Certain transforms consistently degrade performance.

**Suggested Next Steps:**
- Log augmentation-wise performance (Ablation).
- Weight augmentations dynamically via learned augmentation policy (e.g., RandAugment, CTAugment).
- Use augmentation mixing or adaptive sampling.

---

## 119. Feature Collapse Occurs Only With Certain Optimizers

**Symptoms:**
- Model collapses only with, e.g., SGD but not Adam.

**Suggested Next Steps:**
- Tune momentum and weight decay separately per optimizer.
- Try Lookahead or LION optimizers for stability.
- Log parameter norm vs. gradient norm for each optimizer.

---

## 120. Embedding Space Is Dominated By Low-Level Statistics

**Symptoms:**
- PCA or t-SNE clusters reflect color, brightness, or length — not semantics.

**Suggested Next Steps:**
- Add style-invariant losses or cross-style contrastive training.
- Use domain adversarial training to remove low-level factors.
- Balance dataset by stratified sampling across confounds.

---

## 121. Representations Shift Over Epochs Without Improving Performance

**Symptoms:**
- Embeddings change (tracked via cosine sim, PCA, etc.), but validation accuracy or loss doesn’t improve.

**Suggested Next Steps:**
- Apply stability-promoting regularization (e.g., Temporal Consistency Loss).
- Add EMA smoothing to stabilize representations.
- Visualize class-wise or token-wise drift to localize the instability.

---

## 122. Multiple Augmented Views Collapse to the Same Point

**Symptoms:**
- Despite different views, embeddings end up nearly identical.

**Suggested Next Steps:**
- Increase augmentation strength, but diversify types (e.g., not just color + crop).
- Add view-specific noise or stochastic depth.
- Ensure contrastive or predictive loss compares only positive pairs, not self-matching.

---

## 123. Representations Encode Unintended Metadata (e.g., filename, sequence ID)

**Symptoms:**
- Clusters emerge based on metadata, not content.

**Suggested Next Steps:**
- Strip or randomize metadata at load time.
- Add confounder adversarial head to remove leaked info.
- Use information bottleneck or dropout on metadata paths.

---

## 124. Representation Collapse Only Happens With Gradient Accumulation

**Symptoms:**
- Training without accumulation is fine, but collapse appears when gradients are accumulated.

**Suggested Next Steps:**
- Reduce accumulation steps or scale learning rate per accumulation cycle.
- Normalize loss per micro-batch, not macro-batch.
- Track loss per step, not per epoch, to diagnose mismatch.

---

## 125. Loss Decreases on Synthetic Data but Not on Real Data

**Symptoms:**
- Strong learning signals on generated or proxy data, no generalization to real-world data.

**Suggested Next Steps:**
- Use domain adaptation loss (e.g., CORAL, MMD, domain confusion).
- Blend synthetic and real with style transfer, cycle consistency, or shared encoder.
- Add real-vs-synthetic discrimination loss to control blending.

---

## 126. Final Embeddings Lack Cluster Structure

**Symptoms:**
- All points are evenly distributed in space, no class or view-based clustering.

**Suggested Next Steps:**
- Add clustering loss (e.g., SwAV, DeepCluster).
- Apply contrastive loss across hard positives and negatives.
- Normalize embeddings to a hypersphere, then cluster.

---

## 127. Features Have High Variance But Poor Discrimination

**Symptoms:**
- Eigenvalue entropy is high, but classification or matching fails.

**Suggested Next Steps:**
- Add discriminative supervision, like pseudo-labels or neighborhood loss.
- Refine positive/negative sampling: quantity ≠ quality.
- Combine variance loss with alignment or classification head.

---

## 128. Masked Prediction Loss Is Too Easy

**Symptoms:**
- Model solves mask-based tasks trivially (e.g., short-range recon).

**Suggested Next Steps:**
- Mask larger and irregular regions (e.g., BEiT, MAE-style).
- Predict non-reconstructible targets (e.g., codebook index, cluster id).
- Increase semantic prediction difficulty, not just spatial.

---

## 129. Semantic Classes Have Overlapping Representations

**Symptoms:**
- t-SNE/PCA shows overlap between semantically distinct classes.

**Suggested Next Steps:**
- Add margin loss, class-specific contrastive loss, or triplet loss.
- Introduce hierarchical labels and supervise coarse/fine levels jointly.
- Log inter-class cosine similarity distributions over epochs.

---

## 130. Downstream Tasks Show Divergent Trends

**Symptoms:**
- Improvements in one task hurt another (e.g., classification vs. retrieval).

**Suggested Next Steps:**
- Apply multi-head or multi-task learning, allowing task-specific representation flow.
- Introduce a shared encoder + gated adapters for task-specialized features.

---

## 131. Loss Improves, but Only for a Subset of Batches

**Symptoms:**
- Loss drops during some batches but spikes on others.

**Suggested Next Steps:**
- Log loss per batch and compute standard deviation across batches.
- Stratify training data by difficulty, class, domain, and examine batch composition.
- Apply batch-wise loss balancing or curriculum-based batch sampling.

---

## 132. Features Are Too Similar Across Different Modalities

**Symptoms:**
- Audio and text, or image and text, have indistinct embeddings.

**Suggested Next Steps:**
- Use modality-specific encoders before fusion.
- Apply modality contrastive loss (align only cross-modality positives).
- Add orthogonality or diversity loss across modalities.

---

## 133. Gradients Are Dominated By Easy Samples

**Symptoms:**
- Hard examples contribute little to gradient updates.

**Suggested Next Steps:**
- Apply hard example mining (semi-hard triplet, focal loss).
- Track gradient norms per sample and reweight dynamically.
- Try gradient boosting strategies like GHM loss.

---

## 134. Model Stops Learning After First Few Epochs

**Symptoms:**
- Loss stagnates early, even with enough capacity.

**Suggested Next Steps:**
- Reduce weight decay or increase initial learning rate.
- Add stochastic layers or dropout to prevent early convergence.
- Inspect activation saturation or dead neurons.

---

## 135. Training Accuracy is High But Representations are Not Transferable

**Symptoms:**
- SSL pretraining yields poor results in new domains/tasks.

**Suggested Next Steps:**
- Include domain-agnostic augmentations.
- Evaluate representations with linear probing across datasets.
- Regularize with maximum entropy or mutual information objectives.

---

## 136. Augmented Pairs Are Mismatched in Difficulty

**Symptoms:**
- One view is trivial, the other too distorted.

**Suggested Next Steps:**
- Apply augmentation parity checking (both views should retain core signal).
- Use progressive augmentation, ramping up strength symmetrically.
- Consider augmentation-aware loss reweighting.

---

## 137. Downstream Accuracy Increases, but Embedding Rank Drops

**Symptoms:**
- Validation improves while feature covariance rank or entropy decreases.

**Suggested Next Steps:**
- Add a diversity-promoting loss (e.g., VICReg’s variance or covariance loss).
- Introduce multi-head learning to expand embedding use.
- Use attention-based bottlenecks to preserve distinct components.

---

## 138. Temporal SSL Model Predicts the Future Too Easily

**Symptoms:**
- Predictive loss saturates; model memorizes patterns.

**Suggested Next Steps:**
- Increase prediction horizon (e.g., predict t+5 instead of t+1).
- Add random time masking or drop intermediate frames.
- Use shuffled targets or contrastive prediction to increase difficulty.

---

## 139. Some Classes Are Represented Only By Rare Features

**Symptoms:**
- Certain classes rely on uncommon dimensions or embeddings.

**Suggested Next Steps:**
- Apply class-specific normalization or scaling.
- Add representation frequency regularization (e.g., avoid overconcentration).
- Use label-aware dimensional analysis (e.g., variance per class).

---

## 140. Model Only Uses Low-Frequency Patterns in Input

**Symptoms:**
- High-frequency information is ignored (e.g., textures, fine edges, pitch variations).

**Suggested Next Steps:**
- Add frequency domain augmentations (e.g., filtering, jitter).
- Introduce multi-scale processing (e.g., dilated convolutions, wavelets).
- Train with spectral contrastive loss or predict Fourier/CQT/STFT embeddings.

---

## 141. Feature Collapse Occurs Only in the Deeper Layers

**Symptoms:**
- Early layers are diverse; collapse happens near output.

**Suggested Next Steps:**
- Apply VC regularization (variance/covariance) directly on deeper layers.
- Add auxiliary heads at intermediate layers to guide learning progressively.
- Use layer-wise contrastive loss (SimCLR with deep supervision).

---

## 142. Model Only Learns Short-Term Dependencies in Sequences

**Symptoms:**
- Sequence model ignores long-term context.

**Suggested Next Steps:**
- Add dilated convolutions or transformer attention with larger receptive field.
- Include future context prediction or temporal jigsaw tasks.
- Add sequence length-aware curriculum during training.

---

## 143. Representation Entropy is High but Not Informative

**Symptoms:**
- High entropy, but all dimensions contribute noise, not useful variance.

**Suggested Next Steps:**
- Use informative bottlenecks (e.g., mutual info maximization).
- Add target-alignment losses like supervised contrastive or linear probe loss.
- Analyze dimensional usefulness using linear probes per axis.

---

## 144. Optimization Is Trapped in Flat Regions

**Symptoms:**
- Gradients are nonzero but loss barely changes.

**Suggested Next Steps:**
- Switch to optimizer with momentum adaptation (e.g., Lion, RAdam).
- Use sharpness-aware minimization (SAM) or loss landscape smoothing.
- Add batch norm reset mid-training to refresh scale sensitivity.

---

## 145. Multi-View Features Are Misaligned Across Representations

**Symptoms:**
- Augmented views of the same sample drift apart in embedding space.

**Suggested Next Steps:**
- Add alignment loss with cosine or L2 distance between views.
- Normalize both views before projection (z = F.normalize(z)).
- Use stop-gradient to stabilize learning in one view (BYOL-style).

---

## 146. SSL Model Converges Too Fast

**Symptoms:**
- Rapid loss minimization and early stagnation.

**Suggested Next Steps:**
- Lower initial learning rate and add longer warmup.
- Delay EMA updates in target networks or use burn-in.
- Schedule augmentation intensity to ramp up over time.

---

## 147. Different Heads in Multi-Head Models Learn Redundant Information

**Symptoms:**
- Each head learns nearly identical representations.

**Suggested Next Steps:**
- Force heads to focus on different subspaces via attention masks.
- Train with head-specific augmentations or tasks.

---

## 148. Pretext Task Rewards Easy-to-Cheat Shortcuts

**Symptoms:**
- Model solves task without learning useful representations.

**Suggested Next Steps:**
- Modify task to eliminate shortcuts (e.g., masking center patches in jigsaw).
- Add auxiliary tasks that require semantic abstraction.
- Use adversarial pretext noise to suppress spurious signals.

---

## 149. Low-Rank Covariance Is Due to ReLU Dead Units

**Symptoms:**
- Collapse traced to dead ReLU paths (zero output channels).

**Suggested Next Steps:**
- Use LeakyReLU, GELU, or Softplus in early layers.
- Initialize layers with positive bias to avoid ReLU death.
- Track activation rate per neuron/channel and penalize inactivity.

---

## 150. Model Is Sensitive to Padding or Edge Effects

**Symptoms:**
- Predictions shift with minor padding or crop shifts.

**Suggested Next Steps:**
- Use reflection padding or zero-padding removal.
- Add edge-aware augmentations or mask boundary predictions.
- Train with crop consistency (predictions invariant under context shift).

---

## 151. Model Generalizes Poorly Across Domains Despite Similar Label Sets

**Symptoms:**
- Works well on one domain (e.g., photos), fails on another (e.g., sketches, renderings).

**Suggested Next Steps:**
- Apply domain-adversarial training (e.g., DANN).
- Use domain-specific batch normalization or feature whitening.
- Include domain-agnostic augmentations or style mixing.

---

## 152. High Performance on High-Res Images, Poor on Low-Res

**Symptoms:**
- Accuracy or representation quality drops with input downscaling.

**Suggested Next Steps:**
- Train with multi-resolution inputs or resolution jitter.
- Add anti-aliasing filters before downsampling (to preserve signal).
- Use feature pyramids or scale-invariant encoders.

---

## 153. Embedding Space Shows Radial Clustering by Input Length or Area

**Symptoms:**
- Longer or larger inputs are farther from origin, not necessarily semantically different.

**Suggested Next Steps:**
- Apply length normalization to latent vectors.
- Use positional embedding normalization in variable-length inputs.
- Penalize correlation between input length and representation norm.

---

## 154. Representations Are Spatially Biased (e.g., corners encode more)

**Symptoms:**
- Model uses consistent spatial locations more heavily, regardless of content.

**Suggested Next Steps:**
- Add spatial dropout or location shuffling to break positional bias.
- Visualize activation heatmaps to localize reliance.
- Use center bias regularization to equalize focus.

---

## 155. Norm Layers Prevent Meaningful Representation Spread

**Symptoms:**
- BatchNorm or LayerNorm zero-centers all representations too aggressively.

**Suggested Next Steps:**
- Replace BatchNorm with GroupNorm, especially for small batches.
- Add learnable affine scaling after normalization.
- Temporarily disable norm layers to test impact on diversity.

---

## 156. Learned Features Are Highly Sensitive to Color Channels

**Symptoms:**
- RGB perturbations change output significantly even with minor distortion.

**Suggested Next Steps:**
- Train with color jitter, channel drop, or grayscale conversion.
- Add color-invariant auxiliary loss or style-drop layers.
- Separate shape vs. color encoding via dual-path networks.

---

## 157. SSL Loss Is Minimized by Trivial Solutions (e.g., predicting position from padding)

**Symptoms:**
- Model uses unintended input clues to minimize loss.

**Suggested Next Steps:**
- Add masking or crop randomization to remove predictable structure.
- Include false positives to detect spurious shortcuts.
- Regularize using intermediate layer agreement, not just final layer.

---

## 158. Learned Kernels Specialize Too Early in Training

**Symptoms:**
- Filters converge to sharp patterns before semantic features form.

**Suggested Next Steps:**
- Use soft weight decay early and increase regularization later.
- Apply stochastic depth to limit early overfitting.
- Track kernel evolution; freeze early kernels periodically to stabilize.

---

## 159. Transfer Learning Fails When Using Custom Projectors

**Symptoms:**
- Replacing projector head prevents useful downstream reuse.

**Suggested Next Steps:**
- Detach projector during inference and only use encoder features.
- Use skip connection from encoder to projector, and monitor both.
- Pretrain with frozen encoder, then unfreeze gradually.

---

## 160. Learned Representations Encode Temporal Ordering Even When Irrelevant

**Symptoms:**
- Representations leak time step info, harming generalization.

**Suggested Next Steps:**
- Shuffle time blocks during pretraining to disentangle content from order.
- Use relative temporal prediction instead of absolute position.
- Add contrastive tasks across time-shifted, non-ordered views.

---

## 161. Latent Space Saturates Early But Output Still Improves

**Symptoms:**
- Representation metrics (e.g., variance, entropy) plateau early, but task loss continues to drop.

**Suggested Next Steps:**
- Apply nonlinear probing heads to measure deeper latent changes.
- Add feature dynamics tracking (cosine similarity of features across epochs).
- Consider regularizing embedding stability, not just diversity.

---

## 162. Learning Rate Scheduler Triggers Before Model is Ready

**Symptoms:**
- LR drops before representation or loss has stabilized.

**Suggested Next Steps:**
- Use loss-based scheduler (e.g., ReduceLROnPlateau).
- Delay scheduler start or set minimum warmup epochs.
- Replace scheduler with cosine decay with restart to regain performance later.

---

## 163. Representations Across Augmentations Are Over-Aligned

**Symptoms:**
- Feature vectors from multiple views are too similar — loss is minimized trivially.

**Suggested Next Steps:**
- Increase augmentation strength asymmetry (strong–weak pairing).
- Add decorrelation loss across views.
- Replace full alignment with InfoNCE loss, requiring harder matching.

---

## 164. Cluster Assignments Are Dominated by Large or Frequent Classes

**Symptoms:**
- SwAV/DeepCluster-type prototypes collapse to majority classes.

**Suggested Next Steps:**
- Add prototype diversity constraints or entropy maximization.
- Apply balanced sampling or prototype reweighting.
- Temporarily increase the number of prototypes beyond class count.

---

## 165. Early Layers Do Not Learn Without Supervision

**Symptoms:**
- Only deep layers show feature learning under SSL.

**Suggested Next Steps:**
- Apply deep supervision to intermediate activations.
- Add early-layer contrastive loss (SimCLR + deep heads).
- Use skip connections to amplify shallow feature propagation.

---

## 166. Projection Head Dominates Training Signal

**Symptoms:**
- Encoder stops improving, projector keeps changing.

**Suggested Next Steps:**
- Detach or freeze projector mid-training to refocus on encoder.
- Regularize encoder with auxiliary losses not routed through the projector.
- Match encoder-to-encoder similarity directly (e.g., in SimSiam variants).

---

## 167. Pretext Task Is Solved by Superficial Heuristics

**Symptoms:**
- Model uses easily exploitable shortcut instead of learning features.

**Suggested Next Steps:**
- Add adversarial augmentation to prevent shortcut exploitation.
- Apply masking or corruption to shortcut channels (e.g., shape-only masks).
- Validate usefulness with probing tasks to detect overfit pretext-specific patterns.

---

## 168. Latent Space is Too Uniform Across All Samples

**Symptoms:**
- Feature vectors are evenly distributed but not semantically meaningful.

**Suggested Next Steps:**
- Add semantic structure loss (e.g., supervised contrastive loss, label smoothing).
- Cluster on intermediate layers rather than final features.
- Evaluate neighborhood consistency — local similarity within semantic groups.

---

## 169. Overuse of Stop-Gradient Blocks Learning

**Symptoms:**
- Model with stop-gradient fails to improve because no feedback is flowing.

**Suggested Next Steps:**
- Only stop gradient on one branch or one layer, not everywhere.
- Log gradient norms per module to verify flow.
- Add predictor bypass path to allow controlled flow from target to online branches.

---

## 170. Model Is Highly Sensitive to Specific Token Positions (e.g., start or end tokens)

**Symptoms:**
- Performance drops when changing sequence offset or padding.

**Suggested Next Steps:**
- Use relative positional encodings or rotary embeddings.
- Add token masking during pretraining to suppress fixed-position reliance.
- Train with sequence shifting augmentation (e.g., offset and pad randomly).

---

## 171. Multimodal Fusion Layer Causes Instability

**Symptoms:**
- When modalities (e.g., audio + text) are fused, loss becomes unstable or accuracy drops.

**Suggested Next Steps:**
- Use modality-specific encoders followed by late fusion.
- Normalize modality embeddings separately before combining.
- Try cross-attention instead of concatenation or summation.

---

## 172. Contrastive Loss Heavily Skewed Toward Negatives

**Symptoms:**
- Negatives dominate gradient flow; positives have minimal influence.

**Suggested Next Steps:**
- Increase positive-to-negative ratio using hard positive mining.
- Add false negative detection or de-biased contrastive loss (e.g., DCL).
- Reduce temperature to sharpen the softmax distribution.

---

## 173. Freezing Lower Layers Hurts Representation Quality

**Symptoms:**
- Training with frozen early layers limits expressiveness.

**Suggested Next Steps:**
- Use progressive unfreezing: start frozen, then unfreeze gradually.
- Apply adapter modules on top of frozen layers to reintroduce learning.
- Add skip connections from frozen to learnable layers.

---

## 174. Class Prototypes Collapse to a Few Embedding Points

**Symptoms:**
- Feature centers (prototypes) of many classes overlap in space.

**Suggested Next Steps:**
- Increase prototype diversity with repulsion loss or angular margins.
- Add class-conditional decorrelation loss.
- Periodically reinitialize or shuffle prototypes.

---

## 175. Temporal Features Drift Between Epochs

**Symptoms:**
- Features for the same temporal point change drastically each epoch.

**Suggested Next Steps:**
- Add temporal stability loss or EMA features per time step.
- Use time-contrastive learning to anchor representations over time.
- Reduce learning rate or apply temporal dropout.

---

## 176. Representation Collapse Only Happens at Larger Batch Sizes

**Symptoms:**
- Small batches work, but collapse occurs when scaling up.

**Suggested Next Steps:**
- Increase variance/covariance regularization strength.
- Apply gradient noise injection or gradient centralization.
- Normalize embeddings across batch and view, not just batch.

---

## 177. Classifier Overfits to Label Distribution

**Symptoms:**
- Model predicts frequent classes regardless of input signal.

**Suggested Next Steps:**
- Rebalance loss using effective number of samples or focal loss.
- Add label-agnostic self-supervised pretraining.
- Use balanced sampling or class-aware augmentations.

---

## 178. Large Architectures Cause Early Divergence

**Symptoms:**
- Large models diverge faster than small ones.

**Suggested Next Steps:**
- Add learning rate scaling with model width (e.g., √d or 1/d).
- Use grad norm clipping per layer to avoid large jumps.
- Warm up both weights and momentum buffers gradually.

---

## 179. Gradient Norm is Stable, But Loss Plateaus

**Symptoms:**
- Backprop appears normal, but learning stalls.

**Suggested Next Steps:**
- Check for activation saturation (e.g., via histogram of outputs).
- Switch activation functions (e.g., ReLU → GELU).
- Add orthogonal initialization to kick-start dynamics.

---

## 180. Regularization Prevents Signal from Emerging

**Symptoms:**
- High dropout or strong norm constraints suppress learning.

**Suggested Next Steps:**
- Reduce regularization gradually (e.g., schedule weight decay or dropout).
- Visualize weight evolution to detect over-constrained filters.
- Use dynamic regularization based on loss or layer confidence.

---

## 181. Pretrained Model Loses Performance After Domain-Specific Fine-Tuning

**Symptoms:**
- Fine-tuning on a narrow domain destroys generalization.

**Suggested Next Steps:**
- Use layer-wise learning rate decay (lower LR in early layers).
- Apply adapter layers instead of fine-tuning full model.
- Use interleaved fine-tuning: alternate domain and general-domain steps.

---

## 182. Attention Maps Are Static Across Inputs

**Symptoms:**
- Self-attention maps look the same regardless of input.

**Suggested Next Steps:**
- Inspect input norm and variance — model might be understimulated.
- Add attention diversity loss (e.g., max entropy or orthogonality).
- Reduce positional encoding scale if it dominates token interaction.

---

## 183. Contrastive Tokens Collapse to a Single Cluster

**Symptoms:**
- Augmented views or masked tokens converge to identical embeddings.

**Suggested Next Steps:**
- Use token-level decorrelation (e.g., VICReg or Barlow Twins per token).
- Predict token categories or relative positions as an auxiliary task.
- Apply patch dropping or masked token prediction jointly with contrastive loss.

---

## 184. Latent Features Mirror Unintended Artifacts (e.g., filename, camera angle)

**Symptoms:**
- Embeddings cluster by collection metadata, not semantics.

**Suggested Next Steps:**
- Strip metadata during dataloader preprocessing.
- Apply confounder adversarial loss to suppress leaked artifacts.
- Use distributional alignment to balance camera or sensor source.

---

## 185. Model Is Too Confident on Ambiguous Inputs

**Symptoms:**
- Predictive confidence is high even for borderline or noisy inputs.

**Suggested Next Steps:**
- Apply confidence penalty (e.g., softmax entropy maximization).
- Use label smoothing during training.
- Add test-time dropout or Monte Carlo dropout for uncertainty estimation.

---

## 186. Model Learns Class-Specific Augmentation Shortcuts

**Symptoms:**
- Certain augmentations correlate with specific classes, leading to shortcut learning.

**Suggested Next Steps:**
- Log augmentation statistics per class and rebalance augmentation assignment.
- Use class-agnostic or randomized augmentations.
- Add augmentation confusion loss to decorrelate.

---

## 187. Input Tokens Are Underutilized in Attention

**Symptoms:**
- Some tokens receive near-zero attention weights across all heads/layers.

**Suggested Next Steps:**
- Visualize token importance heatmaps across time/space.
- Regularize attention scores with minimum usage constraints.
- Add token dropout to force learning from all parts of the input.

---

## 188. Latent Features Form Hollow Shells (e.g., ring-shaped or boundary-concentrated)

**Symptoms:**
- Embeddings occupy edges of hypersphere with little inner mass.

**Suggested Next Steps:**
- Reduce use of L2 normalization or spherical loss if inappropriate.
- Apply volume-filling regularization or maximize entropy in latent space.
- Replace uniform norm target with controlled radius distribution.

---

## 189. Token-Level Representations Lack Global Context

**Symptoms:**
- Each token is encoded well locally, but global relationships are missed.

**Suggested Next Steps:**
- Add CLS token interaction loss (e.g., contrast token ↔ pooled rep).
- Include graph-based regularization over the token connectivity.
- Use global masking tasks (e.g., shuffle patches and predict alignment).

---

## 190. Fine-Tuned Model Forgets Pretraining Signals Too Quickly

**Symptoms:**
- Downstream training overwrites SSL features completely.

**Suggested Next Steps:**
- Apply feature alignment regularization (keep SSL features close).
- Use low learning rate or freeze early layers during downstream phase.
- Add distillation loss from pretraining version of model.

---

## 191. Model Can’t Handle Very Long Sequences (>1024 tokens)

**Symptoms:**
- Performance drops or memory explodes when sequence length increases.

**Suggested Next Steps:**
- Use chunking or sliding window attention (e.g., Longformer, BigBird).
- Add recurrence modules (e.g., Transformer-XL memory caching).
- Train with truncated sequences and finetune on full-length.

---

## 192. One Augmentation View Always Dominates Representation

**Symptoms:**
- Contrastive pairs are asymmetric; only one view contributes useful features.

**Suggested Next Steps:**
- Equalize augmentation difficulty and diversity between views.
- Swap encoder inputs during training (view_a → encoder_b).
- Track and plot view-specific gradient magnitudes to monitor imbalance.

---

## 193. Encoder Reaches Capacity Limit Early

**Symptoms:**
- Representations saturate even though loss continues improving.

**Suggested Next Steps:**
- Add extra bottleneck layers or increase depth of encoder.
- Use wider embedding layers or more channels.
- Add residual connections to prevent dead-end compression.

---

## 194. Model Exhibits Global Collapse Despite Balanced Loss Terms

**Symptoms:**
- Even with VC regularization or contrastive loss, collapse still occurs.

**Suggested Next Steps:**
- Monitor feature norms, rank, entropy during training in real time.
- Add adaptive scaling of regularization losses (e.g., scale VC by cosine sim drop).
- Add stochastic gradient skip connections to prevent collapse cascades.

---

## 195. Contrastive Features Become Memory-Dependent

**Symptoms:**
- SSL model performs well only when memory banks or queues are active.

**Suggested Next Steps:**
- Reduce reliance on memory by increasing batch size.
- Use momentum encoders (BYOL, MoCo) to stabilize without external queues.
- Simulate large batches with cross-GPU negative sharing.

---

## 196. Features Drift Between Validation and Training Modes

**Symptoms:**
- Latents behave differently between training and evaluation (e.g., in batchnorm mode switch).

**Suggested Next Steps:**
- Use GroupNorm or LayerNorm for consistency across modes.
- Track mean/std statistics of outputs across train vs. eval.
- Add validation-time alignment loss to correct this drift.

---

## 197. Backbone Weights Oscillate or Diverge Under Joint Objectives

**Symptoms:**
- Joint SSL + supervised training causes encoder instability.

**Suggested Next Steps:**
- Apply loss scheduling (e.g., pretrain SSL first, then gradually mix supervision).
- Use multi-head architecture, each head for one task.
- Detach gradients between conflicting branches.

---

## 198. Prototypes Collapse to Uniform Direction

**Symptoms:**
- All cluster centers or anchors in the latent space collapse toward the same point or line.

**Suggested Next Steps:**
- Apply orthogonal constraint or repulsive prototype loss.
- Periodically re-initialize prototypes randomly or based on KMeans.
- Enforce entropy-maximizing soft assignment in cluster heads.

---

## 199. Model Underperforms on Edge Cases Despite High Average Accuracy

**Symptoms:**
- Accuracy good overall, but fails on rare or ambiguous inputs.

**Suggested Next Steps:**
- Track and log performance per input cluster or difficulty bin.
- Use confidence-aware losses or hard-negative oversampling.
- Add a secondary head trained only on difficult examples.

---

## 200. Embedding Space Shows “Grid-Like” Artifacts or Clumping

**Symptoms:**
- t-SNE or PCA shows unnatural geometric artifacts (e.g., grids, bands).

**Suggested Next Steps:**
- Avoid strong weight tying or positional encoding that enforces structure.
- Increase activation noise or dropout during training.
- Add feature-space regularizers that penalize repeating distances.

---

## 201. Bottleneck Layer Becomes Information Sink

**Symptoms:**
- Information gets trapped before or within a narrow hidden layer (e.g., 128-d bottleneck).

**Suggested Next Steps:**
- Add skip connections from earlier layers to bypass the bottleneck.
- Increase bottleneck dimensionality temporarily and schedule decay.
- Apply information maximization loss (e.g., maximize variance, entropy, or MI).

---

## 202. One Modality Is Ignored in Multimodal Pretraining

**Symptoms:**
- Model ignores image/audio/text in favor of just one.

**Suggested Next Steps:**
- Add modality dropout to force robustness without that modality.
- Use modality-wise contrastive loss (e.g., CLIP-style).
- Apply modality-specific projection heads before fusion.

---

## 203. Augmentations Cause Drifting Representation Centers Over Epochs

**Symptoms:**
- Each view’s representation shifts its mean over time.

**Suggested Next Steps:**
- Use centered contrastive loss or online feature centering (as in VICReg).
- Track and normalize mean shift per augmentation type.
- Apply EMA center alignment or momentum projection head.

---

## 204. Model Collapses Only on Local Feature Maps, Not Global Pooling

**Symptoms:**
- Global average pooled features are diverse, but spatial maps are collapsed.

**Suggested Next Steps:**
- Regularize inter-patch variance (patch-wise VC loss).
- Apply token-level contrastive learning.
- Predict masked patch position or class to inject local supervision.

---

## 205. Downstream Classifier Overwrites Pretrained Representation

**Symptoms:**
- Linear/MLP head dominates learning signal during fine-tuning.

**Suggested Next Steps:**
- Freeze the backbone for the first few epochs of fine-tuning.
- Add distillation from pretrained encoder output.
- Use cosine classifier instead of dot product MLP to preserve direction.

---

## 206. SSL Loss Remains Flat While Representation Entropy Drops

**Symptoms:**
- Loss appears stable but latent features become more collapsed over time.

**Suggested Next Steps:**
- Track entropy, eigenvalue spectrum, cosine similarity in addition to loss.
- Scale up variance or covariance loss coefficients dynamically.
- Detect flat loss regions and inject noise or optimizer momentum reset.

---

## 207. LayerNorm Interferes with Representation Spread

**Symptoms:**
- Norm-based layers suppress useful latent feature variance.

**Suggested Next Steps:**
- Test with LayerNorm removed from last encoder/projector layer.
- Add post-LayerNorm learnable scaling.
- Compare to BatchNorm, GroupNorm, or no normalization.

---

## 208. Self-Supervised Features Are Not Linearly Separable

**Symptoms:**
- Linear probe performs poorly; non-linear probe does better.

**Suggested Next Steps:**
- Add supervised contrastive loss or center loss during fine-tuning.
- Regularize for linear alignment (e.g., linear probing at training time).
- Increase embedding dimensionality to reduce projection saturation.

---

## 209. Local Neighborhoods in Latent Space Lack Structure

**Symptoms:**
- Nearest neighbors are semantically unrelated.

**Suggested Next Steps:**
- Add neighborhood consistency loss (e.g., kNN overlap).
- Use semantic contrastive learning if pseudo-labels or hierarchy exist.
- Train with triplet sampling from same label cluster if supervised data is available.

---

## 210. Projections Differ Across Training Seeds Despite Similar Performance

**Symptoms:**
- Model converges to different latent spaces for different runs.

**Suggested Next Steps:**
- Align latent spaces using procrustes alignment or centered kernel alignment.
- Add orthogonality or directional regularization across runs (e.g., ensemble stabilization).
- Seed fix and log random state of augmentations, dropout, and initialization for full reproducibility.

---

## 211. Model Appears to Converge, But Representation Quality Stagnates

**Symptoms:**
- Loss flatlines, but PCA/eigenvalue entropy or transfer accuracy stays low.

**Suggested Next Steps:**
- Switch to secondary diagnostic objectives (e.g., local contrast, token prediction).
- Inject perturbation noise or gradient reversal to escape flat minima.
- Reset or adapt projector weights mid-training (cold-restart regularization).

---

## 212. Representations Stop Improving After BatchNorm Saturation

**Symptoms:**
- BatchNorm outputs flatten or diverge after stabilization.

**Suggested Next Steps:**
- Replace with GroupNorm or LayerNorm for better per-sample stability.
- Track running mean/var drift — reset stats if necessary.
- Apply LayerScale to prevent early BatchNorm domination.

---

## 213. Small-Scale Models Over-Regularize and Underfit

**Symptoms:**
- VC regularization hurts more than helps in compact architectures.

**Suggested Next Steps:**
- Scale regularization coefficients (e.g., VICReg’s µ, ν) proportional to model width.
- Use sparser views or reduce batch size to soften constraints.
- Replace global VC loss with local diversity loss in intermediate layers.

---

## 214. Augmentation Views Leak Identity (e.g., color tags, borders)

**Symptoms:**
- Views differ in trivial artifacts that make identity prediction easy.

**Suggested Next Steps:**
- Visualize augmented views side by side — detect trivial anchors.
- Add augmentation adversarial loss: prevent model from using view-specific cues.
- Randomize augmentation pipeline per batch.

---

## 215. Cross-Entropy Loss Plateaus with No Obvious Collapse

**Symptoms:**
- Accuracy or probe scores stall while loss hovers at constant value.

**Suggested Next Steps:**
- Add cosine classifier or temperature scaling to sharpen logits.
- Increase embedding space curvature using nonlinearities or wider projection.
- Introduce pseudo-supervised cluster heads (e.g., DeepCluster or SwAV prototypes).

---

## 216. Representations Are Semantically Valid But Geometrically Distorted

**Symptoms:**
- Clusters exist, but with poor margin or shape (e.g., elongated blobs).

**Suggested Next Steps:**
- Apply contrastive margin loss or angular margin loss.
- Add isotropy regularization or maximize distance uniformity.
- Visualize local curvature metrics to reshape representation.

---

## 217. Early Convergence Hides Failure to Generalize

**Symptoms:**
- Validation loss improves fast, then drops suddenly on unseen data.

**Suggested Next Steps:**
- Evaluate using cross-domain or corrupted datasets during training.
- Delay optimizer plateau by using learning rate warmup with cosine ramp.
- Introduce task perturbation or weak supervision noise mid-training.

---

## 218. Learning Rate Warmup Has No Effect

**Symptoms:**
- Model behaves the same with or without warmup.

**Suggested Next Steps:**
- Check if early gradient magnitudes are already small (indicates saturation).
- Warm up per parameter group (e.g., encoder vs. projector vs. predictor).
- Replace static schedule with loss-aware LR adjustment.

---

## 219. Final Representations Overfit to View Type Rather Than Content

**Symptoms:**
- t-SNE separates views (augmentation type) more than class/content.

**Suggested Next Steps:**
- Penalize augmentation-type classification (confounder suppression).
- Use mixed-view consistency loss (e.g., blend of two views → same rep).
- Collapse multiple augmentations to a shared intermediate representation.

---

## 220. Model Predicts Latents That Are Statistically Perfect but Semantically Useless

**Symptoms:**
- Perfect cosine similarity / loss match, but features are meaningless.

**Suggested Next Steps:**
- Visualize latent neighborhoods, not just similarity to targets.
- Add semantic probe tasks (linear/MLP head on frozen encoder).
- Validate representation using retrieval, clustering, or out-of-distribution detection.

---

## 221. Token Aggregation Method Bottlenecks Representational Power

**Symptoms:**
- Switching from mean pooling to CLS token drastically changes performance.

**Suggested Next Steps:**
- Use learned pooling (e.g., attention-based pooling or weighted averaging).
- Combine CLS + mean pooling (concatenation or residual blend).
- Track activation statistics of pooled vs. raw token sets.

---

## 222. Multiple Heads Converge to Same Representations

**Symptoms:**
- Multi-head architecture has identical outputs or similar feature maps.

**Suggested Next Steps:**
- Add orthogonality constraints between head outputs.
- Apply head-wise diversity loss or negative cosine similarity.
- Force head specialization via view/task conditioning.

---

## 223. Latents Encode Very Low Mutual Information With Original Input

**Symptoms:**
- Representations are compressive but unrelated to input structure.

**Suggested Next Steps:**
- Add mutual information maximization (e.g., InfoNCE with input embedding).
- Use inpainting or masked autoencoding to tether latent to input.
- Visualize input reconstruction or saliency maps from latents.

---

## 224. Model Fails to Learn When Using Sparse Attention or Sparse Conv Layers

**Symptoms:**
- Sparsity-based models stall in learning phases.

**Suggested Next Steps:**
- Warm up with dense layers, then transition to sparse (gradual pruning).
- Use gating mechanisms to control sparsity activation.
- Increase batch size or sequence length to populate sparse regions.

---

## 225. Feature Drift Magnitude Changes With Batch Size

**Symptoms:**
- Larger batches lead to higher representation shift per step.

**Suggested Next Steps:**
- Track and plot embedding drift norm per batch size.
- Scale optimizer learning rate with batch size (LR ∝ batch_size).
- Use gradient accumulation to simulate large batches with smaller updates.

---

## 226. Projection Head Masks Convergence Issues

**Symptoms:**
- Projected features converge, but encoder features remain noisy.

**Suggested Next Steps:**
- Add loss terms on encoder output in addition to projector output.
- Track alignment between encoder + projector space (cosine sim).
- Temporarily detach projector and evaluate encoder directly.

---

## 227. Positional Encoding Disrupts Learned Patterns on Reordering

**Symptoms:**
- Input permutation breaks latent structure, even for invariant content.

**Suggested Next Steps:**
- Use relative position encoding or rotary embeddings.
- Add shuffled-sequence prediction task (permutation contrastive learning).
- Test positional bias by training with positional dropout.

---

## 228. Semantic Clustering Only Appears at Intermediate Layers

**Symptoms:**
- Early or middle layers show meaningful clusters, final layer doesn’t.

**Suggested Next Steps:**
- Apply multi-layer probing to determine where structure is lost.
- Use early-exit representations or ensemble intermediate outputs.
- Add linear probes or auxiliary contrastive loss to intermediate features.

---

## 229. Late-Training Representations Align with Augmentations More Than Labels

**Symptoms:**
- Data points group by augmentation type even in labeled training.

**Suggested Next Steps:**
- Penalize view-specific components (e.g., dropout embeddings from weak/strong views).
- Add label-view alignment loss to prioritize semantic grouping.
- Limit augmentations post-certain epoch (controlled curriculum).

---

## 230. Increasing Model Capacity Hurts Representation Diversity

**Symptoms:**
- Wider/deeper models learn fewer distinct directions (lower rank).

**Suggested Next Steps:**
- Increase regularization on final layers (e.g., covariance loss, feature whitening).
- Track feature rank, cosine similarity, and entropy across model widths.
- Use bottlenecks or drop-blocks in deep networks to force diversity.

---

## 231. Self-Attention Maps Oscillate or Flip Across Epochs

**Symptoms:**
- Attention heads change drastically between checkpoints even with similar loss.

**Suggested Next Steps:**
- Add temporal attention consistency loss between epochs (or EMA-based regularizer).
- Reduce learning rate of attention parameters specifically.
- Visualize and track head-wise attention entropy over time.

---

## 232. Gradients Collapse in First Few Layers Despite Loss Drop

**Symptoms:**
- Early conv/transformer layers receive ~0 gradient norm.

**Suggested Next Steps:**
- Apply layer-wise learning rate scaling (larger LR for shallow layers).
- Add skip connections from early features to downstream modules.
- Use input-level noise injection or feature shaking to reengage early layers.

---

## 233. Latents Shift Too Rapidly When Model Size Is Increased

**Symptoms:**
- Larger models introduce chaotic drift in latent space even with same training setup.

**Suggested Next Steps:**
- Tune optimizer momentum and warmup schedule for larger capacity.
- Add adaptive weight norm regularization or trust-region clipping.
- Visualize epoch-to-epoch latent cosine similarity across model scales.

---

## 234. Pretraining Optimizes for Alignment but Hurts Transferability

**Symptoms:**
- Latents align perfectly across views, but fail on downstream classification.

**Suggested Next Steps:**
- Reduce alignment strength (e.g., cosine loss weight).
- Introduce view-specific variance loss or semantic structure constraints.
- Add third view or cluster anchor to reintroduce representation diversity.

---

## 235. LayerNorm in Projector Causes Latent Collapse Late in Training

**Symptoms:**
- Collapse occurs near convergence and is traced to norm layers.

**Suggested Next Steps:**
- Remove final LayerNorm in projector.
- Replace with affine LayerNorm or GroupNorm.
- Move normalization before MLP layers rather than after.

---

## 236. Token Representations Lose Positional Awareness Late in Training

**Symptoms:**
- All tokens cluster tightly, and sequence structure disappears.

**Suggested Next Steps:**
- Regularize with token-position prediction or distance-preserving contrastive loss.
- Add temporal or spatial jigsaw tasks.
- Visualize token similarity matrices across space/time and enforce diversity.

---

## 237. Prototype-Centered Representations Become Over-Clustered

**Symptoms:**
- All representations collapse tightly around a few anchors or clusters.

**Suggested Next Steps:**
- Apply entropy maximization on soft cluster assignments.
- Use repulsion loss between prototypes.
- Add a “pull-push” objective to enforce soft overlap while avoiding sharp collapse.

---

## 238. Learning Rate Plateau Prevents Escape from Poor Minima

**Symptoms:**
- Loss stagnates and model can’t escape suboptimal solution.

**Suggested Next Steps:**
- Use cyclical learning rate or restart scheduling.
- Perturb weights slightly to nudge out of basin (w ← w + ε, small noise).
- Inject gradient noise or apply lookahead optimizer to explore farther.

---

## 239. Latent Space Becomes Anisotropic in Final Epochs

**Symptoms:**
- One or two directions dominate variance; others vanish.

**Suggested Next Steps:**
- Add whitening regularization (e.g., off-diagonal covariance penalty).
- Apply eigenvalue entropy loss to encourage uniform spread.
- Normalize features, then apply contrastive/variance loss to avoid directional collapse.

---

## 240. Pretraining Loss Continues Dropping But Semantic Performance Worsens

**Symptoms:**
- Proxy objective keeps improving while downstream validation degrades.

**Suggested Next Steps:**
- Introduce semantic probes during pretraining (e.g., online linear probe).
- Apply multi-objective learning with auxiliary semantic constraints.
- Monitor alignment between feature space and ground truth labels over time.

---

## 241. Final Layers Show Increasing Norms but Decreasing Variance

**Symptoms:**
- Feature magnitudes grow, but spread across dimensions shrinks.

**Suggested Next Steps:**
- Normalize features before applying any objective (unit norm or LayerNorm).
- Add feature variance preservation loss.
- Use log-variance loss instead of raw covariance to detect saturation.

---

## 242. Representations Align Too Quickly in Contrastive Pretraining

**Symptoms:**
- Cosine similarity between views hits >0.99 early in training.

**Suggested Next Steps:**
- Decrease alignment loss weight, especially early on.
- Add decorrelation or variance loss in parallel to contrastive objective.
- Introduce curriculum scheduling to delay alignment.

---

## 243. Shared Encoder Becomes Biased Toward One Branch in Two-View Architectures

**Symptoms:**
- Online and target branches diverge in performance; shared encoder starts encoding one side’s signal more than the other.

**Suggested Next Steps:**
- Balance gradient flow with stop-gradient or inverse prediction paths.
- Apply symmetrized training (swap branches or use shared weight averaging).
- Add loss consistency penalty across branches.

---

## 244. Local Contrastive Objective Dominates Global Structure

**Symptoms:**
- Neighborhood contrast improves while inter-class spread is lost.

**Suggested Next Steps:**
- Apply multi-scale contrastive learning (patch-to-global and sample-to-sample).
- Add global clustering loss (e.g., SwAV, SCAN).
- Visualize embedding topology (e.g., UMAP or manifold curvature).

---

## 245. Linear Probe Accuracy Drops Midway Through Pretraining

**Symptoms:**
- Initially good semantic alignment becomes worse even as loss drops.

**Suggested Next Steps:**
- Add linear probe loss as auxiliary term.
- Use EMA encoder for linear probe evaluation instead of raw encoder.
- Adjust learning rate or dropout to reduce over-regularization.

---

## 246. Classifier Collapses Without Clear Signal in Feature Space

**Symptoms:**
- Downstream classifier outputs one or two classes confidently with little variance.

**Suggested Next Steps:**
- Apply entropy penalty on classifier output.
- Add label smoothing or sample-mixing augmentation (e.g., MixUp).
- Reinitialize and monitor the last layer weights and their update norms.

---

## 247. Different Feature Layers Disagree Strongly on Prediction

**Symptoms:**
- Deeper layers predict different class than earlier ones, especially in SSL transfer.

**Suggested Next Steps:**
- Use deep supervision — apply auxiliary heads to intermediate layers.
- Fuse multiple layer outputs (e.g., layer-wise weighted average).
- Apply temporal consistency across layer predictions.

---

## 248. Combined SSL + Supervised Training Leads to Feature Drift

**Symptoms:**
- Encoder trained jointly starts losing contrastive properties when supervised signal increases.

**Suggested Next Steps:**
- Use loss annealing — ramp down SSL objective gradually.
- Train with multi-task heads and allow independent decoders.
- Measure inter-feature angle stability before/after fine-tuning steps.

---

## 249. Representations Become Label-Specific But Non-Linearly Separable

**Symptoms:**
- t-SNE shows label-based grouping, but linear probe fails.

**Suggested Next Steps:**
- Apply supervised contrastive loss to enforce margin.
- Increase embedding dimensionality or add nonlinear projection head.
- Add whitening loss to encourage axis-aligned separation.

---

## 250. Latent Space Shows Structured Collapse (e.g., ring, cube, or lattice artifacts)

**Symptoms:**
- PCA/t-SNE plots show geometric artifacts instead of natural clusters.

**Suggested Next Steps:**
- Remove positional encoding artifacts or spatial biases in augmentations.
- Apply noise injection to feature space during training.
- Add isotropy regularization, e.g., maximize entropy over projection directions.

---


More is in progress! Stay tuned!

