# SSL-1000-Fixes

Aimed at supporting the systematic study and refinement of **self-supervised learning workflows**, this compilation includes over 1,000 **LLM-generated** potential issues and suggested next steps. It is shared for use by deep learning researchers following their own review and judgment, and to invite expert feedback and evaluation.

**Note:** This list was generated by an LLM with human supervision but has not been fully reviewed by domain experts after generation. It is intended to spark ideas and highlight possible directions to explore. **Please critically evaluate any items you plan to experiment with, and consider consulting domain experts before incorporating them into your work.**

---


## 1. Representation Rank is Low

**Symptoms:**
- Low covariance matrix rank or eigenvalue entropy.

**Suggested Next Steps:**
- Increase the variance loss coefficient (e.g., in VICReg).
- Increase the embedding dimension (if too narrow).
- Increase batch size to capture more variation.
- Use stronger augmentations to diversify views.
- Add residual connections to encourage diversity in output.

---

## 2. Learned Kernels Are Noisy or Dead

**Symptoms:**
- Kernels look random or many are close to zero.

**Suggested Next Steps:**
- Lower the learning rate or add gradient clipping.
- Apply weight decay or L1 regularization on conv weights.
- Inspect data normalization — ensure it’s not too aggressive.
- Add initialization schemes like Kaiming or orthogonal init.

---

## 3. Embeddings Collapse or Are Too Similar

**Symptoms:**
- Cosine similarity between features too high, low variance.

**Suggested Next Steps:**
- Add or strengthen variance regularization.
- Introduce weak asymmetry (e.g., stop gradient, predictor MLP).
- Delay EMA (Exponential Moving Average) updates if using target networks.
- Increase prediction horizon to reduce trivial matching.

---

## 4. Early Overfitting or Poor Generalization

**Symptoms:**
- Training loss drops, but validation loss rises quickly.

**Suggested Next Steps:**
- Add dropout, augmentations, or weight decay.
- Check if the model is memorizing shortcuts (like pixel positions).
- Use early stopping or learning rate warmup.
- Reduce model capacity or simplify the task.

---

## 5. Feature Norms Are Highly Uneven

**Symptoms:**
- Some features dominate (e.g., 2–3 features have high norms).

**Suggested Next Steps:**
- Normalize features before loss computation.
- Strengthen covariance decorrelation loss.
- Use BatchNorm or LayerNorm in the projector or encoder.
- Consider whitening transforms in analysis.

---

## 6. Covariance Matrix is Highly Correlated

**Symptoms:**
- High off-diagonal energy, many features redundant.

**Suggested Next Steps:**
- Increase covariance loss coefficient (e.g., VICReg’s nu).
- Reduce use of overlapping augmentations that induce similarity.
- Increase depth or non-linearity in encoder/projector.

---

## 7. Eigenvalues Decay Too Quickly

**Symptoms:**
- Only top 5–10 components explain most variance.

**Suggested Next Steps:**
- Increase batch diversity or apply feature decorrelation.
- Try orthogonality constraints (e.g., QR loss or mutual information maximization).
- Apply contrastive loss with negative samples.

---

## 8. Prediction Errors Are Localized or Systematic

**Symptoms:**
- Specific regions or classes consistently fail.

**Suggested Next Steps:**
- Analyze dataset imbalance or sample quality.
- Add local receptive field diversity (e.g., dilated convolutions).
- Train with curriculum or targeted augmentation on weak regions.

---

## 9. Latent Space Visualization Shows Poor Separation

**Symptoms:**
- t-SNE or PCA shows all points clumped together.

**Suggested Next Steps:**
- Improve augmentation diversity to avoid trivial alignment.
- Add auxiliary tasks (e.g., rotation prediction or jigsaw).
- Increase embedding dimensionality or training time.

---

## 10. Gradient Norms Are Exploding or Vanishing

**Symptoms:**
- Training becomes unstable, NaNs in loss, or gradients ~0.

**Suggested Next Steps:**
- Use gradient clipping (e.g., clip value or clip norm).
- Check for ReLU dead units — try LeakyReLU or GELU.
- Switch to Adam or RMSprop with proper eps setting.
- Use smaller learning rate or mixed-precision training with GradScaler.

---

## 11. Loss Plateaus Early

**Symptoms:**
- Loss stops improving in early epochs.

**Suggested Next Steps:**
- Use a learning rate scheduler (StepLR, CosineAnnealing, etc.).
- Add warmup steps at the start.
- Increase model capacity (width or depth).
- Check for data leakage or label noise if applicable.

---

## 12. Very Low Output Variance

**Symptoms:**
- All output features are near zero or constant across samples.

**Suggested Next Steps:**
- Initialize final layer biases to small non-zero values.
- Use LayerNorm in the output head.
- Regularize output to have unit variance.
- Inspect input normalization and activation function saturation.

---

## 13. Poor Transfer to Downstream Task

**Symptoms:**
- Learned representations perform poorly on supervised tasks.

**Suggested Next Steps:**
- Introduce task-aware pretext signals.
- Add semantic consistency loss if applicable.
- Use larger datasets or pretrained backbones.
- Fine-tune with a small supervised head to guide representation.

---

## 14. Slow Training

**Symptoms:**
- Training epochs take too long.

**Suggested Next Steps:**
- Optimize data loading (e.g., increase num_workers, use .pin_memory()).
- Use mixed precision or channels_last format.
- Profile with torch.profiler or nvprof to locate bottlenecks.
- Chunk long sequences if you’re doing sequential modeling.

---

## 15. BatchNorm or LayerNorm Acting Erratically

**Symptoms:**
- Normalization hurts performance or destabilizes training.

**Suggested Next Steps:**
- Try GroupNorm or InstanceNorm instead.
- Replace with no normalization and tune learning rate carefully.
- If using DDP: ensure BatchNorm is synchronized (SyncBatchNorm).

---

## 16. Representations Are Nearly Identical Across Augmented Views

**Symptoms:**
- High cosine similarity between outputs from different views.

**Suggested Next Steps:**
- Add stochastic layers (e.g., dropout) before the projector.
- Increase augmentation intensity (e.g., stronger color jitter, masking).
- Add temporal or spatial shift if applicable (e.g., for video or sequence data).

---

## 17. Checkpoint Sizes Are Too Large

**Symptoms:**
- Disk fills up quickly or model is hard to share.

**Suggested Next Steps:**
- Save only model weights, not full optimizer states.
- Use state_dict() filtering to exclude unneeded buffers.
- Save in float16 if inference is not affected.

---

## 18. Embedding Dimensions Used Unevenly

**Symptoms:**
- Some dimensions are always close to 0, others dominate.

**Suggested Next Steps:**
- Add uniformity loss or entropy maximization on features.
- Apply decorrelation or orthogonality constraints.
- Use learned positional encoding or non-linear projectors.

---

## 19. EMA (Exponential Moving Average) Target Doesn’t Help

**Symptoms:**
- EMA-targeted representations don’t stabilize or improve.

**Suggested Next Steps:**
- Reduce the EMA decay at early epochs (e.g., start at 0.9 and anneal to 0.99).
- Re-initialize EMA with online model every few epochs (experimental).
- Try using fixed encoder instead of EMA to test isolation.

---

## 20. Visualization Metrics Don’t Change Over Epochs

**Symptoms:**
- PCA, cosine similarity, rank, or eigenvalue plots are flat.

**Suggested Next Steps:**
- Increase the training duration.
- Visualize per-class metrics or region-specific patterns.
- Log metrics after more granular intervals (e.g., every few batches).
- Inspect if loss is still changing — maybe the model is learning but not diversifying representations.

---

## 21. Feature Activations Are Sparse or Mostly Zero

**Symptoms:**
- Most features in intermediate layers are zero.

**Suggested Next Steps:**
- Replace ReLU with GELU, ELU, or Swish to allow negative flows.
- Add BatchNorm/LayerNorm before nonlinearity to stabilize activation spread.
- Lower the weight decay, which might be shrinking weights excessively.

---

## 22. Learned Representations Are Overly Sensitive to Noise

**Symptoms:**
- Small input perturbations drastically change output.

**Suggested Next Steps:**
- Add consistency loss (e.g., L2 between outputs of noisy inputs).
- Use adversarial training or noise augmentations.
- Try input denoising autoencoder or robust contrastive objectives.

---

## 23. Training Is Stochastic or Unstable Across Runs

**Symptoms:**
- Large performance variation between seeds.

**Suggested Next Steps:**
- Set all seeds (torch, numpy, random) and enable `torch.backends.cudnn.deterministic = True`.
- Use larger batch sizes or gradient accumulation.
- Tune EMA decay and optimizer momentum for stability.

---

## 24. Model Predicts Only the Majority Class or Background

**Symptoms:**
- Class imbalance in predictions.

**Suggested Next Steps:**
- Use class-balanced sampling or weighted loss functions.
- Over-sample minority classes or under-sample majority ones.
- Add focal loss or label smoothing to improve calibration.

---

## 25. Model Performs Well Quantitatively but Fails Visually

**Symptoms:**
- High accuracy but poor qualitative results (e.g., in reconstructions or predictions).

**Suggested Next Steps:**
- Add perceptual loss or structural similarity index (SSIM).
- Visualize feature maps or intermediate outputs to detect bottlenecks.
- Investigate if model is overfitting shortcut features.

---

## 26. High Feature Redundancy Across Time or Space

**Symptoms:**
- Features don’t evolve across spatial/temporal positions.

**Suggested Next Steps:**
- Use multi-scale features or dilated convolutions.
- Encourage diversity with contrastive prediction across time or space.
- Add shuffle-and-predict or jigsaw tasks to break local patterns.

---

## 27. Learned Embeddings Show Bias (e.g., class, texture, position)

**Symptoms:**
- Embeddings cluster by confounding factors.

**Suggested Next Steps:**
- Add domain adversarial training to suppress bias signals.
- Use class-conditioned augmentation or stratified sampling.
- Regularize with mutual information minimization between embeddings and bias.

---

## 28. Final Layer Activations Are Saturated

**Symptoms:**
- Activations stuck near ±1 (e.g., tanh or sigmoid saturation).

**Suggested Next Steps:**
- Use identity or linear activation in last layer for representation learning.
- Normalize final layer outputs before loss (e.g., L2 norm for cosine similarity).
- Replace sigmoid/tanh with scaled versions or avoid them altogether.

---

## 29. Latent Space Changes Are Too Abrupt Between Epochs

**Symptoms:**
- Drastic shift in embedding space across epochs.

**Suggested Next Steps:**
- Reduce learning rate or use cosine annealing.
- Stabilize training with EMA targets or slower scheduler decay.
- Visualize trajectory of embeddings to identify jump points.

---

## 30. Too Many Metrics Make it Hard to Interpret

**Symptoms:**
- Overwhelming WandB or CSV logs with unclear insight.

**Suggested Next Steps:**
- Group metrics into categories (e.g., loss/, rep/, kernels/).
- Plot composite metrics: entropy, rank, norm, diversity.
- Use EMA smoothing or epoch-wise summaries.

---

## 31. Feature Norms Grow Over Time

**Symptoms:**
- Feature magnitudes increase steadily, possibly causing instability.

**Suggested Next Steps:**
- Apply feature normalization (e.g., L2) before computing loss.
- Add a feature norm penalty to encourage compactness.
- Use BatchNorm or LayerNorm in deeper layers.

---

## 32. Learned Representations Are Too Task-Specific

**Symptoms:**
- Representations don’t transfer well or generalize poorly to new domains.

**Suggested Next Steps:**
- Use multi-task learning or domain generalization objectives.
- Apply domain-specific augmentations during pretraining.
- Encourage invariance to domain-specific features with auxiliary losses.

---

## 33. Loss Has High Variance Across Batches

**Symptoms:**
- Spiky loss curve within epochs.

**Suggested Next Steps:**
- Increase batch size or enable gradient accumulation.
- Use gradient clipping or smoother optimizers (e.g., RMSprop).
- Check for label noise, outliers, or data corruption.

---

## 34. Model Learns Trivial Identity Mapping

**Symptoms:**
- Autoencoder or predictor learns to copy input.

**Suggested Next Steps:**
- Increase prediction difficulty (e.g., masked inputs, farther temporal targets).
- Add bottlenecks (e.g., smaller latent size, dropout).
- Use contrastive or predictive coding objectives.

---

## 35. Cosine Similarities Between Embeddings Are All ~1

**Symptoms:**
- Representations collapse toward the same direction.

**Suggested Next Steps:**
- Introduce decorrelation loss (as in VICReg, Barlow Twins).
- Verify augmentations are not too weak (causing alignment to dominate).
- Add predictor asymmetry or stop-gradient operations.

---

## 36. Augmentations Make Model Too Invariant

**Symptoms:**
- Loss increases or performance drops after aggressive augmentation.

**Suggested Next Steps:**
- Tune augmentation intensity based on dataset.
- Use augmentation mixing (e.g., random select from strong/weak).
- Include augmentation-specific loss components (e.g., patch-wise consistency).

---

## 37. Temporal Representations Are Not Predictive

**Symptoms:**
- Predictive performance on future states is poor.

**Suggested Next Steps:**
- Add positional encoding or temporal convolutions.
- Use causal attention or masked convolutions for temporal structure.
- Predict multiple future steps instead of just one.

---

## 38. Spatial Attention Maps Are Too Uniform

**Symptoms:**
- No distinct focus regions in visual attention models.

**Suggested Next Steps:**
- Add attention diversity loss.
- Reduce model’s global pooling dependence.
- Use local self-attention or hierarchical attention blocks.

---

## 39. Embeddings Fail to Capture Class Separability

**Symptoms:**
- No clustering in embedding space despite supervised signals.

**Suggested Next Steps:**
- Add contrastive loss or triplet loss during supervised phase.
- Use center loss or ArcFace-style margins to pull apart classes.
- Visualize with t-SNE or UMAP to better interpret overlaps.

---

## 40. Embedding Space Is Not Smooth

**Symptoms:**
- Small changes in input lead to large jumps in embedding.

**Suggested Next Steps:**
- Apply Lipschitz regularization or Jacobian norm penalty.
- Use interpolation consistency training (e.g., MixUp, CutMix).
- Add a manifold smoothing loss between nearby embeddings.

---

## 41. Training Works but Validation Always Lags Behind

**Symptoms:**
- Validation performance never catches up to training.

**Suggested Next Steps:**
- Increase data augmentation or use test-time augmentation.
- Inspect for distribution shift between train/val datasets.
- Try label smoothing or confidence calibration losses.

---

## 42. Model Too Slow During Inference

**Symptoms:**
- Good training speed but slow forward pass at test time.

**Suggested Next Steps:**
- Replace inefficient layers (e.g., use depthwise separable convs).
- Use TorchScript, ONNX, or TensorRT to optimize inference.
- Prune or quantize model using PyTorch Quantization Toolkit.

---

## 43. Early Epochs Improve, Then Suddenly Diverge

**Symptoms:**
- Model trains initially then becomes unstable.

**Suggested Next Steps:**
- Reduce learning rate decay speed.
- Enable gradient norm tracking — add alerts for spikes.
- Monitor activation magnitudes to catch exploding outputs.

---

## 44. Loss Curve Shows Sudden Jumps or Drops

**Symptoms:**
- Sharp spikes in training/validation loss.

**Suggested Next Steps:**
- Check for batch corruption or augmentation bugs.
- Add gradient clipping to prevent numerical overflow.
- Re-initialize suspicious layers if only one head is unstable.

---

## 45. PCA Shows Linear Patterns, Not Clusters

**Symptoms:**
- Projected embeddings lie on a line or plane.

**Suggested Next Steps:**
- Increase embedding dimensionality or batch size.
- Add non-linearities or residual connections in the projector.
- Use multi-view losses (e.g., more than 2 views in contrastive learning).

---

## 46. Learned Filters Are Identical or Repeating

**Symptoms:**
- Several convolution filters are near-duplicates.

**Suggested Next Steps:**
- Use group convolutions or increase kernel diversity via dropout.
- Enforce low-rank decomposition or sparsity constraints.

---

## 47. Positional Encoding Seems Ineffective

**Symptoms:**
- Model ignores order or position in sequences.

**Suggested Next Steps:**
- Switch from sinusoidal to learned positional embeddings.
- Use relative position encoding (as in Transformer-XL).
- Add position prediction tasks to reinforce order awareness.

---

## 48. Embedding Norms Vary Greatly Across Batches

**Symptoms:**
- Some batches have much higher norms than others.

**Suggested Next Steps:**
- Add batch-wise norm regularization.
- Normalize at the feature level, not the batch level, before projection.
- Log norm histogram across multiple batches and average.

---

## 49. Feature Collapse Only Happens in One Branch

**Symptoms:**
- One encoder or projection path dominates.

**Suggested Next Steps:**
- Tune EMA decay or learning rates independently for branches.
- Add balance loss between online and target representations.
- Ensure gradient is not leaking through stop-gradient nodes.

---

## 50. Model Is Too Sensitive to Hyperparameters

**Symptoms:**
- Tiny hyperparameter changes cause big performance shifts.

**Suggested Next Steps:**
- Add regularization to reduce sensitivity.
- Use hyperparameter sweeps with logging (e.g., Optuna + WandB).
- Start with robust optimizers (AdamW, Lookahead, Lion).

---

## 51. Features Are Disentangled but Not Predictive

**Symptoms:**
- Latent dimensions are cleanly separated but don’t correlate with useful downstream variables.

**Suggested Next Steps:**
- Incorporate supervised fine-tuning on downstream tasks.
- Add mutual information maximization between features and labels.
- Apply causal feature selection to determine which representations matter.

---

## 52. Training Becomes Slower Over Time

**Symptoms:**
- Epoch time increases as training progresses.

**Suggested Next Steps:**
- Investigate data loader memory leaks or GPU fragmentation.
- Restart the job periodically or clear CUDA cache manually.
- Profile with torch.profiler to see if backward pass is bloating.

---

## 53. Final Layer Has Very High or Low Weight Norm

**Symptoms:**
- Final linear/projection layer absorbs too much or too little weight magnitude.

**Suggested Next Steps:**
- Apply weight norm penalty or L2 normalization on output.
- Consider weight normalization or spectral normalization.
- Use layer scaling (small init gain) to control output range.

---

## 54. Representations Work Only With Linear Probes

**Symptoms:**
- Linear classifiers perform well but non-linear downstream tasks fail.

**Suggested Next Steps:**
- Enrich pretraining with nonlinear objectives or auxiliary heads.
- Try representation fusion across layers (e.g., skip connections).
- Add nonlinear probes during analysis to reveal hidden structure.

---

## 55. Model Learns Shortcut Artifacts (e.g., borders, color)

**Symptoms:**
- Model overfits visual or statistical artifacts instead of semantic signals.

**Suggested Next Steps:**
- Use background substitution or style augmentation.
- Add patch mixing or region swapping.
- Train with adversarial examples to suppress shortcut reliance.

---

## 56. Highly Entangled Representations

**Symptoms:**
- Each dimension mixes multiple factors (e.g., pitch + rhythm in audio).

**Suggested Next Steps:**
- Apply factorized loss (e.g., predict rhythm separately).
- Use β-VAE, infoGAN, or total correlation minimization.
- Introduce structured bottlenecks to force separation.

---

## 57. Strong Representations but Weak Predictions

**Symptoms:**
- Intermediate features are rich, but final outputs are weak.

**Suggested Next Steps:**
- Inspect predictor head capacity — may need more depth or nonlinearity.
- Use intermediate supervision (deep supervision).

---

## 58. Embeddings Are Invariant to Too Many Transformations

**Symptoms:**
- Model ignores differences that are actually important (e.g., time shift, expression, class).

**Suggested Next Steps:**
- Refine augmentations: make only task-irrelevant ones invariant.
- Use contrastive pairs with task-relevant variation.
- Add equivariance constraints instead of full invariance.

---

## 59. Loss Improves but Evaluation Metric Stalls

**Symptoms:**
- Loss function keeps decreasing, but downstream metrics (e.g., accuracy, F1, NMI) do not.

**Suggested Next Steps:**
- Double-check that loss correlates with the true objective.
- Use evaluation metrics as auxiliary loss terms.
- Replace MSE with contrastive or ranking losses that better reflect task needs.

---

## 60. Model Struggles With Out-of-Distribution (OOD) Inputs

**Symptoms:**
- Good performance on in-distribution data but fails on minor shifts.

**Suggested Next Steps:**
- Train with domain randomization or style transfer.
- Use Outlier Exposure or self-supervised OOD detection.
- Apply Bayesian or ensemble methods to estimate uncertainty.

---

## 61. Model Quickly Memorizes Training Set

**Symptoms:**
- Training accuracy reaches 100% rapidly, validation stagnates.

**Suggested Next Steps:**
- Increase data augmentation and randomization.
- Apply label smoothing to reduce overconfidence.
- Add dropout, cutout, or mixup to regularize learning.

---

## 62. Model Fails to Learn From Rare Examples

**Symptoms:**
- Minority class or edge-case inputs are consistently mispredicted.

**Suggested Next Steps:**
- Apply oversampling or loss reweighting (e.g., focal loss).
- Use curriculum learning to emphasize rare samples over time.
- Introduce contrastive pairs between rare and common instances.

---

## 63. Model’s Confidence Is Always High

**Symptoms:**
- Predicts with near-certain confidence, even when wrong.

**Suggested Next Steps:**
- Add confidence penalty or entropy maximization regularizer.
- Apply temperature scaling or Bayesian uncertainty estimation.
- Train with label noise or adversarial samples to soften confidence.

---

## 64. Representations Have High Mutual Information With Augmentations

**Symptoms:**
- Embeddings leak augmentation type (e.g., rotation, color jitter).

**Suggested Next Steps:**
- Add augmentation-invariance loss (e.g., contrastive across augment types).
- Mask augmentation identity from predictor.
- Include augmentation prediction task and subtract gradients (as in adversarial multitask setups).

---

## 65. Projection Head Is Bottlenecking Learning

**Symptoms:**
- Increasing projection head depth reduces performance.

**Suggested Next Steps:**
- Try skip connections across projection layers.
- Reduce bottleneck dimension only after sufficient training.
- Move regularization from projection head to encoder directly.

---

## 66. Layer Outputs Are Identical Across Samples

**Symptoms:**
- Intermediate activations are static for different inputs.

**Suggested Next Steps:**
- Use batch-dependent normalization (BatchNorm or GroupNorm).
- Replace static convolutions with dynamic or conditional layers.
- Check for frozen parameters or misinitialized layers.

---

## 67. Kernel Weights Are All Small in Magnitude

**Symptoms:**
- Kernels converge to very low norm, model becomes underpowered.

**Suggested Next Steps:**
- Add weight magnitude regularization (maximize norm).
- Remove or reduce L2 regularization if too aggressive.
- Track gradient norms per layer to ensure flow is active.

---

## 68. Representation Similarity Across Batches Is Too High

**Symptoms:**
- Features learned from different batches are overly aligned.

**Suggested Next Steps:**
- Add batch decorrelation loss.
- Use batch shuffling to reduce alignment bias.
- Compare representations across augment sets, not just batch samples.

---

## 69. Final Output Layer Overfits the Loss Function

**Symptoms:**
- Very low loss, but meaningless or misleading predictions.

**Suggested Next Steps:**
- Replace final linear layer with nonlinear or gated alternatives.
- Regularize final layer weights (e.g., spectral norm).
- Detach loss target partially (e.g., stop gradient halfway through).

---

## 70. Self-Supervised Learning Works Only on Specific Data Formats

**Symptoms:**
- SSL succeeds on spectrograms but fails on raw waveforms, etc.

**Suggested Next Steps:**
- Use multi-view SSL across formats (e.g., CQT + waveform).
- Add domain adapters to bridge modalities.
- Train with cross-modal prediction or co-training tasks.

---

## 71. Symmetric Architectures Cause Collapse

**Symptoms:**
- Identical encoder/predictor/projector in both branches leads to collapse in SSL.

**Suggested Next Steps:**
- Introduce asymmetry via:
  - Extra MLP layer in one branch
  - Predictor module in only the online branch
  - EMA update in one branch only (BYOL-style)
- Add stop-gradient on the target branch.

---

## 72. Projector Output Has No Meaningful Structure

**Symptoms:**
- Projector output is unstructured even if encoder is rich.

**Suggested Next Steps:**
- Visualize encoder vs projector PCA — if projector collapses, simplify or remove layers.
- Use skip connection from encoder to projector output.
- Regularize projector with whitened contrastive losses or decorrelation terms.

---

## 73. Model Learns Only Global Features

**Symptoms:**
- Model ignores local textures or positional signals.

**Suggested Next Steps:**
- Add local prediction tasks (e.g., patch-level contrastive loss).
- Use convolutions or transformer tokens with localized attention.
- Predict patch order, rotation, or center-surround relations.

---

## 74. Loss Gradually Increases Instead of Decreasing

**Symptoms:**
- Loss slowly rises during training (e.g., with large batch sizes).

**Suggested Next Steps:**
- Decrease learning rate or switch to adaptive optimizers like AdamW.
- Try LayerScale or zero-initialized residuals to stabilize.
- Warmup learning rate more slowly or for longer.

---

## 75. All Embedding Norms Converge to the Same Value

**Symptoms:**
- Features saturate to same L2 norm across batch.

**Suggested Next Steps:**
- Add variance loss to push norm diversity (e.g., VICReg style).
- Apply norm-aware augmentation (perturb inputs by feature direction).
- Disable normalization layers temporarily to test effect.

---

## 76. Self-Supervised Model Ignores Temporal Order

**Symptoms:**
- Sequence models don't distinguish between orderings.

**Suggested Next Steps:**
- Add permutation prediction task.
- Use relative position encoding (instead of absolute).
- Predict future representations, not just contrast.

---

## 77. Attention Maps Are Uniform or Random

**Symptoms:**
- Self-attention fails to focus on meaningful regions.

**Suggested Next Steps:**
- Add auxiliary attention supervision if labels available (e.g., saliency).
- Use local/global attention hybrids (e.g., Swin, deformable attention).
- Regularize attention maps with entropy loss or diversity loss across heads.

---

## 78. Output Depends Heavily on Positional Encoding

**Symptoms:**
- Removing positional encodings drastically drops performance.

**Suggested Next Steps:**
- Use learned relative positions instead of absolute.
- Introduce data augmentations that shuffle position to test robustness.
- Add position prediction as an auxiliary task.

---

## 79. Loss Oscillates in Later Epochs

**Symptoms:**
- Noisy up-down loss in late training.

**Suggested Next Steps:**
- Try lowering the learning rate or switch to a smoother optimizer.
- Use Poly decay or Cosine Annealing with restarts.
- Check for numerical instability or NaNs in gradients.

---

## 80. First Few Layers Are Frozen in Learning

**Symptoms:**
- Weights of early layers barely change.

**Suggested Next Steps:**
- Add layer-wise learning rates — increase early layer LR.
- Add skip connections to force gradient flow.
- Inspect input normalization or weight scale mismatch.

---

## 81. Embeddings Are Nearly Binary (Hard Clustering)

**Symptoms:**
- Output features are ~0 or ~1, like hard assignment.

**Suggested Next Steps:**
- Add entropy maximization or softmax temperature tuning.
- Introduce noise in the representation (e.g., Gaussian noise).
- Reduce use of sharpening functions (e.g., hard Gumbel-softmax).

---

## 82. Representations Vary Too Much Across Augmentations

**Symptoms:**
- Representations for the same image under augmentations are too dissimilar.

**Suggested Next Steps:**
- Add alignment loss (L2/cosine between augmented views).
- Reduce augmentation strength (especially when starting training).
- Use temporal averaging or EMA smoothing across augmentations.

---

## 83. Cosine Similarity Between Batches Is Too Low

**Symptoms:**
- Batches of features don’t align at all — model is unstructured.

**Suggested Next Steps:**
- Reduce model capacity to prevent random exploration.
- Add soft clustering loss or global contrastive anchors.
- Inspect for data shuffling bugs causing semantic mismatch.

---

## 84. Model Stops Responding to New Data

**Symptoms:**
- Introducing new data doesn’t affect loss or features.

**Suggested Next Steps:**
- Check for batch normalization stats — may have saturated.
- Increase learning rate temporarily to overcome flat region.
- Reset optimizer state or remove stale EMA tracking.

---

## 85. KL Divergence or InfoNCE Loss is Always Near Zero

**Symptoms:**
- Representation space is uniform or trivially aligned.

**Suggested Next Steps:**
- Reduce use of strong priors (e.g., strong regularization or layer norm).
- Add negative sampling diversity (e.g., memory banks or cross-batch).
- Add temperature tuning in softmax.

---

## 86. Model Performance Degrades After Pretraining

**Symptoms:**
- Transfer performance is worse than random init.

**Suggested Next Steps:**
- Pretrain with task-relevant augmentations or sampling.
- Check if encoder is overfitted to self-supervised proxy task.
- Try partial freezing or reinitializing the final layers during finetuning.

---

## 87. BatchNorm Statistics Drift Wildly Between Epochs

**Symptoms:**
- Sudden jumps in feature norms or accuracy.

**Suggested Next Steps:**
- Replace with GroupNorm or LayerNorm for more stability.
- Switch to SyncBatchNorm in multi-GPU setups.
- Reduce batch size but accumulate stats across steps manually.

---

## 88. PCA Shows Embedding Collapse Only in One Class

**Symptoms:**
- One class has low-variance collapsed representations.

**Suggested Next Steps:**
- Apply class-conditional VC regularization (only penalize collapse inside each class).
- Introduce class-aware augmentations.
- Visualize feature spread per class and adjust sampling accordingly.

---

## 89. Loss Minimizes Too Fast in First Epoch

**Symptoms:**
- Sudden drop in loss without generalization gain.

**Suggested Next Steps:**
- Increase warmup steps for optimizer.
- Try cyclical learning rate to recover from bad convergence basin.
- Track gradient norms across layers — signs of early saturation.

---

## 90. Gradients Are Nonzero but Do Not Change Parameters

**Symptoms:**
- Model reports active gradients, but weights stay almost unchanged.

**Suggested Next Steps:**
- Check for gradient clipping too tight (e.g., clip_value=0.1).
- Inspect optimizer momentum or weight decay conflicts.
- Log parameter delta norms per update to confirm actual change.

---
## 91. Same Features Are Activated Across All Classes

**Symptoms:**
- Activations in final or intermediate layers are class-agnostic.

**Suggested Next Steps:**
- Introduce class-specific loss components (e.g., center loss, ArcFace).
- Add conditional projection heads.
- Visualize per-class activation heatmaps to detect overlaps.

---

## 92. Model Fails After Moving to Mixed Precision

**Symptoms:**
- Loss explodes or underflows when using AMP.

**Suggested Next Steps:**
- Inspect for instabilities in small-scale activations.
- Use `GradScaler()` properly and log scale overflow events.
- Temporarily disable AMP on sensitive layers (e.g., batch norm).

---

## 93. Outputs Saturate at Specific Values (e.g., 0 or 1)

**Symptoms:**
- Model output distribution collapses to a small range.

**Suggested Next Steps:**
- Remove final activation function if unnecessary (e.g., sigmoid before contrastive loss).
- Check for label encoding mismatch (e.g., float vs int targets).
- Clip or rescale labels if they cause saturation in early loss curves.

---

## 94. Per-layer Gradient Norms Are Inverted (later layers have larger gradients)

**Symptoms:**
- Downstream layers receive stronger updates than upstream.

**Suggested Next Steps:**
- Use grad norm balancing or gradient scaling hooks.
- Add skip connections to improve gradient flow.
- Consider progressive layer unfreezing if using transfer learning.

---

## 95. Self-Supervised Representations Are Too Fine-Grained

**Symptoms:**
- Model overfocuses on textures or local details, underperforms on generalization.

**Suggested Next Steps:**
- Apply style randomization or texture-blind augmentations.
- Include global context prediction tasks.
- Penalize local redundancy using contrastive patch-to-global loss.

---

## 96. Early Layers Dominate Kernel Norms

**Symptoms:**
- Weight energy concentrates in first 1–2 conv layers.

**Suggested Next Steps:**
- Normalize all kernel norms per layer.
- Add layer-wise weight regularization to flatten energy profile.
- Visualize kernel statistics to track norm growth over time.

---

## 97. Weight Norms Grow Without Bound

**Symptoms:**
- Unchecked weight magnitude increases even with good validation metrics.

**Suggested Next Steps:**
- Use weight decay, trust region methods, or adaptive optimizers.
- Add explicit norm constraint loss (e.g., L2 penalty > target threshold).
- Normalize weights before output projection.

---

## 98. Similar Input Samples Lead to Divergent Outputs

**Symptoms:**
- Small variations in inputs cause inconsistent predictions.

**Suggested Next Steps:**
- Add consistency regularization (e.g., Mean Teacher or R-Drop).
- Apply feature noise injection and penalize output divergence.
- Check if input preprocessing is unstable (e.g., resizing artifacts).

---

## 99. Feature Channels Are Imbalanced

**Symptoms:**
- Some feature channels are always high or dead across all data.

**Suggested Next Steps:**
- Visualize channel-wise statistics (mean, std, activation frequency).
- Apply channel dropout or learnable channel gating.
- Add channel-wise decorrelation loss (e.g., as in Barlow Twins).

---

## 100. Augmentation Diversity Saturation Reduces Learning Signal

**Symptoms:**
- Weak or overly similar augmentations cause different views of the same input to produce nearly identical features too early in training, reducing learning signal.

**Suggested Next Steps:**
- Increase augmentation diversity or strength (e.g., mix strong+weak views).
- Use view-based dropout or masking (ViewDrop, PatchDrop) to disrupt easy alignment.
- Add a variance constraint or contrastive regularization across views to maintain diversity.

---

## 101. Model Learns Slowly Despite Good Architecture

**Symptoms:**
- Loss decreases very slowly even with powerful architectures.

**Suggested Next Steps:**
- Warm up with easier proxy tasks (e.g., autoencoding or next-patch prediction).
- Apply layer-wise adaptive learning rates (e.g., LARS or Ranger).
- Check that gradients are not being silently clipped or nan-ed.

---

## 102. Training Collapses When Switching Dataset

**Symptoms:**
- Model works on one dataset but diverges or collapses on another.

**Suggested Next Steps:**
- Check if input normalization statistics need adjusting.
- Log label distributions, length distributions, or input ranges.
- Pretrain on the new dataset using a simplified task, then fine-tune.

---

## 103. Representations Are Not Smooth in Latent Space

**Symptoms:**
- Interpolating between embeddings leads to unnatural outputs.

**Suggested Next Steps:**
- Use variational bottlenecks (VAE-style).
- Add latent interpolation consistency loss.
- Train with interpolation-based contrastive learning (e.g., MixCo, InterCLR).

---

## 104. Training Requires Frequent Restarts

**Symptoms:**
- Model gets stuck in poor minima unless manually restarted.

**Suggested Next Steps:**
- Use cyclical learning rates or random restarts as part of training.
- Consider sharpness-aware minimization (SAM) to flatten loss landscape.
- Add noise injection to weights or activations during training.

---

## 105. Feature Collapse Only Happens in Subgroups

**Symptoms:**
- Certain classes, time segments, or data sources show low-rank features.

**Suggested Next Steps:**
- Apply group-wise VC regularization (e.g., per class, per domain).
- Visualize feature diversity conditioned on subgroup identity.
- Use mixture-of-experts or domain-specific branches.

---

## 106. Latent Variables Show Strong Mutual Redundancy

**Symptoms:**
- High mutual information between feature dimensions.

**Suggested Next Steps:**
- Apply total correlation regularization (e.g., in β-TCVAE).
- Use PCA whitening or Barlow Twins-style decorrelation loss.
- Introduce structured sparsity to push diversity across features.

---

## 107. Encoder Layers Are Bypassed

**Symptoms:**
- Final features ignore early-layer representations.

**Suggested Next Steps:**
- Add skip connections and supervise intermediate outputs.
- Penalize activation redundancy across layers.
- Replace deep linear chains with multi-scale feature fusion.

---

## 108. Noise Sensitivity Appears Only During Inference

**Symptoms:**
- Model behaves robustly during training but is brittle at test time.

**Suggested Next Steps:**
- Add test-time augmentation (e.g., augment-and-aggregate).
- Perform Monte Carlo dropout or ensembling during inference.
- Train with distributional robustness objectives.

---

## 109. Positional Encodings Dominate Early Training

**Symptoms:**
- Position information overwhelms content features.

**Suggested Next Steps:**
- Decay position encodings over time (e.g., annealed strength).
- Switch to relative position encoding.
- Add auxiliary tasks that rely solely on content, not position.

---

## 110. Model Can’t Differentiate Easy from Hard Samples

**Symptoms:**
- Accuracy is uniform across all sample types (easy/hard, frequent/rare).

**Suggested Next Steps:**
- Add difficulty-aware losses (e.g., focal loss, margin-based reweighting).
- Use self-paced learning or curriculum learning.
- Label easy vs. hard examples with a confidence head.

---

## 111. Model Learns Dataset-Specific Artifacts

**Symptoms:**
- Performs well on training set, but fails on slightly restructured data (e.g., shifted, rescaled, renamed classes).

**Suggested Next Steps:**
- Augment with synthetic transformations or adversarial corruption.
- Apply domain randomization (e.g., color, blur, resolution).
- Add artifact suppression loss (e.g., decorrelate background).

---

## 112. Embedding Similarity Is Not Transitive

**Symptoms:**
- A ≈ B and B ≈ C, but A ≠ C in latent space.

**Suggested Next Steps:**
- Apply triplet loss or relational contrastive loss to enforce transitivity.
- Introduce semantic anchors (e.g., prototype centers).
- Try contrastive clustering frameworks (e.g., SwAV, DeepCluster).

---

## 113. Model Improves Accuracy but Forgets Earlier Examples

**Symptoms:**
- Accuracy goes up overall but drops on previously mastered samples.

**Suggested Next Steps:**
- Use Elastic Weight Consolidation (EWC) or LwF to retain past knowledge.
- Add memory replay buffer for prior batch samples.
- Introduce importance-weighted regularization on older activations.

---

## 114. Features Are Over-Aligned Across Classes

**Symptoms:**
- Representations for different classes fall into same regions.

**Suggested Next Steps:**
- Add class-conditional contrastive terms.
- Apply Supervised Contrastive Learning (SupCon).
- Add margin penalties between class centers.

---

## 115. Model Fails to Learn Abstract Patterns

**Symptoms:**
- Learns pixel-level or token-level structure, but not higher-level concepts.

**Suggested Next Steps:**
- Add hierarchical prediction objectives (e.g., segment-level prediction).
- Use multi-level pooling and cross-scale attention.
- Introduce task-aware augmentation (e.g., mask shapes, object removal).

---

## 116. Latent Space Is Not Interpretable

**Symptoms:**
- Difficult to associate any axis or region with meaningful behavior.

**Suggested Next Steps:**
- Apply disentanglement constraints (e.g., β-VAE, InfoGAN).
- Add axis alignment loss (match specific dimensions to interpretable features).
- Use linear probing or mutual information diagnostics on each axis.

---

## 117. Contrastive Loss Doesn’t Improve After Few Epochs

**Symptoms:**
- InfoNCE or triplet loss stagnates early.

**Suggested Next Steps:**
- Increase batch size or introduce cross-batch memory.
- Add hard negative mining or false positive suppression.
- Anneal temperature parameter to sharpen contrast.

---

## 118. Some Augmentations Hurt More Than Help

**Symptoms:**
- Certain transforms consistently degrade performance.

**Suggested Next Steps:**
- Log augmentation-wise performance (Ablation).
- Weight augmentations dynamically via learned augmentation policy (e.g., RandAugment, CTAugment).
- Use augmentation mixing or adaptive sampling.

---

## 119. Feature Collapse Occurs Only With Certain Optimizers

**Symptoms:**
- Model collapses only with, e.g., SGD but not Adam.

**Suggested Next Steps:**
- Tune momentum and weight decay separately per optimizer.
- Try Lookahead or LION optimizers for stability.
- Log parameter norm vs. gradient norm for each optimizer.

---

## 120. Embedding Space Is Dominated By Low-Level Statistics

**Symptoms:**
- PCA or t-SNE clusters reflect color, brightness, or length — not semantics.

**Suggested Next Steps:**
- Add style-invariant losses or cross-style contrastive training.
- Use domain adversarial training to remove low-level factors.
- Balance dataset by stratified sampling across confounds.

---

## 121. Representations Shift Over Epochs Without Improving Performance

**Symptoms:**
- Embeddings change (tracked via cosine sim, PCA, etc.), but validation accuracy or loss doesn’t improve.

**Suggested Next Steps:**
- Apply stability-promoting regularization (e.g., Temporal Consistency Loss).
- Add EMA smoothing to stabilize representations.
- Visualize class-wise or token-wise drift to localize the instability.

---

## 122. Multiple Augmented Views Collapse to the Same Point

**Symptoms:**
- Despite different views, embeddings end up nearly identical.

**Suggested Next Steps:**
- Increase augmentation strength, but diversify types (e.g., not just color + crop).
- Add view-specific noise or stochastic depth.
- Ensure contrastive or predictive loss compares only positive pairs, not self-matching.

---

## 123. Representations Encode Unintended Metadata (e.g., filename, sequence ID)

**Symptoms:**
- Clusters emerge based on metadata, not content.

**Suggested Next Steps:**
- Strip or randomize metadata at load time.
- Add confounder adversarial head to remove leaked info.
- Use information bottleneck or dropout on metadata paths.

---

## 124. Representation Collapse Only Happens With Gradient Accumulation

**Symptoms:**
- Training without accumulation is fine, but collapse appears when gradients are accumulated.

**Suggested Next Steps:**
- Reduce accumulation steps or scale learning rate per accumulation cycle.
- Normalize loss per micro-batch, not macro-batch.
- Track loss per step, not per epoch, to diagnose mismatch.

---

## 125. Loss Decreases on Synthetic Data but Not on Real Data

**Symptoms:**
- Strong learning signals on generated or proxy data, no generalization to real-world data.

**Suggested Next Steps:**
- Use domain adaptation loss (e.g., CORAL, MMD, domain confusion).
- Blend synthetic and real with style transfer, cycle consistency, or shared encoder.
- Add real-vs-synthetic discrimination loss to control blending.

---

## 126. Final Embeddings Lack Cluster Structure

**Symptoms:**
- All points are evenly distributed in space, no class or view-based clustering.

**Suggested Next Steps:**
- Add clustering loss (e.g., SwAV, DeepCluster).
- Apply contrastive loss across hard positives and negatives.
- Normalize embeddings to a hypersphere, then cluster.

---

## 127. Features Have High Variance But Poor Discrimination

**Symptoms:**
- Eigenvalue entropy is high, but classification or matching fails.

**Suggested Next Steps:**
- Add discriminative supervision, like pseudo-labels or neighborhood loss.
- Refine positive/negative sampling: quantity ≠ quality.
- Combine variance loss with alignment or classification head.

---

## 128. Masked Prediction Loss Is Too Easy

**Symptoms:**
- Model solves mask-based tasks trivially (e.g., short-range recon).

**Suggested Next Steps:**
- Mask larger and irregular regions (e.g., BEiT, MAE-style).
- Predict non-reconstructible targets (e.g., codebook index, cluster id).
- Increase semantic prediction difficulty, not just spatial.

---

## 129. Semantic Classes Have Overlapping Representations

**Symptoms:**
- t-SNE/PCA shows overlap between semantically distinct classes.

**Suggested Next Steps:**
- Add margin loss, class-specific contrastive loss, or triplet loss.
- Introduce hierarchical labels and supervise coarse/fine levels jointly.
- Log inter-class cosine similarity distributions over epochs.

---

## 130. Downstream Tasks Show Divergent Trends

**Symptoms:**
- Improvements in one task hurt another (e.g., classification vs. retrieval).

**Suggested Next Steps:**
- Apply multi-head or multi-task learning, allowing task-specific representation flow.
- Introduce a shared encoder + gated adapters for task-specialized features.

---

## 131. Loss Improves, but Only for a Subset of Batches

**Symptoms:**
- Loss drops during some batches but spikes on others.

**Suggested Next Steps:**
- Log loss per batch and compute standard deviation across batches.
- Stratify training data by difficulty, class, domain, and examine batch composition.
- Apply batch-wise loss balancing or curriculum-based batch sampling.

---

## 132. Features Are Too Similar Across Different Modalities

**Symptoms:**
- Audio and text, or image and text, have indistinct embeddings.

**Suggested Next Steps:**
- Use modality-specific encoders before fusion.
- Apply modality contrastive loss (align only cross-modality positives).
- Add orthogonality or diversity loss across modalities.

---

## 133. Gradients Are Dominated By Easy Samples

**Symptoms:**
- Hard examples contribute little to gradient updates.

**Suggested Next Steps:**
- Apply hard example mining (semi-hard triplet, focal loss).
- Track gradient norms per sample and reweight dynamically.
- Try gradient boosting strategies like GHM loss.

---

## 134. Model Stops Learning After First Few Epochs

**Symptoms:**
- Loss stagnates early, even with enough capacity.

**Suggested Next Steps:**
- Reduce weight decay or increase initial learning rate.
- Add stochastic layers or dropout to prevent early convergence.
- Inspect activation saturation or dead neurons.

---

## 135. Training Accuracy is High But Representations are Not Transferable

**Symptoms:**
- SSL pretraining yields poor results in new domains/tasks.

**Suggested Next Steps:**
- Include domain-agnostic augmentations.
- Evaluate representations with linear probing across datasets.
- Regularize with maximum entropy or mutual information objectives.

---

## 136. Augmented Pairs Are Mismatched in Difficulty

**Symptoms:**
- One view is trivial, the other too distorted.

**Suggested Next Steps:**
- Apply augmentation parity checking (both views should retain core signal).
- Use progressive augmentation, ramping up strength symmetrically.
- Consider augmentation-aware loss reweighting.

---

## 137. Downstream Accuracy Increases, but Embedding Rank Drops

**Symptoms:**
- Validation improves while feature covariance rank or entropy decreases.

**Suggested Next Steps:**
- Add a diversity-promoting loss (e.g., VICReg’s variance or covariance loss).
- Introduce multi-head learning to expand embedding use.
- Use attention-based bottlenecks to preserve distinct components.

---

## 138. Temporal SSL Model Predicts the Future Too Easily

**Symptoms:**
- Predictive loss saturates; model memorizes patterns.

**Suggested Next Steps:**
- Increase prediction horizon (e.g., predict t+5 instead of t+1).
- Add random time masking or drop intermediate frames.
- Use shuffled targets or contrastive prediction to increase difficulty.

---

## 139. Some Classes Are Represented Only By Rare Features

**Symptoms:**
- Certain classes rely on uncommon dimensions or embeddings.

**Suggested Next Steps:**
- Apply class-specific normalization or scaling.
- Add representation frequency regularization (e.g., avoid overconcentration).
- Use label-aware dimensional analysis (e.g., variance per class).

---

## 140. Model Only Uses Low-Frequency Patterns in Input

**Symptoms:**
- High-frequency information is ignored (e.g., textures, fine edges, pitch variations).

**Suggested Next Steps:**
- Add frequency domain augmentations (e.g., filtering, jitter).
- Introduce multi-scale processing (e.g., dilated convolutions, wavelets).
- Train with spectral contrastive loss or predict Fourier/CQT/STFT embeddings.

---

## 141. Feature Collapse Occurs Only in the Deeper Layers

**Symptoms:**
- Early layers are diverse; collapse happens near output.

**Suggested Next Steps:**
- Apply VC regularization (variance/covariance) directly on deeper layers.
- Add auxiliary heads at intermediate layers to guide learning progressively.
- Use layer-wise contrastive loss (SimCLR with deep supervision).

---

## 142. Model Only Learns Short-Term Dependencies in Sequences

**Symptoms:**
- Sequence model ignores long-term context.

**Suggested Next Steps:**
- Add dilated convolutions or transformer attention with larger receptive field.
- Include future context prediction or temporal jigsaw tasks.
- Add sequence length-aware curriculum during training.

---

## 143. Representation Entropy is High but Not Informative

**Symptoms:**
- High entropy, but all dimensions contribute noise, not useful variance.

**Suggested Next Steps:**
- Use informative bottlenecks (e.g., mutual info maximization).
- Add target-alignment losses like supervised contrastive or linear probe loss.
- Analyze dimensional usefulness using linear probes per axis.

---

## 144. Optimization Is Trapped in Flat Regions

**Symptoms:**
- Gradients are nonzero but loss barely changes.

**Suggested Next Steps:**
- Switch to optimizer with momentum adaptation (e.g., Lion, RAdam).
- Use sharpness-aware minimization (SAM) or loss landscape smoothing.
- Add batch norm reset mid-training to refresh scale sensitivity.

---

## 145. Multi-View Features Are Misaligned Across Representations

**Symptoms:**
- Augmented views of the same sample drift apart in embedding space.

**Suggested Next Steps:**
- Add alignment loss with cosine or L2 distance between views.
- Normalize both views before projection (z = F.normalize(z)).
- Use stop-gradient to stabilize learning in one view (BYOL-style).

---

## 146. SSL Model Converges Too Fast

**Symptoms:**
- Rapid loss minimization and early stagnation.

**Suggested Next Steps:**
- Lower initial learning rate and add longer warmup.
- Delay EMA updates in target networks or use burn-in.
- Schedule augmentation intensity to ramp up over time.

---

## 147. Different Heads in Multi-Head Models Learn Redundant Information

**Symptoms:**
- Each head learns nearly identical representations.

**Suggested Next Steps:**
- Force heads to focus on different subspaces via attention masks.
- Train with head-specific augmentations or tasks.

---

## 148. Pretext Task Rewards Easy-to-Cheat Shortcuts

**Symptoms:**
- Model solves task without learning useful representations.

**Suggested Next Steps:**
- Modify task to eliminate shortcuts (e.g., masking center patches in jigsaw).
- Add auxiliary tasks that require semantic abstraction.
- Use adversarial pretext noise to suppress spurious signals.

---

## 149. Low-Rank Covariance Is Due to ReLU Dead Units

**Symptoms:**
- Collapse traced to dead ReLU paths (zero output channels).

**Suggested Next Steps:**
- Use LeakyReLU, GELU, or Softplus in early layers.
- Initialize layers with positive bias to avoid ReLU death.
- Track activation rate per neuron/channel and penalize inactivity.

---

## 150. Model Is Sensitive to Padding or Edge Effects

**Symptoms:**
- Predictions shift with minor padding or crop shifts.

**Suggested Next Steps:**
- Use reflection padding or zero-padding removal.
- Add edge-aware augmentations or mask boundary predictions.
- Train with crop consistency (predictions invariant under context shift).

---

## 151. Model Generalizes Poorly Across Domains Despite Similar Label Sets

**Symptoms:**
- Works well on one domain (e.g., photos), fails on another (e.g., sketches, renderings).

**Suggested Next Steps:**
- Apply domain-adversarial training (e.g., DANN).
- Use domain-specific batch normalization or feature whitening.
- Include domain-agnostic augmentations or style mixing.

---

## 152. High Performance on High-Res Images, Poor on Low-Res

**Symptoms:**
- Accuracy or representation quality drops with input downscaling.

**Suggested Next Steps:**
- Train with multi-resolution inputs or resolution jitter.
- Add anti-aliasing filters before downsampling (to preserve signal).
- Use feature pyramids or scale-invariant encoders.

---

## 153. Embedding Space Shows Radial Clustering by Input Length or Area

**Symptoms:**
- Longer or larger inputs are farther from origin, not necessarily semantically different.

**Suggested Next Steps:**
- Apply length normalization to latent vectors.
- Use positional embedding normalization in variable-length inputs.
- Penalize correlation between input length and representation norm.

---

## 154. Representations Are Spatially Biased (e.g., corners encode more)

**Symptoms:**
- Model uses consistent spatial locations more heavily, regardless of content.

**Suggested Next Steps:**
- Add spatial dropout or location shuffling to break positional bias.
- Visualize activation heatmaps to localize reliance.
- Use center bias regularization to equalize focus.

---

## 155. Norm Layers Prevent Meaningful Representation Spread

**Symptoms:**
- BatchNorm or LayerNorm zero-centers all representations too aggressively.

**Suggested Next Steps:**
- Replace BatchNorm with GroupNorm, especially for small batches.
- Add learnable affine scaling after normalization.
- Temporarily disable norm layers to test impact on diversity.

---

## 156. Learned Features Are Highly Sensitive to Color Channels

**Symptoms:**
- RGB perturbations change output significantly even with minor distortion.

**Suggested Next Steps:**
- Train with color jitter, channel drop, or grayscale conversion.
- Add color-invariant auxiliary loss or style-drop layers.
- Separate shape vs. color encoding via dual-path networks.

---

## 157. SSL Loss Is Minimized by Trivial Solutions (e.g., predicting position from padding)

**Symptoms:**
- Model uses unintended input clues to minimize loss.

**Suggested Next Steps:**
- Add masking or crop randomization to remove predictable structure.
- Include false positives to detect spurious shortcuts.
- Regularize using intermediate layer agreement, not just final layer.

---

## 158. Learned Kernels Specialize Too Early in Training

**Symptoms:**
- Filters converge to sharp patterns before semantic features form.

**Suggested Next Steps:**
- Use soft weight decay early and increase regularization later.
- Apply stochastic depth to limit early overfitting.
- Track kernel evolution; freeze early kernels periodically to stabilize.

---

## 159. Transfer Learning Fails When Using Custom Projectors

**Symptoms:**
- Replacing projector head prevents useful downstream reuse.

**Suggested Next Steps:**
- Detach projector during inference and only use encoder features.
- Use skip connection from encoder to projector, and monitor both.
- Pretrain with frozen encoder, then unfreeze gradually.

---

## 160. Learned Representations Encode Temporal Ordering Even When Irrelevant

**Symptoms:**
- Representations leak time step info, harming generalization.

**Suggested Next Steps:**
- Shuffle time blocks during pretraining to disentangle content from order.
- Use relative temporal prediction instead of absolute position.
- Add contrastive tasks across time-shifted, non-ordered views.

---

## 161. Latent Space Saturates Early But Output Still Improves

**Symptoms:**
- Representation metrics (e.g., variance, entropy) plateau early, but task loss continues to drop.

**Suggested Next Steps:**
- Apply nonlinear probing heads to measure deeper latent changes.
- Add feature dynamics tracking (cosine similarity of features across epochs).
- Consider regularizing embedding stability, not just diversity.

---

## 162. Learning Rate Scheduler Triggers Before Model is Ready

**Symptoms:**
- LR drops before representation or loss has stabilized.

**Suggested Next Steps:**
- Use loss-based scheduler (e.g., ReduceLROnPlateau).
- Delay scheduler start or set minimum warmup epochs.
- Replace scheduler with cosine decay with restart to regain performance later.

---

## 163. Representations Across Augmentations Are Over-Aligned

**Symptoms:**
- Feature vectors from multiple views are too similar — loss is minimized trivially.

**Suggested Next Steps:**
- Increase augmentation strength asymmetry (strong–weak pairing).
- Add decorrelation loss across views.
- Replace full alignment with InfoNCE loss, requiring harder matching.

---

## 164. Cluster Assignments Are Dominated by Large or Frequent Classes

**Symptoms:**
- SwAV/DeepCluster-type prototypes collapse to majority classes.

**Suggested Next Steps:**
- Add prototype diversity constraints or entropy maximization.
- Apply balanced sampling or prototype reweighting.
- Temporarily increase the number of prototypes beyond class count.

---

## 165. Early Layers Do Not Learn Without Supervision

**Symptoms:**
- Only deep layers show feature learning under SSL.

**Suggested Next Steps:**
- Apply deep supervision to intermediate activations.
- Add early-layer contrastive loss (SimCLR + deep heads).
- Use skip connections to amplify shallow feature propagation.

---

## 166. Projection Head Dominates Training Signal

**Symptoms:**
- Encoder stops improving, projector keeps changing.

**Suggested Next Steps:**
- Detach or freeze projector mid-training to refocus on encoder.
- Regularize encoder with auxiliary losses not routed through the projector.
- Match encoder-to-encoder similarity directly (e.g., in SimSiam variants).

---

## 167. Pretext Task Is Solved by Superficial Heuristics

**Symptoms:**
- Model uses easily exploitable shortcut instead of learning features.

**Suggested Next Steps:**
- Add adversarial augmentation to prevent shortcut exploitation.
- Apply masking or corruption to shortcut channels (e.g., shape-only masks).
- Validate usefulness with probing tasks to detect overfit pretext-specific patterns.

---

## 168. Latent Space is Too Uniform Across All Samples

**Symptoms:**
- Feature vectors are evenly distributed but not semantically meaningful.

**Suggested Next Steps:**
- Add semantic structure loss (e.g., supervised contrastive loss, label smoothing).
- Cluster on intermediate layers rather than final features.
- Evaluate neighborhood consistency — local similarity within semantic groups.

---

## 169. Overuse of Stop-Gradient Blocks Learning

**Symptoms:**
- Model with stop-gradient fails to improve because no feedback is flowing.

**Suggested Next Steps:**
- Only stop gradient on one branch or one layer, not everywhere.
- Log gradient norms per module to verify flow.
- Add predictor bypass path to allow controlled flow from target to online branches.

---

## 170. Model Is Highly Sensitive to Specific Token Positions (e.g., start or end tokens)

**Symptoms:**
- Performance drops when changing sequence offset or padding.

**Suggested Next Steps:**
- Use relative positional encodings or rotary embeddings.
- Add token masking during pretraining to suppress fixed-position reliance.
- Train with sequence shifting augmentation (e.g., offset and pad randomly).

---

## 171. Multimodal Fusion Layer Causes Instability

**Symptoms:**
- When modalities (e.g., audio + text) are fused, loss becomes unstable or accuracy drops.

**Suggested Next Steps:**
- Use modality-specific encoders followed by late fusion.
- Normalize modality embeddings separately before combining.
- Try cross-attention instead of concatenation or summation.

---

## 172. Contrastive Loss Heavily Skewed Toward Negatives

**Symptoms:**
- Negatives dominate gradient flow; positives have minimal influence.

**Suggested Next Steps:**
- Increase positive-to-negative ratio using hard positive mining.
- Add false negative detection or de-biased contrastive loss (e.g., DCL).
- Reduce temperature to sharpen the softmax distribution.

---

## 173. Freezing Lower Layers Hurts Representation Quality

**Symptoms:**
- Training with frozen early layers limits expressiveness.

**Suggested Next Steps:**
- Use progressive unfreezing: start frozen, then unfreeze gradually.
- Apply adapter modules on top of frozen layers to reintroduce learning.
- Add skip connections from frozen to learnable layers.

---

## 174. Class Prototypes Collapse to a Few Embedding Points

**Symptoms:**
- Feature centers (prototypes) of many classes overlap in space.

**Suggested Next Steps:**
- Increase prototype diversity with repulsion loss or angular margins.
- Add class-conditional decorrelation loss.
- Periodically reinitialize or shuffle prototypes.

---

## 175. Temporal Features Drift Between Epochs

**Symptoms:**
- Features for the same temporal point change drastically each epoch.

**Suggested Next Steps:**
- Add temporal stability loss or EMA features per time step.
- Use time-contrastive learning to anchor representations over time.
- Reduce learning rate or apply temporal dropout.

---

## 176. Representation Collapse Only Happens at Larger Batch Sizes

**Symptoms:**
- Small batches work, but collapse occurs when scaling up.

**Suggested Next Steps:**
- Increase variance/covariance regularization strength.
- Apply gradient noise injection or gradient centralization.
- Normalize embeddings across batch and view, not just batch.

---

## 177. Classifier Overfits to Label Distribution

**Symptoms:**
- Model predicts frequent classes regardless of input signal.

**Suggested Next Steps:**
- Rebalance loss using effective number of samples or focal loss.
- Add label-agnostic self-supervised pretraining.
- Use balanced sampling or class-aware augmentations.

---

## 178. Large Architectures Cause Early Divergence

**Symptoms:**
- Large models diverge faster than small ones.

**Suggested Next Steps:**
- Add learning rate scaling with model width (e.g., √d or 1/d).
- Use grad norm clipping per layer to avoid large jumps.
- Warm up both weights and momentum buffers gradually.

---

## 179. Gradient Norm is Stable, But Loss Plateaus

**Symptoms:**
- Backprop appears normal, but learning stalls.

**Suggested Next Steps:**
- Check for activation saturation (e.g., via histogram of outputs).
- Switch activation functions (e.g., ReLU → GELU).
- Add orthogonal initialization to kick-start dynamics.

---

## 180. Regularization Prevents Signal from Emerging

**Symptoms:**
- High dropout or strong norm constraints suppress learning.

**Suggested Next Steps:**
- Reduce regularization gradually (e.g., schedule weight decay or dropout).
- Visualize weight evolution to detect over-constrained filters.
- Use dynamic regularization based on loss or layer confidence.

---

## 181. Pretrained Model Loses Performance After Domain-Specific Fine-Tuning

**Symptoms:**
- Fine-tuning on a narrow domain destroys generalization.

**Suggested Next Steps:**
- Use layer-wise learning rate decay (lower LR in early layers).
- Apply adapter layers instead of fine-tuning full model.
- Use interleaved fine-tuning: alternate domain and general-domain steps.

---

## 182. Attention Maps Are Static Across Inputs

**Symptoms:**
- Self-attention maps look the same regardless of input.

**Suggested Next Steps:**
- Inspect input norm and variance — model might be understimulated.
- Add attention diversity loss (e.g., max entropy or orthogonality).
- Reduce positional encoding scale if it dominates token interaction.

---

## 183. Contrastive Tokens Collapse to a Single Cluster

**Symptoms:**
- Augmented views or masked tokens converge to identical embeddings.

**Suggested Next Steps:**
- Use token-level decorrelation (e.g., VICReg or Barlow Twins per token).
- Predict token categories or relative positions as an auxiliary task.
- Apply patch dropping or masked token prediction jointly with contrastive loss.

---

## 184. Latent Features Mirror Unintended Artifacts (e.g., filename, camera angle)

**Symptoms:**
- Embeddings cluster by collection metadata, not semantics.

**Suggested Next Steps:**
- Strip metadata during dataloader preprocessing.
- Apply confounder adversarial loss to suppress leaked artifacts.
- Use distributional alignment to balance camera or sensor source.

---

## 185. Model Is Too Confident on Ambiguous Inputs

**Symptoms:**
- Predictive confidence is high even for borderline or noisy inputs.

**Suggested Next Steps:**
- Apply confidence penalty (e.g., softmax entropy maximization).
- Use label smoothing during training.
- Add test-time dropout or Monte Carlo dropout for uncertainty estimation.

---

## 186. Model Learns Class-Specific Augmentation Shortcuts

**Symptoms:**
- Certain augmentations correlate with specific classes, leading to shortcut learning.

**Suggested Next Steps:**
- Log augmentation statistics per class and rebalance augmentation assignment.
- Use class-agnostic or randomized augmentations.
- Add augmentation confusion loss to decorrelate.

---

## 187. Input Tokens Are Underutilized in Attention

**Symptoms:**
- Some tokens receive near-zero attention weights across all heads/layers.

**Suggested Next Steps:**
- Visualize token importance heatmaps across time/space.
- Regularize attention scores with minimum usage constraints.
- Add token dropout to force learning from all parts of the input.

---

## 188. Latent Features Form Hollow Shells (e.g., ring-shaped or boundary-concentrated)

**Symptoms:**
- Embeddings occupy edges of hypersphere with little inner mass.

**Suggested Next Steps:**
- Reduce use of L2 normalization or spherical loss if inappropriate.
- Apply volume-filling regularization or maximize entropy in latent space.
- Replace uniform norm target with controlled radius distribution.

---

## 189. Token-Level Representations Lack Global Context

**Symptoms:**
- Each token is encoded well locally, but global relationships are missed.

**Suggested Next Steps:**
- Add CLS token interaction loss (e.g., contrast token ↔ pooled rep).
- Include graph-based regularization over the token connectivity.
- Use global masking tasks (e.g., shuffle patches and predict alignment).

---

## 190. Fine-Tuned Model Forgets Pretraining Signals Too Quickly

**Symptoms:**
- Downstream training overwrites SSL features completely.

**Suggested Next Steps:**
- Apply feature alignment regularization (keep SSL features close).
- Use low learning rate or freeze early layers during downstream phase.
- Add distillation loss from pretraining version of model.

---

## 191. Model Can’t Handle Very Long Sequences (>1024 tokens)

**Symptoms:**
- Performance drops or memory explodes when sequence length increases.

**Suggested Next Steps:**
- Use chunking or sliding window attention (e.g., Longformer, BigBird).
- Add recurrence modules (e.g., Transformer-XL memory caching).
- Train with truncated sequences and finetune on full-length.

---

## 192. One Augmentation View Always Dominates Representation

**Symptoms:**
- Contrastive pairs are asymmetric; only one view contributes useful features.

**Suggested Next Steps:**
- Equalize augmentation difficulty and diversity between views.
- Swap encoder inputs during training (view_a → encoder_b).
- Track and plot view-specific gradient magnitudes to monitor imbalance.

---

## 193. Encoder Reaches Capacity Limit Early

**Symptoms:**
- Representations saturate even though loss continues improving.

**Suggested Next Steps:**
- Add extra bottleneck layers or increase depth of encoder.
- Use wider embedding layers or more channels.
- Add residual connections to prevent dead-end compression.

---

## 194. Model Exhibits Global Collapse Despite Balanced Loss Terms

**Symptoms:**
- Even with VC regularization or contrastive loss, collapse still occurs.

**Suggested Next Steps:**
- Monitor feature norms, rank, entropy during training in real time.
- Add adaptive scaling of regularization losses (e.g., scale VC by cosine sim drop).
- Add stochastic gradient skip connections to prevent collapse cascades.

---

## 195. Contrastive Features Become Memory-Dependent

**Symptoms:**
- SSL model performs well only when memory banks or queues are active.

**Suggested Next Steps:**
- Reduce reliance on memory by increasing batch size.
- Use momentum encoders (BYOL, MoCo) to stabilize without external queues.
- Simulate large batches with cross-GPU negative sharing.

---

## 196. Features Drift Between Validation and Training Modes

**Symptoms:**
- Latents behave differently between training and evaluation (e.g., in batchnorm mode switch).

**Suggested Next Steps:**
- Use GroupNorm or LayerNorm for consistency across modes.
- Track mean/std statistics of outputs across train vs. eval.
- Add validation-time alignment loss to correct this drift.

---

## 197. Backbone Weights Oscillate or Diverge Under Joint Objectives

**Symptoms:**
- Joint SSL + supervised training causes encoder instability.

**Suggested Next Steps:**
- Apply loss scheduling (e.g., pretrain SSL first, then gradually mix supervision).
- Use multi-head architecture, each head for one task.
- Detach gradients between conflicting branches.

---

## 198. Prototypes Collapse to Uniform Direction

**Symptoms:**
- All cluster centers or anchors in the latent space collapse toward the same point or line.

**Suggested Next Steps:**
- Apply orthogonal constraint or repulsive prototype loss.
- Periodically re-initialize prototypes randomly or based on KMeans.
- Enforce entropy-maximizing soft assignment in cluster heads.

---

## 199. Model Underperforms on Edge Cases Despite High Average Accuracy

**Symptoms:**
- Accuracy good overall, but fails on rare or ambiguous inputs.

**Suggested Next Steps:**
- Track and log performance per input cluster or difficulty bin.
- Use confidence-aware losses or hard-negative oversampling.
- Add a secondary head trained only on difficult examples.

---

## 200. Embedding Space Shows “Grid-Like” Artifacts or Clumping

**Symptoms:**
- t-SNE or PCA shows unnatural geometric artifacts (e.g., grids, bands).

**Suggested Next Steps:**
- Avoid strong weight tying or positional encoding that enforces structure.
- Increase activation noise or dropout during training.
- Add feature-space regularizers that penalize repeating distances.

---

## 201. Bottleneck Layer Becomes Information Sink

**Symptoms:**
- Information gets trapped before or within a narrow hidden layer (e.g., 128-d bottleneck).

**Suggested Next Steps:**
- Add skip connections from earlier layers to bypass the bottleneck.
- Increase bottleneck dimensionality temporarily and schedule decay.
- Apply information maximization loss (e.g., maximize variance, entropy, or MI).

---

## 202. One Modality Is Ignored in Multimodal Pretraining

**Symptoms:**
- Model ignores image/audio/text in favor of just one.

**Suggested Next Steps:**
- Add modality dropout to force robustness without that modality.
- Use modality-wise contrastive loss (e.g., CLIP-style).
- Apply modality-specific projection heads before fusion.

---

## 203. Augmentations Cause Drifting Representation Centers Over Epochs

**Symptoms:**
- Each view’s representation shifts its mean over time.

**Suggested Next Steps:**
- Use centered contrastive loss or online feature centering (as in VICReg).
- Track and normalize mean shift per augmentation type.
- Apply EMA center alignment or momentum projection head.

---

## 204. Model Collapses Only on Local Feature Maps, Not Global Pooling

**Symptoms:**
- Global average pooled features are diverse, but spatial maps are collapsed.

**Suggested Next Steps:**
- Regularize inter-patch variance (patch-wise VC loss).
- Apply token-level contrastive learning.
- Predict masked patch position or class to inject local supervision.

---

## 205. Downstream Classifier Overwrites Pretrained Representation

**Symptoms:**
- Linear/MLP head dominates learning signal during fine-tuning.

**Suggested Next Steps:**
- Freeze the backbone for the first few epochs of fine-tuning.
- Add distillation from pretrained encoder output.
- Use cosine classifier instead of dot product MLP to preserve direction.

---

## 206. SSL Loss Remains Flat While Representation Entropy Drops

**Symptoms:**
- Loss appears stable but latent features become more collapsed over time.

**Suggested Next Steps:**
- Track entropy, eigenvalue spectrum, cosine similarity in addition to loss.
- Scale up variance or covariance loss coefficients dynamically.
- Detect flat loss regions and inject noise or optimizer momentum reset.

---

## 207. LayerNorm Interferes with Representation Spread

**Symptoms:**
- Norm-based layers suppress useful latent feature variance.

**Suggested Next Steps:**
- Test with LayerNorm removed from last encoder/projector layer.
- Add post-LayerNorm learnable scaling.
- Compare to BatchNorm, GroupNorm, or no normalization.

---

## 208. Self-Supervised Features Are Not Linearly Separable

**Symptoms:**
- Linear probe performs poorly; non-linear probe does better.

**Suggested Next Steps:**
- Add supervised contrastive loss or center loss during fine-tuning.
- Regularize for linear alignment (e.g., linear probing at training time).
- Increase embedding dimensionality to reduce projection saturation.

---

## 209. Local Neighborhoods in Latent Space Lack Structure

**Symptoms:**
- Nearest neighbors are semantically unrelated.

**Suggested Next Steps:**
- Add neighborhood consistency loss (e.g., kNN overlap).
- Use semantic contrastive learning if pseudo-labels or hierarchy exist.
- Train with triplet sampling from same label cluster if supervised data is available.

---

## 210. Projections Differ Across Training Seeds Despite Similar Performance

**Symptoms:**
- Model converges to different latent spaces for different runs.

**Suggested Next Steps:**
- Align latent spaces using procrustes alignment or centered kernel alignment.
- Add orthogonality or directional regularization across runs (e.g., ensemble stabilization).
- Seed fix and log random state of augmentations, dropout, and initialization for full reproducibility.

---

## 211. Model Appears to Converge, But Representation Quality Stagnates

**Symptoms:**
- Loss flatlines, but PCA/eigenvalue entropy or transfer accuracy stays low.

**Suggested Next Steps:**
- Switch to secondary diagnostic objectives (e.g., local contrast, token prediction).
- Inject perturbation noise or gradient reversal to escape flat minima.
- Reset or adapt projector weights mid-training (cold-restart regularization).

---

## 212. Representations Stop Improving After BatchNorm Saturation

**Symptoms:**
- BatchNorm outputs flatten or diverge after stabilization.

**Suggested Next Steps:**
- Replace with GroupNorm or LayerNorm for better per-sample stability.
- Track running mean/var drift — reset stats if necessary.
- Apply LayerScale to prevent early BatchNorm domination.

---

## 213. Small-Scale Models Over-Regularize and Underfit

**Symptoms:**
- VC regularization hurts more than helps in compact architectures.

**Suggested Next Steps:**
- Scale regularization coefficients (e.g., VICReg’s µ, ν) proportional to model width.
- Use sparser views or reduce batch size to soften constraints.
- Replace global VC loss with local diversity loss in intermediate layers.

---

## 214. Augmentation Views Leak Identity (e.g., color tags, borders)

**Symptoms:**
- Views differ in trivial artifacts that make identity prediction easy.

**Suggested Next Steps:**
- Visualize augmented views side by side — detect trivial anchors.
- Add augmentation adversarial loss: prevent model from using view-specific cues.
- Randomize augmentation pipeline per batch.

---

## 215. Cross-Entropy Loss Plateaus with No Obvious Collapse

**Symptoms:**
- Accuracy or probe scores stall while loss hovers at constant value.

**Suggested Next Steps:**
- Add cosine classifier or temperature scaling to sharpen logits.
- Increase embedding space curvature using nonlinearities or wider projection.
- Introduce pseudo-supervised cluster heads (e.g., DeepCluster or SwAV prototypes).

---

## 216. Representations Are Semantically Valid But Geometrically Distorted

**Symptoms:**
- Clusters exist, but with poor margin or shape (e.g., elongated blobs).

**Suggested Next Steps:**
- Apply contrastive margin loss or angular margin loss.
- Add isotropy regularization or maximize distance uniformity.
- Visualize local curvature metrics to reshape representation.

---

## 217. Early Convergence Hides Failure to Generalize

**Symptoms:**
- Validation loss improves fast, then drops suddenly on unseen data.

**Suggested Next Steps:**
- Evaluate using cross-domain or corrupted datasets during training.
- Delay optimizer plateau by using learning rate warmup with cosine ramp.
- Introduce task perturbation or weak supervision noise mid-training.

---

## 218. Learning Rate Warmup Has No Effect

**Symptoms:**
- Model behaves the same with or without warmup.

**Suggested Next Steps:**
- Check if early gradient magnitudes are already small (indicates saturation).
- Warm up per parameter group (e.g., encoder vs. projector vs. predictor).
- Replace static schedule with loss-aware LR adjustment.

---

## 219. Final Representations Overfit to View Type Rather Than Content

**Symptoms:**
- t-SNE separates views (augmentation type) more than class/content.

**Suggested Next Steps:**
- Penalize augmentation-type classification (confounder suppression).
- Use mixed-view consistency loss (e.g., blend of two views → same rep).
- Collapse multiple augmentations to a shared intermediate representation.

---

## 220. Model Predicts Latents That Are Statistically Perfect but Semantically Useless

**Symptoms:**
- Perfect cosine similarity / loss match, but features are meaningless.

**Suggested Next Steps:**
- Visualize latent neighborhoods, not just similarity to targets.
- Add semantic probe tasks (linear/MLP head on frozen encoder).
- Validate representation using retrieval, clustering, or out-of-distribution detection.

---

## 221. Token Aggregation Method Bottlenecks Representational Power

**Symptoms:**
- Switching from mean pooling to CLS token drastically changes performance.

**Suggested Next Steps:**
- Use learned pooling (e.g., attention-based pooling or weighted averaging).
- Combine CLS + mean pooling (concatenation or residual blend).
- Track activation statistics of pooled vs. raw token sets.

---

## 222. Multiple Heads Converge to Same Representations

**Symptoms:**
- Multi-head architecture has identical outputs or similar feature maps.

**Suggested Next Steps:**
- Add orthogonality constraints between head outputs.
- Apply head-wise diversity loss or negative cosine similarity.
- Force head specialization via view/task conditioning.

---

## 223. Latents Encode Very Low Mutual Information With Original Input

**Symptoms:**
- Representations are compressive but unrelated to input structure.

**Suggested Next Steps:**
- Add mutual information maximization (e.g., InfoNCE with input embedding).
- Use inpainting or masked autoencoding to tether latent to input.
- Visualize input reconstruction or saliency maps from latents.

---

## 224. Model Fails to Learn When Using Sparse Attention or Sparse Conv Layers

**Symptoms:**
- Sparsity-based models stall in learning phases.

**Suggested Next Steps:**
- Warm up with dense layers, then transition to sparse (gradual pruning).
- Use gating mechanisms to control sparsity activation.
- Increase batch size or sequence length to populate sparse regions.

---

## 225. Feature Drift Magnitude Changes With Batch Size

**Symptoms:**
- Larger batches lead to higher representation shift per step.

**Suggested Next Steps:**
- Track and plot embedding drift norm per batch size.
- Scale optimizer learning rate with batch size (LR ∝ batch_size).
- Use gradient accumulation to simulate large batches with smaller updates.

---

## 226. Projection Head Masks Convergence Issues

**Symptoms:**
- Projected features converge, but encoder features remain noisy.

**Suggested Next Steps:**
- Add loss terms on encoder output in addition to projector output.
- Track alignment between encoder + projector space (cosine sim).
- Temporarily detach projector and evaluate encoder directly.

---

## 227. Positional Encoding Disrupts Learned Patterns on Reordering

**Symptoms:**
- Input permutation breaks latent structure, even for invariant content.

**Suggested Next Steps:**
- Use relative position encoding or rotary embeddings.
- Add shuffled-sequence prediction task (permutation contrastive learning).
- Test positional bias by training with positional dropout.

---

## 228. Semantic Clustering Only Appears at Intermediate Layers

**Symptoms:**
- Early or middle layers show meaningful clusters, final layer doesn’t.

**Suggested Next Steps:**
- Apply multi-layer probing to determine where structure is lost.
- Use early-exit representations or ensemble intermediate outputs.
- Add linear probes or auxiliary contrastive loss to intermediate features.

---

## 229. Late-Training Representations Align with Augmentations More Than Labels

**Symptoms:**
- Data points group by augmentation type even in labeled training.

**Suggested Next Steps:**
- Penalize view-specific components (e.g., dropout embeddings from weak/strong views).
- Add label-view alignment loss to prioritize semantic grouping.
- Limit augmentations post-certain epoch (controlled curriculum).

---

## 230. Increasing Model Capacity Hurts Representation Diversity

**Symptoms:**
- Wider/deeper models learn fewer distinct directions (lower rank).

**Suggested Next Steps:**
- Increase regularization on final layers (e.g., covariance loss, feature whitening).
- Track feature rank, cosine similarity, and entropy across model widths.
- Use bottlenecks or drop-blocks in deep networks to force diversity.

---

## 231. Self-Attention Maps Oscillate or Flip Across Epochs

**Symptoms:**
- Attention heads change drastically between checkpoints even with similar loss.

**Suggested Next Steps:**
- Add temporal attention consistency loss between epochs (or EMA-based regularizer).
- Reduce learning rate of attention parameters specifically.
- Visualize and track head-wise attention entropy over time.

---

## 232. Gradients Collapse in First Few Layers Despite Loss Drop

**Symptoms:**
- Early conv/transformer layers receive ~0 gradient norm.

**Suggested Next Steps:**
- Apply layer-wise learning rate scaling (larger LR for shallow layers).
- Add skip connections from early features to downstream modules.
- Use input-level noise injection or feature shaking to reengage early layers.

---

## 233. Latents Shift Too Rapidly When Model Size Is Increased

**Symptoms:**
- Larger models introduce chaotic drift in latent space even with same training setup.

**Suggested Next Steps:**
- Tune optimizer momentum and warmup schedule for larger capacity.
- Add adaptive weight norm regularization or trust-region clipping.
- Visualize epoch-to-epoch latent cosine similarity across model scales.

---

## 234. Pretraining Optimizes for Alignment but Hurts Transferability

**Symptoms:**
- Latents align perfectly across views, but fail on downstream classification.

**Suggested Next Steps:**
- Reduce alignment strength (e.g., cosine loss weight).
- Introduce view-specific variance loss or semantic structure constraints.
- Add third view or cluster anchor to reintroduce representation diversity.

---

## 235. LayerNorm in Projector Causes Latent Collapse Late in Training

**Symptoms:**
- Collapse occurs near convergence and is traced to norm layers.

**Suggested Next Steps:**
- Remove final LayerNorm in projector.
- Replace with affine LayerNorm or GroupNorm.
- Move normalization before MLP layers rather than after.

---

## 236. Token Representations Lose Positional Awareness Late in Training

**Symptoms:**
- All tokens cluster tightly, and sequence structure disappears.

**Suggested Next Steps:**
- Regularize with token-position prediction or distance-preserving contrastive loss.
- Add temporal or spatial jigsaw tasks.
- Visualize token similarity matrices across space/time and enforce diversity.

---

## 237. Prototype-Centered Representations Become Over-Clustered

**Symptoms:**
- All representations collapse tightly around a few anchors or clusters.

**Suggested Next Steps:**
- Apply entropy maximization on soft cluster assignments.
- Use repulsion loss between prototypes.
- Add a “pull-push” objective to enforce soft overlap while avoiding sharp collapse.

---

## 238. Learning Rate Plateau Prevents Escape from Poor Minima

**Symptoms:**
- Loss stagnates and model can’t escape suboptimal solution.

**Suggested Next Steps:**
- Use cyclical learning rate or restart scheduling.
- Perturb weights slightly to nudge out of basin (w ← w + ε, small noise).
- Inject gradient noise or apply lookahead optimizer to explore farther.

---

## 239. Latent Space Becomes Anisotropic in Final Epochs

**Symptoms:**
- One or two directions dominate variance; others vanish.

**Suggested Next Steps:**
- Add whitening regularization (e.g., off-diagonal covariance penalty).
- Apply eigenvalue entropy loss to encourage uniform spread.
- Normalize features, then apply contrastive/variance loss to avoid directional collapse.

---

## 240. Pretraining Loss Continues Dropping But Semantic Performance Worsens

**Symptoms:**
- Proxy objective keeps improving while downstream validation degrades.

**Suggested Next Steps:**
- Introduce semantic probes during pretraining (e.g., online linear probe).
- Apply multi-objective learning with auxiliary semantic constraints.
- Monitor alignment between feature space and ground truth labels over time.

---

## 241. Final Layers Show Increasing Norms but Decreasing Variance

**Symptoms:**
- Feature magnitudes grow, but spread across dimensions shrinks.

**Suggested Next Steps:**
- Normalize features before applying any objective (unit norm or LayerNorm).
- Add feature variance preservation loss.
- Use log-variance loss instead of raw covariance to detect saturation.

---

## 242. Representations Align Too Quickly in Contrastive Pretraining

**Symptoms:**
- Cosine similarity between views hits >0.99 early in training.

**Suggested Next Steps:**
- Decrease alignment loss weight, especially early on.
- Add decorrelation or variance loss in parallel to contrastive objective.
- Introduce curriculum scheduling to delay alignment.

---

## 243. Shared Encoder Becomes Biased Toward One Branch in Two-View Architectures

**Symptoms:**
- Online and target branches diverge in performance; shared encoder starts encoding one side’s signal more than the other.

**Suggested Next Steps:**
- Balance gradient flow with stop-gradient or inverse prediction paths.
- Apply symmetrized training (swap branches or use shared weight averaging).
- Add loss consistency penalty across branches.

---

## 244. Local Contrastive Objective Dominates Global Structure

**Symptoms:**
- Neighborhood contrast improves while inter-class spread is lost.

**Suggested Next Steps:**
- Apply multi-scale contrastive learning (patch-to-global and sample-to-sample).
- Add global clustering loss (e.g., SwAV, SCAN).
- Visualize embedding topology (e.g., UMAP or manifold curvature).

---

## 245. Linear Probe Accuracy Drops Midway Through Pretraining

**Symptoms:**
- Initially good semantic alignment becomes worse even as loss drops.

**Suggested Next Steps:**
- Add linear probe loss as auxiliary term.
- Use EMA encoder for linear probe evaluation instead of raw encoder.
- Adjust learning rate or dropout to reduce over-regularization.

---

## 246. Classifier Collapses Without Clear Signal in Feature Space

**Symptoms:**
- Downstream classifier outputs one or two classes confidently with little variance.

**Suggested Next Steps:**
- Apply entropy penalty on classifier output.
- Add label smoothing or sample-mixing augmentation (e.g., MixUp).
- Reinitialize and monitor the last layer weights and their update norms.

---

## 247. Different Feature Layers Disagree Strongly on Prediction

**Symptoms:**
- Deeper layers predict different class than earlier ones, especially in SSL transfer.

**Suggested Next Steps:**
- Use deep supervision — apply auxiliary heads to intermediate layers.
- Fuse multiple layer outputs (e.g., layer-wise weighted average).
- Apply temporal consistency across layer predictions.

---

## 248. Combined SSL + Supervised Training Leads to Feature Drift

**Symptoms:**
- Encoder trained jointly starts losing contrastive properties when supervised signal increases.

**Suggested Next Steps:**
- Use loss annealing — ramp down SSL objective gradually.
- Train with multi-task heads and allow independent decoders.
- Measure inter-feature angle stability before/after fine-tuning steps.

---

## 249. Representations Become Label-Specific But Non-Linearly Separable

**Symptoms:**
- t-SNE shows label-based grouping, but linear probe fails.

**Suggested Next Steps:**
- Apply supervised contrastive loss to enforce margin.
- Increase embedding dimensionality or add nonlinear projection head.
- Add whitening loss to encourage axis-aligned separation.

---

## 250. Latent Space Shows Structured Collapse (e.g., ring, cube, or lattice artifacts)

**Symptoms:**
- PCA/t-SNE plots show geometric artifacts instead of natural clusters.

**Suggested Next Steps:**
- Remove positional encoding artifacts or spatial biases in augmentations.
- Apply noise injection to feature space during training.
- Add isotropy regularization, e.g., maximize entropy over projection directions.

---

## 251. Pretrained Model Performs Worse Than Random Init on Certain Tasks

**Symptoms:**
- Downstream accuracy drops after pretraining, underperforms even untrained encoder.

**Suggested Next Steps:**
- Check for task mismatch: does pretraining induce wrong invariances?
- Try frozen backbone transfer first, then fine-tune carefully.
- Use task-aware augmentation filtering during pretraining.

---

## 252. Projector with Dropout Leads to Representation Drift Across Epochs

**Symptoms:**
- Latents fluctuate across epochs; instability traced to projector dropout.

**Suggested Next Steps:**
- Evaluate stability with/without dropout in projector at validation time.
- Use dropout only during training, and log feature drift over time.
- Try stochastic depth instead of dropout if needed for regularization.

---

## 253. Model Learns to Ignore Predictive Tokens

**Symptoms:**
- Masked token prediction or autoregression ignores certain positions.

**Suggested Next Steps:**
- Add token dropout or random token masking to force usage.
- Penalize zero-gradient tokens (e.g., mask if never used).
- Add auxiliary token importance prediction task.

---

## 254. Augmentation Leads to Over-Invariance to Useful Signal

**Symptoms:**
- Model becomes invariant to variations that should matter (e.g., color in leaf classification).

**Suggested Next Steps:**
- Redesign augmentation pipeline based on task relevance.
- Add contrastive positives with small semantic variations (intra-class negatives).
- Regularize with feature sensitivity loss across augmented views.

---

## 255. Projection Head Becomes Too Powerful and Overfits Pretext Task

**Symptoms:**
- Projector learns perfectly, encoder is weak or frozen.

**Suggested Next Steps:**
- Track gradients through encoder vs. projector to detect imbalance.
- Apply stop-gradient or bottleneck inside projector.
- Add invariance loss directly on encoder output.

---

## 256. Intermediate Representations Have High Redundancy, Final Do Not

**Symptoms:**
- Early layers are low-rank, final layers look high-rank but unstructured.

**Suggested Next Steps:**
- Add deep VC regularization across layers, not just final one.
- Visualize rank profile across depth (layer-wise covariance rank).
- Apply intermediate layer orthogonality or diversity constraints.

---

## 257. Representations Have High Cosine Similarity but Low Transferability

**Symptoms:**
- Aligned latents across views, but fail on downstream.

**Suggested Next Steps:**
- Add view-invariant semantic structure loss (e.g., cross-view class matching).
- Replace alignment-only objective with pull–push style contrastive loss.
- Monitor cosine similarity and class-separability jointly.

---

## 258. Low-Eigenvalue Latent Collapse Is Missed By Rank Monitoring

**Symptoms:**
- Full-rank covariance, but last 10–20 dimensions are near-zero variance.

**Suggested Next Steps:**
- Track eigenvalue spectrum flatness or entropy, not just rank.
- Apply variance floor regularization (e.g., VICReg-style min std).
- Weight low-variance dimension penalties higher.

---

## 259. Training Appears Stable Until Long Runs Introduce Collapse

**Symptoms:**
- Loss, accuracy stable for 100+ epochs, then collapse occurs.

**Suggested Next Steps:**
- Use periodic checkpoint comparison for latent drift detection.
- Apply weight norm monitors, gradient explosion/collapse checks.
- Add anti-collapse entropy loss that grows over time.

---

## 260. Model Scaling Causes Latent Overfitting to Batch Statistics

**Symptoms:**
- Larger models rely too heavily on batch composition (especially in BN-heavy pipelines).

**Suggested Next Steps:**
- Replace BatchNorm with GroupNorm or LayerNorm at scale.
- Use cross-batch feature decorrelation.
- Track latent shift across batch resamples and apply regularization.

---

## 261. One View in Contrastive Pair Has Consistently Higher Feature Norms

**Symptoms:**
- Augmented view A has much larger norms than view B across batches.

**Suggested Next Steps:**
- Normalize both views (F.normalize()) before similarity or loss computation.
- Log and compare view-wise norms as a training diagnostic.
- Add norm balancing loss to equalize representational energy between views.

---

## 262. Masking Strategy Causes Collapse in Masked Autoencoding

**Symptoms:**
- Too much masking leads to trivial reconstructions or zero outputs.

**Suggested Next Steps:**
- Reduce masking ratio or apply curriculum masking (start low, increase).
- Use spatially diverse masking (e.g., block or variable-length).
- Add unmasked token prediction or contrastive loss as a stabilizer.

---

## 263. Final Layer Attention Focuses Only on One Token (e.g., CLS)

**Symptoms:**
- Final attention maps show degenerate all-to-one behavior.

**Suggested Next Steps:**
- Add token diversity penalty (maximize attention entropy).
- Visualize head-wise token interaction heatmaps.
- Apply auxiliary attention distribution targets or structure loss.

---

## 264. Sample-Level Representations Drift Randomly Between Epochs

**Symptoms:**
- Same sample shows radically different embeddings across checkpoints.

**Suggested Next Steps:**
- Add temporal stability loss or checkpoint-wise cosine similarity regularizer.
- Replace dropout with deterministic regularization (e.g., stochastic depth).
- Smooth training trajectory using EMA on model weights or latents.

---

## 265. Multiple Inputs Map to Nearly Identical Embeddings

**Symptoms:**
- Different samples produce near-duplicate features (e.g., cosine sim > 0.99).

**Suggested Next Steps:**
- Increase input augmentation diversity.
- Apply triplet loss or redundancy penalty between unrelated samples.
- Detect and prevent this using false positive tracking in contrastive batches.

---

## 266. Early Epochs Fit Augmentations Instead of Content

**Symptoms:**
- Training loss drops, but representations cluster by augmentation type, not content.

**Suggested Next Steps:**
- Add augmentation confusion loss or adversarial augmentation prediction task.
- Delay heavy augmentations until mid-training (augmentation curriculum).
- Penalize cross-view clustering mismatch.

---

## 267. Feature Space Has Repeating Structures (e.g., mirrored clusters)

**Symptoms:**
- Clusters appear symmetric or duplicated across axes.

**Suggested Next Steps:**
- Add axis decorrelation loss or use whitening (e.g., VICReg-style).
- Randomly rotate or shuffle axes during training to prevent hardcoded geometry.
- Analyze cluster overlap via pairwise distance matrix.

---

## 268. Attention Heads Collapse to Same Pattern Across Layers

**Symptoms:**
- All heads output similar maps regardless of depth or input.

**Suggested Next Steps:**
- Add diversity loss across heads/layers (e.g., cosine or KL divergence).
- Use per-head positional encoding bias.
- Regularize heads using orthogonal initialization + dropout.

---

## 269. Representations Encode Unintended Spatial Features

**Symptoms:**
- Latents capture image borders, padding artifacts, or corner bias.

**Suggested Next Steps:**
- Randomize crop origin, padding method, and center alignment during augmentation.
- Mask or reweight border regions in attention.
- Add a confounder suppression loss for spatial artifacts.

---

## 270. Early Projection Layer Collapses While Deeper Layers Remain Diverse

**Symptoms:**
- First projection MLP layer has very low activation variance.

**Suggested Next Steps:**
- Apply residual or skip connection in projection head.
- Use LayerNorm + GELU before and after this layer to improve flow.
- Track per-layer feature variance, and reinitialize if collapse is detected.

---

## 271. Long-Sequence Model Overfits to Local Patterns Only

**Symptoms:**
- Model performs well on short-range tasks but fails at long dependencies.

**Suggested Next Steps:**
- Add long-span auxiliary tasks (e.g., next-chunk prediction).
- Train with mixed-length sequences to force scale adaptability.
- Use memory-enhanced architectures (e.g., recurrence, cache, attention pooling).

---

## 272. Model Overfits to Batch Layout or Shuffling Order

**Symptoms:**
- Slight changes in batch ordering reduce performance or change representations.

**Suggested Next Steps:**
- Enable drop-last in data loader and shuffle every epoch.
- Add batch-level random perturbation (e.g., sample reindexing).
- Track per-batch drift of global feature statistics.

---

## 273. Visual Model Relies on Textural Cues Instead of Shape or Structure

**Symptoms:**
- Fails when textures are shuffled or blurred, despite clear object outline.

**Suggested Next Steps:**
- Train with stylized augmentations or texture-shuffled images.
- Add shape-consistency loss (e.g., segmentation-contrastive).
- Evaluate with Stylized-ImageNet or similar benchmarks.

---

## 274. Attention Maps Collapse into Aliased Patterns (e.g., stripes or grids)

**Symptoms:**
- Visualizations show checkerboards or repeating structures in attention.

**Suggested Next Steps:**
- Add spatial positional noise or randomized patching.
- Regularize attention spectrum using Fourier-domain diversity loss.
- Replace absolute positional encoding with continuous embeddings.

---

## 275. Masked Tokens Are Ignored During Reconstruction

**Symptoms:**
- Model learns to reconstruct only visible tokens, not masked ones.

**Suggested Next Steps:**
- Increase mask ratio gradually via curriculum.
- Add contrastive or codebook prediction loss for masked positions.
- Penalize zero activations on masked token decoder path.

---

## 276. Projection Head Output Norms Explode Near Convergence

**Symptoms:**
- Norms of projected latents grow rapidly in final epochs.

**Suggested Next Steps:**
- Track per-step feature norm of the projector output.
- Add L2 norm penalty or apply unit-normalization.
- Introduce norm-aware learning rate decay in final epochs.

---

## 277. Positional Embeddings Create Unnatural Token Clustering

**Symptoms:**
- Tokens with nearby positions cluster tightly in feature space, despite different content.

**Suggested Next Steps:**
- Switch to relative or rotary position encoding.
- Add token disentanglement loss for similar-position tokens.
- Visualize positional similarity matrix in latent space.

---

## 278. EMA Encoder Lags Too Far Behind Online Model

**Symptoms:**
- Target encoder cannot keep up with fast-evolving online encoder.

**Suggested Next Steps:**
- Use cosine schedule for EMA decay (start low, increase to 0.99+).
- Re-initialize EMA with online model every N epochs.
- Evaluate lagged alignment loss to tune decay rate.

---

## 279. Patch-Based Models Fail When Spatial Layout is Slightly Shifted

**Symptoms:**
- Positional offset or pad changes drastically hurt output.

**Suggested Next Steps:**
- Use translation-invariant training (e.g., random crop + paste at center).
- Regularize using spatial consistency loss or multi-view matching.
- Test with padded vs. unpadded consistency checks.

---

## 280. Final Epochs Learn to “Undo” Augmentations Instead of Learning Robustness

**Symptoms:**
- Model memorizes augmentation reversal tricks (e.g., rotation, cropping).

**Suggested Next Steps:**
- Add augmentation-invariant loss (e.g., anchor–positive from different views).
- Randomize or combine augmentations more aggressively.
- Penalize predictability of augmentation parameters (adversarial masking).

---

## 281. Alignment Objective Creates Collapse to the Mean

**Symptoms:**
- Features from all views or samples converge to a shared average direction.

**Suggested Next Steps:**
- Introduce repulsion or contrastive components to balance alignment.
- Apply decorrelation loss (e.g., VICReg’s covariance or Barlow Twins).
- Monitor pairwise feature cosine similarity across non-matching views.

---

## 282. High-Frequency Input Features Are Suppressed in Deeper Layers

**Symptoms:**
- Early layers detect edges or textures, but deeper features ignore detail.

**Suggested Next Steps:**
- Add frequency-preserving skip connections (e.g., wavelet-based or dilated fusion).
- Use multi-resolution auxiliary tasks (e.g., contrastive loss at multiple scales).
- Apply spectral diversity regularization in the latent space.

---

## 283. Multiple Views Coalesce into a Single Cluster

**Symptoms:**
- All views of a sample (strong/weak) collapse to nearly identical latent vector.

**Suggested Next Steps:**
- Encourage view diversity via view-specific heads or dual-space mapping.
- Introduce inter-view repulsion loss (force non-identical views apart).
- Reduce augmentation strength if collapse is due to trivial transformations.

---

## 284. Batch Similarity Leaks into Representation

**Symptoms:**
- Samples within the same batch have more similar embeddings than across batches, regardless of content.

**Suggested Next Steps:**
- Shuffle batches more aggressively and evaluate batch-effect metrics.
- Normalize features across multiple batches (cross-batch whitening).
- Add inter-batch contrastive sampling (not just within-batch).

---

## 285. Decoder Introduces Representational Bias in Masked Architectures

**Symptoms:**
- Decoder architecture leads to trivial reconstructions that skew upstream representations.

**Suggested Next Steps:**
- Train decoder separately or apply stop-gradient at decoder input.
- Reduce decoder depth and increase masking ratio.
- Add auxiliary representation-preserving loss before decoder.

---

## 286. Representations Are Sensitive to Random Mask Locations

**Symptoms:**
- Slightly different masked token selections produce wildly different latents.

**Suggested Next Steps:**
- Apply mask consistency loss: multiple masks from same input should yield similar rep.
- Use smooth masking (Gaussian dropout or token fading).
- Evaluate with fixed-mask probes to measure sensitivity.

---

## 287. Patch Representations Are Too Correlated Within the Same Sample

**Symptoms:**
- All tokens from an image have nearly identical embeddings.

**Suggested Next Steps:**
- Apply token decorrelation loss (minimize intra-sample cosine similarity).
- Add patch-level contrastive loss across space and time.
- Train with partial occlusion and force token-level diversity.

---

## 288. Cross-View Representations Have Misaligned Feature Axes

**Symptoms:**
- Same content across views aligns poorly in latent space dimensions.

**Suggested Next Steps:**
- Use shared projection heads with axis alignment constraints.
- Apply Procrustes regularization between projected feature bases.
- Add PCA-consistency loss: align dominant axes over views.

---

## 289. Masked Inputs Encourage Overemphasis on Unmasked Tokens

**Symptoms:**
- Model puts too much representational weight on unmasked parts.

**Suggested Next Steps:**
- Train with mask drop — randomly remove unmasked info too.
- Penalize activation magnitude discrepancy between masked/unmasked tokens.
- Use uniform masking across samples to reduce bias.

---

## 290. Final Layer Drift Cancels Representational Gains from Previous Epochs

**Symptoms:**
- Validation accuracy or feature quality improves, then regresses suddenly.

**Suggested Next Steps:**
- Track EMA of final layer weights, not just encoder.
- Add prediction consistency loss between previous/future states.
- Use layer-wise learning rate decay to freeze or slow deeper layers.

---

## 291. Convolutional Kernels Are Reused Across Filters Without Meaningful Variation

**Symptoms:**
- Filters in a conv layer are copies or scalar multiples of each other.

**Suggested Next Steps:**
- Add filter diversity loss (e.g., pairwise orthogonality or cosine dissimilarity).
- Use group convolutions or depthwise separable convolutions to decouple features.
- Periodically re-initialize duplicate kernels mid-training.

---

## 292. Multimodal Fusion Layer Collapses to One Dominant Modality

**Symptoms:**
- Features from text/image/audio are ignored except one.

**Suggested Next Steps:**
- Add modality dropout to force reliance on all views.
- Apply modality balance loss — normalize representation contribution per modality.
- Use cross-attention fusion instead of concatenation or averaging.

---

## 293. Positional Encoding Leaks Sample Identity in Self-Supervised Tasks

**Symptoms:**
- Model memorizes fixed position patterns, indirectly revealing the sample.

**Suggested Next Steps:**
- Add position noise (jitter or dropout).
- Use relative positional embeddings instead of absolute ones.
- Randomize sequence length and offset to suppress overfitting.

---

## 294. Prototype-Based Models Collapse to Single-Class Dominance

**Symptoms:**
- All samples gravitate toward a few prototypes, ignoring others.

**Suggested Next Steps:**
- Regularize prototype usage with soft entropy loss or repulsion penalties.
- Initialize prototypes using KMeans++ or orthogonal vectors.
- Resample or replace unused prototypes during training.

---

## 295. Semantic Drift Occurs in Long Training Runs

**Symptoms:**
- Cluster centers or class boundaries slowly shift despite stable accuracy.

**Suggested Next Steps:**
- Log embedding drift over time using cosine or center distance metrics.
- Use EMA clustering or center smoothing to stabilize decision regions.
- Freeze latent geometry periodically and re-align with a probe head.

---

## 296. Patch-Based Models Overfit to Grid Layout

**Symptoms:**
- Slight jitter or patch shuffle significantly degrades performance.

**Suggested Next Steps:**
- Add random crop-to-patch misalignment during training.
- Train with patch order prediction task.
- Apply patch permutation or masked jigsaw as a regularizer.

---

## 297. Latent Representations Predict View Identity Instead of Content

**Symptoms:**
- SSL model can easily recover which augmentation was applied.

**Suggested Next Steps:**
- Penalize view classification accuracy via auxiliary adversarial task.
- Use uniform augmentation sampling to remove consistent view signals.
- Add view confusion loss to force content-invariance.

---

## 298. Self-Supervised Features Drift When Transitioning to Supervised Finetuning

**Symptoms:**
- Activations change radically after adding classification head.

**Suggested Next Steps:**
- Add frozen probe head during fine-tuning to stabilize encoder.
- Use gradual unfreezing of encoder layers.
- Apply feature distillation loss from pretraining phase to preserve geometry.

---

## 299. Last Few Epochs Introduce Sudden Class Collapse

**Symptoms:**
- Per-class accuracy or centroid spread drops suddenly at end of training.

**Suggested Next Steps:**
- Monitor class-wise variance and center drift.
- Apply early stopping with latent stability condition, not just loss plateau.
- Add anti-collapse loss that activates late (e.g., increasing VC strength).

---

## 300. Convergence Is Blocked by Overly Sharp Decision Boundaries

**Symptoms:**
- Accuracy plateaus, and misclassifications occur near class borders.

**Suggested Next Steps:**
- Add margin-based loss (e.g., ArcFace, cosine margin).
- Smooth predictions with temperature scaling.
- Use mixup or manifold mixup to blur decision transitions.

---

## 301. Latent Representations Suddenly Become Uniform Across Samples

**Symptoms:**
- High entropy early in training gives way to nearly identical features for all inputs.

**Suggested Next Steps:**
- Increase variance/covariance regularization strength dynamically over epochs.
- Add batch-level feature dispersion loss.
- Periodically inject latent noise or dropout near convergence.

---

## 302. Input Resolution Affects Representations More Than Content

**Symptoms:**
- t-SNE clusters separate by input size instead of class or content.

**Suggested Next Steps:**
- Normalize input scale or apply multi-resolution training.
- Add scale-consistency loss (e.g., features from rescaled input must match).
- Include random resize jitter in augmentation pipeline.

---

## 303. Asymmetric Collapse Occurs in One Branch of Joint-Encoder Architectures

**Symptoms:**
- Online encoder or target encoder collapses independently, causing loss to diverge.

**Suggested Next Steps:**
- Add branch-specific normalization or loss weighting.
- Use cosine EMA ramp to allow gradual weight sharing.
- Track and log branch-wise representation metrics (rank, entropy, norm).

---

## 304. Pretext Task Dominates and Prevents Transfer to Target Task

**Symptoms:**
- SSL-trained model underperforms even with fine-tuning on real labels.

**Suggested Next Steps:**
- Add semantic alignment loss to guide pretext-task signals.
- Pretrain with multiple proxy tasks for generalization.
- Evaluate probing accuracy across layers to localize entanglement.

---

## 305. Multi-Objective Training Causes One Loss to Mask Gradient of Another

**Symptoms:**
- Adding a new auxiliary loss causes original loss to stop decreasing.

**Suggested Next Steps:**
- Use gradient surgery (e.g., PCGrad, GradNorm) to avoid interference.
- Scale loss terms dynamically based on per-task learning progress.
- Monitor gradient norm per loss component per batch.

---

## 306. Convolutional Filters Are Directionally Biased (e.g., horizontal edges only)

**Symptoms:**
- Kernels specialize toward a limited orientation or spatial pattern.

**Suggested Next Steps:**
- Apply orientation-agnostic augmentation (e.g., rotation, flip).
- Use filter diversity regularization across direction-sensitive activations.
- Train with adversarial input rotations and minimize drift in predictions.

---

## 307. Latent Representations Memorize Class Distribution Statistics

**Symptoms:**
- Embeddings encode class frequency or batch-level priors, not features.

**Suggested Next Steps:**
- Train with uniform class sampling or balanced mini-batches.
- Add instance-level normalization to remove global structure leakage.
- Penalize representation–label frequency correlation directly.

---

## 308. Early Layers Output Almost Zero Activation Across All Inputs

**Symptoms:**
- First conv or transformer layers are effectively off (dead ReLUs or silent tokens).

**Suggested Next Steps:**
- Use positive bias initialization or switch to GELU/LeakyReLU.
- Track per-layer activation stats (mean, std) and visualize dead neurons.
- Add activation encouragement loss (non-zero unit penalty).

---

## 309. Representations Are Sensitive to Crop Origin Despite Global Pooling

**Symptoms:**
- Slight change in crop position causes large latent vector shift.

**Suggested Next Steps:**
- Add crop consistency loss: different crops should yield similar global feature.
- Include position-invariant modules (e.g., deformable attention or pooling).
- Reduce reliance on absolute coordinates by masking positional encodings.

---

## 310. Final Representations Encode Positional Order in Non-Sequential Tasks

**Symptoms:**
- Features retain sequence or patch order even for unordered prediction.

**Suggested Next Steps:**
- Remove or randomize positional encoding when unnecessary.
- Add shuffling or permutation invariance loss.
- Use set-based encoder heads (e.g., Set Transformer or DeepSets).

---

## 311. Masking Strategy Disrupts High-Level Semantics

**Symptoms:**
- Masked tokens randomly erase semantically important regions (e.g., object centers).

**Suggested Next Steps:**
- Use saliency-aware masking (mask background or low-activation tokens).
- Mix semantic and random masking for balance.
- Add auxiliary objectness prediction loss or bounding-box alignment.

---

## 312. Representation Quality Drops After Restoring from a Checkpoint

**Symptoms:**
- Re-loaded model shows worse accuracy or rank than before saving.

**Suggested Next Steps:**
- Validate that all buffers (e.g., BatchNorm stats) are saved/restored.
- Check for optimizer state mismatches (e.g., loss plateaus due to stale LR).
- Reload with EMA version if you used weight averaging during training.

---

## 313. Pretraining Leads to Overfitting Distribution-Specific Artifacts (e.g., lighting, background)

**Symptoms:**
- Features encode dataset-specific cues, hurting generalization.

**Suggested Next Steps:**
- Use domain randomization or style transfer augmentations.
- Add background suppression loss (e.g., foreground contrast only).
- Train with multiple datasets and minimize domain-class entanglement.

---

## 314. Modality Dropout Causes Sudden Accuracy Dips at Inference

**Symptoms:**
- When one modality is missing, performance collapses completely.

**Suggested Next Steps:**
- Train with modality dropout scheduling — gradually increase dropout strength.
- Introduce cross-modal redundancy loss: each view must encode shared content.
- Use modality-agnostic fusion mechanisms (e.g., attention-based fusion).

---

## 315. Hierarchical Labels Cause Embedding Confusion

**Symptoms:**
- Embeddings fail to distinguish subclasses if superclass labels are similar.

**Suggested Next Steps:**
- Use hierarchical contrastive loss (match coarse then fine labels).
- Train with multi-level projection heads (one per hierarchy level).
- Visualize embedding separation at each level (coarse/fine) over training.

---

## 316. Unsupervised Clustering Models Collapse to Uniform Cluster Usage

**Symptoms:**
- All data points are assigned to all clusters equally (no structure).

**Suggested Next Steps:**
- Add entropy maximization on assignments AND sample separation loss.
- Replace hard assignments with softmax + temperature annealing.
- Use mutual information maximization between input and cluster ID.

---

## 317. Contrastive Loss is Dominated by Easy Positives

**Symptoms:**
- Easy positive pairs cause the model to ignore harder, informative ones.

**Suggested Next Steps:**
- Use hard-positive mining or temperature-aware alignment loss.
- Track view difficulty and filter out trivial pairs.
- Add positive dropout: randomly remove the easiest matches.

---

## 318. Feature Usage is Imbalanced Across Latent Dimensions

**Symptoms:**
- Some embedding dimensions are always zero or inactive.

**Suggested Next Steps:**
- Apply feature decorrelation loss (e.g., Barlow Twins, VICReg).
- Use dimension-wise dropout during training.
- Track per-dimension variance and reinitialize dead ones mid-training.

---

## 319. Early Layers Leak Supervision Signals in SSL Pretraining

**Symptoms:**
- Probing reveals class info already in shallow layers — signs of shortcut.

**Suggested Next Steps:**
- Use deep supervision with noise injection in shallow layers.
- Mask class-correlated tokens or channels.
- Evaluate with layer-wise linear probes and shift learning into later blocks.

---

## 320. Representations Encode Sample Source or Collection Timestamp

**Symptoms:**
- Latents correlate with metadata fields instead of visual/audio features.

**Suggested Next Steps:**
- Apply adversarial debiasing to remove metadata identity from latents.
- Stratify sampling so that source/timestamp is uncorrelated with content.
- Add confounder contrastive loss to cancel non-content-based similarities.

---

## 321. SSL Representation Collapse Occurs Only With High Augmentation Strength

**Symptoms:**
- Mild augmentations yield stable training, but strong augmentations trigger collapse.

**Suggested Next Steps:**
- Implement augmentation strength curriculum: gradually increase difficulty.
- Use multiple view types (strong + weak) and align them separately.
- Add invariance thresholding — prevent over-aligning extremely distorted views.

---

## 322. Prototype Representations Become Confused Across Semantic Classes

**Symptoms:**
- Cluster centers drift toward overlapping class boundaries.

**Suggested Next Steps:**
- Apply class-aware repulsion loss between prototypes if labels are known.
- Add entropy regularization to sharpen assignment distributions.
- Use contrastive prototype refinement with harder negatives.

---

## 323. Learned Invariance Overgeneralizes Across Task-Relevant Variability

**Symptoms:**
- Model becomes invariant to differences that define downstream class (e.g., shape, pitch, accent).

**Suggested Next Steps:**
- Audit invariance-targeted augmentations and reduce task-relevant ones.
- Use view disagreement loss — penalize collapsing semantically distinct examples.
- Add view-specific contrastive task (e.g., intra-class contrast for fine-grained classes).

---

## 324. Representation Rank Collapses Within Batches, But Not Globally

**Symptoms:**
- Per-batch rank is low, but full dataset covariance appears full-rank.

**Suggested Next Steps:**
- Track batch-level covariance and rank — not just global stats.
- Increase inter-batch diversity by mixing from multiple sources/domains.
- Add mini-batch decorrelation loss or cross-batch consistency constraint.

---

## 325. Encoder Freezes When Trained With Powerful Decoder

**Symptoms:**
- Decoder reconstructs targets so well that encoder stops improving.

**Suggested Next Steps:**
- Detach decoder input or apply stop-gradient from decoder to encoder.
- Freeze decoder periodically to refocus updates on encoder.
- Shift loss weight to encoder-only objectives like contrastive or VC losses.

---

## 326. Representations Encode Relative Position Even in Permuted Sequences

**Symptoms:**
- Latents reflect token order even when permutation is irrelevant or random.

**Suggested Next Steps:**
- Remove positional encoding or replace with content-based bias.
- Add permutation-invariant loss or train with random shuffle tasks.
- Regularize using token-wise alignment invariance.

---

## 327. Similar Samples From Different Domains Fail to Align

**Symptoms:**
- Two images/text/audio segments with shared semantics diverge due to domain style.

**Suggested Next Steps:**
- Use domain-aware contrastive sampling (match cross-domain positives).
- Train with domain confusion loss to suppress source-style leakage.
- Apply style mixing augmentation to blend domain-specific priors.

---

## 328. Decoder Activations Leak Into Latent Representations

**Symptoms:**
- Representations align with decoder errors or reconstructions instead of input content.

**Suggested Next Steps:**
- Clamp decoder influence with loss stop-gradient at encoder output.
- Evaluate decoder-induced drift on frozen encoder checkpoints.
- Restructure model to use asymmetric architecture (e.g., BYOL-style).

---

## 329. Projector Collapse Only Happens in Late Fine-Tuning

**Symptoms:**
- SSL-pretrained projector collapses to mean vector after supervised finetuning begins.

**Suggested Next Steps:**
- Add projection-space regularization during fine-tuning (e.g., variance, norm constraints).
- Freeze projector or replace with a simpler linear mapping during downstream training.
- Introduce auxiliary contrastive head as backup representation stream.

---

## 330. Representations Vary Wildly With Seed Despite Similar Accuracy

**Symptoms:**
- Cosine similarity between same-sample latents is low across training runs.

**Suggested Next Steps:**
- Use feature alignment loss across seeds if ensembling is desired.
- Regularize latent space via global structure constraints (e.g., CKA, Procrustes).
- Log and compare latent manifold structure across seeds using t-SNE/PCA per run.

---

## 331. Contrastive Loss Reinforces Ambiguous Decision Boundaries

**Symptoms:**
- Latents from nearby but different classes become more similar than intra-class pairs.

**Suggested Next Steps:**
- Add label-aware margin constraints (e.g., SupCon with temperature annealing).
- Introduce false-negative detection or reweight hard positives.
- Visualize decision boundaries via probe classifiers over training.

---

## 332. Features Are Entangled With Specific Augmentation Parameters

**Symptoms:**
- Latents encode crop size, rotation angle, or blur level as a confounding factor.

**Suggested Next Steps:**
- Add augmentation-confusion loss (adversarial view masking).
- Train a predict-augmentation auxiliary task, then subtract gradient.
- Switch to discrete augmentation bins to remove regression signals.

---

## 333. Temporal Representations Fail to Generalize Across Different Sampling Rates

**Symptoms:**
- Changing audio/video fps breaks learned features.

**Suggested Next Steps:**
- Add rate-agnostic training (e.g., dynamic frame sampling or interpolation).
- Train with temporal warping augmentation.
- Align representations using temporal contrastive learning at multiple time resolutions.

---

## 334. Checkpoints Drift Semantically While Loss Remains Flat

**Symptoms:**
- Evaluation metrics stay stable, but latent features slowly change meaning over epochs.

**Suggested Next Steps:**
- Log representation-to-previous-checkpoint similarity every N steps.
- Use temporal smoothing of encoder weights (EMA, SWA).
- Add semantic stability regularizer (e.g., cluster center drift penalty).

---

## 335. Augmentation Echo Persists Across Epochs and Views

**Symptoms:**
- Model remembers augmentation-specific features and reuses them across unrelated examples.

**Suggested Next Steps:**
- Increase augmentation diversity and dropout probability.
- Add augmentation decorrelation loss.
- Track and reset view memory bias using inter-view feature similarity histograms.

---

## 336. Rank of Latent Space Collapses During Domain Transfer

**Symptoms:**
- Pretrained model’s representation is high-rank, but collapses after adapting to new domain.

**Suggested Next Steps:**
- Freeze encoder and train a domain-specific projector first.
- Use domain-specific batchnorm or instance normalization.
- Add rank-preserving loss (variance, entropy, PCA flatness constraint).

---

## 337. Fine-Tuning Degrades Feature Smoothness Across Similar Samples

**Symptoms:**
- Close examples (e.g., adjacent video frames) diverge post-finetuning.

**Suggested Next Steps:**
- Add temporal or spatial smoothness penalty (cosine or L2 between neighbors).
- Use frame/patch alignment objectives.
- Inject temporal dropout to regularize high-frequency shifts.

---

## 338. Self-Supervised Features Become Non-Monotonic in Class Hierarchies

**Symptoms:**
- Higher-level classes (e.g., animals vs. vehicles) show worse separation than sub-classes.

**Suggested Next Steps:**
- Add hierarchical structure loss (e.g., coarse-class contrast, taxonomic pull).
- Supervise coarse class with projection head A, fine class with head B.
- Train using multi-level N-pair loss.

---

## 339. Shared Encoder Becomes Overfit to a Single Projection Head

**Symptoms:**
- Features work well only for one head or objective.

**Suggested Next Steps:**
- Rotate or drop projection heads during training (head dropout).
- Add head decorrelation loss to encourage encoder reuse diversity.
- Use shared encoder, private heads with delayed update per head.

---

## 340. Decoder Gradient Fluctuations Destabilize Early Feature Learning

**Symptoms:**
- Early training shows high loss oscillation caused by decoder influence.

**Suggested Next Steps:**
- Clip decoder gradients or apply adaptive learning rate to decoder only.
- Train decoder separately for first few epochs, then rejoin.
- Use loss blending schedule: weight decoder loss from 0 → 1 gradually.

---

## 341. Latent Representations Are Correlated With Unused Auxiliary Inputs

**Symptoms:**
- Representations encode metadata (e.g., filename, padding tokens) even if unused in loss.

**Suggested Next Steps:**
- Apply gradient-blocking or removal of auxiliary tokens during forward pass.
- Audit dataloader and input pipeline for leakage paths.
- Add input attribution map to detect unwanted input influence.

---

## 342. Backbone Features Collapse When Attached to New Heads

**Symptoms:**
- Swapping classifier/projection heads causes representation drift or collapse.

**Suggested Next Steps:**
- Add feature retention loss between old and new head outputs.
- Freeze backbone during initial head training warmup.
- Use head adapter layers to minimize sharp representational transitions.

---

## 343. Learned Features Repeat Across Distant Samples

**Symptoms:**
- Different classes or domains map to identical feature vectors.

**Suggested Next Steps:**
- Introduce instance-level contrastive loss to decorrelate unrelated samples.
- Track nearest neighbor overlap between distinct classes.
- Add sample-specific dropout mask or latent augmentation.

---

## 344. Representations Diverge Across Views in Later Layers

**Symptoms:**
- Early layers align features from two views, later layers separate them.

**Suggested Next Steps:**
- Apply view-alignment regularization at multiple depths.
- Use multi-depth heads to enforce consistency throughout the stack.
- Freeze early layers and fine-tune only top layers during final alignment.

---

## 345. Frozen Features Are Forgotten During Finetuning

**Symptoms:**
- Previously frozen encoder features degrade when unfreezing later.

**Suggested Next Steps:**
- Use gradual unfreezing or layer-wise learning rate decay.
- Add anchor loss to preserve frozen-feature distributions.
- Store early feature statistics and penalize shift upon reactivation.

---

## 346. Representations Overdepend on Context Tokens or Prompts

**Symptoms:**
- Removal of a special token (e.g., CLS, BOS) ruins downstream task accuracy.

**Suggested Next Steps:**
- Apply prompt dropout or vary the prompt across training.
- Add prompt-invariance loss: features should be stable under prompt perturbation.
- Use multi-token pooling to avoid single-token reliance.

---

## 347. Decoder Learns Shortcuts to Reconstruct Without Semantics

**Symptoms:**
- Reconstructions are good, but encoder learns nothing useful (e.g., based on pixel statistics).

**Suggested Next Steps:**
- Add semantic supervision to encoder output (e.g., class prediction or clustering).
- Use codebook prediction instead of raw pixel loss (e.g., VQ or masked token prediction).
- Regularize decoder independence from shallow pixel patterns.

---

## 348. Class Prototypes Drift During Joint Learning With Augmentations

**Symptoms:**
- Adding new augmentation views causes prototype instability.

**Suggested Next Steps:**
- Stabilize prototypes via EMA updates or gradient clipping.
- Track prototype consistency across view changes.
- Anchor prototypes using center regularization or KMeans seeds.

---

## 349. Global Features Collapse While Local Tokens Stay Diverse

**Symptoms:**
- Global pooled vector is identical across inputs; token-wise features are fine.

**Suggested Next Steps:**
- Add global vector diversity loss (e.g., cosine dissimilarity across samples).
- Use attention pooling instead of mean/CLS pooling.
- Penalize under-distribution of pooled norms.

---

## 350. Representations Memorize Batch Composition Patterns

**Symptoms:**
- Embeddings depend on which other samples were seen together.

**Suggested Next Steps:**
- Detect and randomize batch ID bias by logging intra-batch similarity.
- Apply batch decorrelation loss or shuffle-based regularization.
- Introduce cross-batch matching losses to reduce co-occurrence effects.

---

## 351. Transformer Positional Embeddings Cause Overfitting to Input Length

**Symptoms:**
- Accuracy drops sharply when inference is run on slightly longer or shorter sequences.

**Suggested Next Steps:**
- Switch to relative positional encoding or rotary embeddings.
- Train with variable-length inputs to break length memorization.
- Evaluate with position sensitivity analysis (shifted and padded sequences).

---

## 352. Pretraining Collapses When Class Imbalance Exists in Augmented Views

**Symptoms:**
- SSL model collapses only when certain views are more common for some classes.

**Suggested Next Steps:**
- Track view–class co-occurrence matrix and rebalance sampling.
- Apply view-invariant class reweighting or stratified augmentation.
- Add view adversarial loss to suppress class-specific augment leakage.

---

## 353. Multi-View Features Align Globally but Differ Locally

**Symptoms:**
- Latents from two views are aligned as vectors, but differ in their internal token/patch representations.

**Suggested Next Steps:**
- Add token-level alignment loss or patch consistency regularization.
- Supervise intermediate tokens across views, not just pooled vectors.
- Penalize latent structure mismatch, e.g., via CKA or cosine similarity on tokens.

---

## 354. Model Generalizes to One Resolution But Fails at Others

**Symptoms:**
- Accuracy good on 224×224 but fails on 192×192 or 288×288.

**Suggested Next Steps:**
- Use multi-resolution pretraining or random resize sampling.
- Add scale-invariance loss or feature-matching across scales.
- Evaluate using resolution stress tests and fine-tune across sizes.

---

## 355. Shallow Layers Drift During Late-Stage Finetuning

**Symptoms:**
- Early encoder layers change significantly even after convergence.

**Suggested Next Steps:**
- Freeze shallow layers after N epochs or use differential learning rates.
- Add activation stability penalty for early blocks.
- Log feature shift by layer between checkpoints (e.g., cosine sim vs. epoch 10).

---

## 356. Model Overuses BatchNorm Statistics During Pretraining

**Symptoms:**
- Accuracy and loss change drastically when batchnorm runs in evaluation mode.

**Suggested Next Steps:**
- Replace BatchNorm with GroupNorm or LayerNorm for small-batch training.
- Use sync BatchNorm in multi-GPU or distributed setups.
- Log BN running stats and analyze their drift during training.

---

## 357. Feature Distribution Varies Too Widely Across Views

**Symptoms:**
- Each augmented view has a different feature scale, skew, or norm.

**Suggested Next Steps:**
- Normalize views before loss computation (F.normalize(z)).
- Add feature scale matching loss or enforce consistent batch stats.
- Visualize view-specific mean and variance across batches.

---

## 358. Decoder Pushes Representations Toward Shortcut Features

**Symptoms:**
- Decoder starts to use trivial color, corner, or low-frequency cues to reconstruct.

**Suggested Next Steps:**
- Randomize or drop shortcut-prone tokens (e.g., color-shuffled or border-masked inputs).
- Apply semantic bottleneck in decoder input (force high-level info only).
- Penalize decoder entropy when it converges to shortcut modes.

---

## 359. Pooled Representations Depend More on Empty Regions Than Signal

**Symptoms:**
- Padding or blank areas dominate attention or pooled outputs.

**Suggested Next Steps:**
- Mask out empty regions before pooling or scoring.
- Train with attention regularization that penalizes padding focus.
- Use learned pooling with context gates to downweight irrelevant zones.

---

## 360. Finetuned Model Has Fragile Embedding Geometry Across Random Seeds

**Symptoms:**
- Representations vary significantly even when accuracy is stable across seeds.

**Suggested Next Steps:**
- Add latent alignment constraints across runs (e.g., L2 between means, Procrustes alignment).
- Stabilize via EMA over multiple seeds or model averaging ensembles.
- Log t-SNE/PCA per run and detect geometric divergence.

---

## 361. Temporal Memory Fades in Models with Recurrent or Causal Attention

**Symptoms:**
- Long-term dependencies are lost, even with memory mechanisms enabled.

**Suggested Next Steps:**
- Evaluate using delayed-dependency benchmarks or shuffled-labels across time.
- Train with auxiliary recall tasks or next-chunk prediction.
- Reset memory buffers during training to prevent over-reliance on recent context.

---

## 362. Multi-Objective Training Leads to Task Forgetting Mid-Epoch

**Symptoms:**
- Improvement on one task erases progress on another (e.g., SSL + classification).

**Suggested Next Steps:**
- Use interleaved task sampling per step, not per epoch.
- Apply gradient projection methods (e.g., PCGrad or GradDrop).
- Add inter-task alignment metrics to monitor latent divergence.

---

## 363. Deeper Layers Exhibit Feature Collapse While Shallow Layers Remain Diverse

**Symptoms:**
- Top encoder layers produce low-variance or low-rank outputs.

**Suggested Next Steps:**
- Add layer-wise VC regularization instead of only final projection.
- Track variance and cosine similarity across all depths.
- Apply intermediate layer supervision to keep higher-level structure alive.

---

## 364. Aggressive Augmentation Reduces Latent Rank

**Symptoms:**
- Strong transformations (e.g., CutMix, RandAugment) lead to low-rank features.

**Suggested Next Steps:**
- Balance strong/weak augmentation in each batch (asymmetric view pairing).
- Train with augmentation-aware regularization (e.g., rank preservation loss).
- Monitor PCA eigenvalue entropy per augmentation type.

---

## 365. Token-Level Representations Are Highly Redundant Across Positions

**Symptoms:**
- Transformers produce almost identical patch/token embeddings.

**Suggested Next Steps:**
- Add patch-wise decorrelation loss or token diversity penalty.
- Increase drop-token probability to force local representation learning.
- Use local masking tasks or token jigsaw puzzles as auxiliary objectives.

---

## 366. Position Embedding Interpolation Causes Prediction Drift

**Symptoms:**
- When resizing inputs, position embedding resampling introduces semantic shifts.

**Suggested Next Steps:**
- Train with interpolation-aware encoding (e.g., continuous or rotary embeddings).
- Normalize position vectors after resize to avoid drift.
- Fine-tune with mixed-resolution training to calibrate embeddings.

---

## 367. Masked Models Overfit to the Unmasked Pattern

**Symptoms:**
- Masking strategy becomes predictable; model detects structure rather than semantics.

**Suggested Next Steps:**
- Add mask pattern dropout or block shuffle masking.
- Randomize mask geometry, not just ratio.
- Penalize mask-inference head if model starts predicting masked layout.

---

## 368. Multi-Task Heads Fight Over Shared Features

**Symptoms:**
- Head A improves while Head B’s loss increases, despite shared encoder.

**Suggested Next Steps:**
- Add orthogonality constraint between gradients from each head.
- Use per-head projection adapters to isolate learning signals.
- Apply gradient variance balancing per head using running stats.

---

## 369. Representations Encode Batch Identity Without Explicit Cues

**Symptoms:**
- Samples from the same batch have high cosine similarity, unrelated to content.

**Suggested Next Steps:**
- Add batch decorrelation loss across random batches.
- Apply sample-wise normalization to reduce shared bias.
- Introduce cross-batch memory bank for more stable comparisons.

---

## 370. Late-Stage Finetuning Unlearns Previously Acquired Invariances

**Symptoms:**
- Representations become sensitive to noise or crop despite initial robustness.

**Suggested Next Steps:**
- Add invariance preservation loss based on saved earlier representations.
- Reintroduce SSL-style contrastive alignment during finetuning.
- Freeze early blocks and fine-tune only head + mid encoder.

---

## 371. Frozen Layers Cause Distribution Shift Between Old and New Data

**Symptoms:**
- Pretrained frozen encoder layers misalign when dataset distribution changes.

**Suggested Next Steps:**
- Monitor activation statistics of frozen layers on new data.
- Use adapter layers between frozen and trainable components.
- Warm up on new domain with low LR unfreezing or distillation from frozen checkpoint.

---

## 372. Masking Interferes With Attention Pathways and Breaks Gradient Flow

**Symptoms:**
- High mask ratios cause loss spikes or vanishing gradients in transformer models.

**Suggested Next Steps:**
- Add mask-aware attention weighting to preserve gradient routes.
- Use multi-mask training (train with multiple overlapping mask sets).
- Evaluate gradient flow across layers using per-token gradient norms.

---

## 373. Prototypes Become Oversaturated (e.g., Assigned All Samples Equally)

**Symptoms:**
- Prototype assignment histograms become flat, without useful clustering.

**Suggested Next Steps:**
- Apply entropy penalty to prevent uniform usage.
- Replace softmax assignment with sharpened Gumbel-softmax.
- Periodically reinitialize or prune dead prototypes.

---

## 374. Learned Positional Biases Obstruct Generalization

**Symptoms:**
- Slight change in token order or spatial layout degrades performance severely.

**Suggested Next Steps:**
- Add layout shuffle augmentation or token permutation tasks.
- Replace fixed positions with content-based encodings.
- Evaluate with position variance heatmaps across layers.

---

## 375. Embedding Space Is Locally Quantized or Binned

**Symptoms:**
- Representations cluster around discrete anchor points, even without VQ.

**Suggested Next Steps:**
- Add smoothness loss (e.g., between close neighbors in latent space).
- Apply feature-space noise injection to prevent local stickiness.
- Penalize embedding histogram spikes or norm quantization.

---

## 376. Class-Invariant Pretraining Causes Downstream Confusion

**Symptoms:**
- SSL leads to representations where class-discriminative features are suppressed.

**Suggested Next Steps:**
- Add class-aware contrastive loss or pseudo-label clustering mid-pretraining.
- Introduce semantic prototypes early if known classes exist.
- Fine-tune with low-rank probing heads to detect subspace collapse.

---

## 377. Attention Collapse Happens in Early Blocks But Not Final Layers

**Symptoms:**
- First few attention heads have flat or degenerate maps, while upper layers are fine.

**Suggested Next Steps:**
- Add early block auxiliary tasks (e.g., token prediction, jigsaw).
- Apply attention entropy regularization to enforce early diversity.
- Delay dropout in early blocks until convergence stabilizes.

---

## 378. Projection Head Norm Shrinks Over Epochs, Weakening Downstream Signal

**Symptoms:**
- Final projection layer has low weight norm or vanishing gradients.

**Suggested Next Steps:**
- Normalize output vectors and enforce unit norm during training.
- Apply cosine classifier or centered loss for stability.
- Periodically rescale or reinitialize final projection layer.

---

## 379. Masked Autoencoding Creates Overconfidence in Visible Tokens

**Symptoms:**
- Decoder heavily amplifies unmasked regions, ignoring masked ones.

**Suggested Next Steps:**
- Apply loss reweighting to emphasize masked token recovery.
- Use target-aware masking (e.g., mask only semantically dense areas).
- Visualize decoder gradient attribution on masked vs. unmasked tokens.

---

## 380. Feature Drift Increases With Batch Size Without Affecting Loss

**Symptoms:**
- Larger batches cause representation instability while training metrics remain unchanged.

**Suggested Next Steps:**
- Log cosine similarity of features across batch sizes.
- Add batch-wise latent drift penalty or pairwise stability constraints.
- Adjust optimizer momentum or learning rate scaling for large-batch regimes.

---

## 381. Signal Propagation is Delayed in Deep Encoders

**Symptoms:**
- Early layers show near-zero activation or low variance even after convergence.

**Suggested Next Steps:**
- Add residual pre-activation blocks or shallow skip connections.
- Warm-start shallow layers with auxiliary shallow-task supervision.
- Apply ReZero-style initialization to allow gradual layer engagement.

---

## 382. False Positives Are Treated as True Alignments in Contrastive Training

**Symptoms:**
- Similar views from unrelated inputs align too well, degrading contrast.

**Suggested Next Steps:**
- Add false-positive detection via auxiliary clustering or view ID prediction.
- Lower temperature to sharpen contrastive logits.
- Filter hard false positives using semantic or spatial dissimilarity thresholds.

---

## 383. Bottleneck Layers Underutilize Their Capacity

**Symptoms:**
- High-capacity projection or hidden layers collapse to low-rank outputs.

**Suggested Next Steps:**
- Apply per-layer VC loss or eigenvalue entropy regularization.
- Introduce drop-path or stochastic skip to activate sparse units.
- Monitor and reactivate dead channels or units during training.

---

## 384. Hierarchical Models Learn Shortcuts via Coarse Supervision

**Symptoms:**
- Fine-grained discrimination fails because coarse labels dominate learning.

**Suggested Next Steps:**
- Train heads independently per level with minimal feature sharing.
- Apply conditional masking to suppress coarse signals during fine-class updates.
- Penalize class-pair similarity drift across hierarchy levels.

---

## 385. Positional Embeddings Are Reused Across Domains and Interfere

**Symptoms:**
- Domain A’s position encoding clashes with Domain B, causing collapse in transfer.

**Suggested Next Steps:**
- Reset or learn separate positional encodings per domain.
- Apply domain-specific projection adapters after position embedding.
- Use relative position encoding to decouple domain-specific coordinate grids.

---

## 386. Latent Representations Oscillate Around a Mean Vector

**Symptoms:**
- Features move in circles or ellipses in latent space, failing to stabilize.

**Suggested Next Steps:**
- Add momentum update smoothing or EMA projection heads.
- Penalize trajectory arc length across consecutive representations.
- Monitor latent trajectory curvature and clip high-frequency oscillations.

---

## 387. Label Leakage Through Augmentation Pipelines

**Symptoms:**
- Augmentations (e.g., color or crop type) correlate with class, causing shortcut learning.

**Suggested Next Steps:**
- Visualize augmentation-class heatmaps to detect correlation.
- Add label shuffling sanity checks to detect artificial signal paths.
- Train with view-level class confusion as a penalty term.

---

## 388. Low-Level Features Persist Despite High-Level Tasks

**Symptoms:**
- Edges, textures, and positional info dominate deep representations even during semantic tasks.

**Suggested Next Steps:**
- Apply low-level feature suppression loss in deep layers.
- Freeze shallow blocks and retrain heads to shift inductive bias.
- Use semantic-focused augmentations (e.g., CutMix, color erase).

---

## 389. Latent Distributions Differ Across Similar Classes

**Symptoms:**
- Similar classes (e.g., different dog breeds) have vastly different embedding distributions.

**Suggested Next Steps:**
- Add class-conditional mean/variance alignment loss.
- Use supervised contrastive loss with small temperature to promote tight clustering.
- Normalize per-class features before final projection.

---

## 390. Temporal Models Forget Early Input Tokens in Long Sequences

**Symptoms:**
- Attention and output quality decay over early time steps in long sequences.

**Suggested Next Steps:**
- Use global summary tokens or early token memory buffers.
- Add distance-weighted attention regularization to rebalance focus.
- Apply token dropout at later steps to encourage retention of early context.

---

## 391. Representations Saturate Near the End of Training

**Symptoms:**
- Cosine similarities between all samples converge toward 1.0, degrading class separability.

**Suggested Next Steps:**
- Add decorrelation or redundancy penalties (e.g., off-diagonal loss).
- Introduce hard negative sampling late in training to re-diversify.
- Reset or fine-tune projection head weights to break degeneracy.

---

## 392. Positional Tokens Dominate Embedding Space in Patch-Based Models

**Symptoms:**
- Learned positions (e.g., 0th patch, corner patches) create separable clusters in latent space.

**Suggested Next Steps:**
- Use relative or learned rotation-invariant positional encoding.
- Regularize by masking or perturbing position tokens during training.
- Visualize token PCA colored by position to audit spatial dominance.

---

## 393. Momentum Encoder Falls Behind View Distribution Changes

**Symptoms:**
- Online encoder adapts to new augmentations or data, but momentum encoder lags and harms alignment.

**Suggested Next Steps:**
- Use adaptive EMA schedule based on feature divergence or cosine similarity.
- Restart target encoder from online weights periodically.
- Reduce augmentation volatility mid-training to stabilize distributions.

---

## 394. Mixed-Domain SSL Training Causes Latent Collisions

**Symptoms:**
- Representations from multiple domains collapse to overlapping feature space without class signal.

**Suggested Next Steps:**
- Apply domain-specific projection heads or adapter modules.
- Use domain contrastive loss or domain confusion loss to enforce separation.
- Train with shared encoder + domain anchors to partition latent space.

---

## 395. Attention Maps Become Uniform Across Heads

**Symptoms:**
- All heads produce similar or flat attention distributions, reducing modeling capacity.

**Suggested Next Steps:**
- Add head-wise diversity regularization (e.g., pairwise KL or cosine distance).
- Introduce head dropout or per-head masking to force independence.
- Track attention entropy per head and penalize saturation.

---

## 396. Model Shows Early Generalization But Late Forgetting in Continual Learning

**Symptoms:**
- First tasks are learned well but then overwritten by later ones during finetuning.

**Suggested Next Steps:**
- Use elastic weight consolidation (EWC) or memory replay buffers.
- Penalize weight deviation from early-task optima.
- Train with interleaved data and balanced task sampling.

---

## 397. Masked Feature Maps Become Too Predictable Across Epochs

**Symptoms:**
- Decoder or projection layers learn to guess masked areas based on prior patterns.

**Suggested Next Steps:**
- Vary masking schemes epoch-to-epoch (e.g., Gaussian mask, region growing).
- Use adaptive masking based on attention entropy or novelty score.
- Penalize decoder for low prediction entropy on masked tokens.

---

## 398. Latent Space Favors Directionality Over Magnitude

**Symptoms:**
- Features align correctly directionally (cosine similarity) but vary widely in norm, breaking downstream tasks.

**Suggested Next Steps:**
- Normalize features post-projection or use cosine-based classifiers.
- Add norm consistency loss between views or augmentations.
- Visualize feature norm histograms across batches and training steps.

---

## 399. Low-Signal Classes Collapse in Feature Space

**Symptoms:**
- Rare or ambiguous classes become indistinguishable from each other.

**Suggested Next Steps:**
- Apply class-aware reweighting or dynamic temperature scaling.
- Train with focal contrastive loss or semantic margin loss.
- Augment minority classes with stronger or targeted views.

---

## 400. Final Layer Forgets Generalization Signal From Earlier Layers

**Symptoms:**
- Good representations exist in penultimate layer, but logits discard that structure.

**Suggested Next Steps:**
- Use linear probe during training to keep generalization signal alive.
- Add deep supervision loss or logit-to-latent alignment constraint.
- Freeze final layer and train only deeper encoder blocks to enforce feature continuity.

---

## 401. Representations Overfit to Anchor Points in Contrastive Learning

**Symptoms:**
- A few samples become universal attractors for others, dominating latent space.

**Suggested Next Steps:**
- Add instance frequency regularization to penalize repeated attraction.
- Use momentum queue filtering to drop overused anchors.
- Track anchor occurrence histograms and reset overrepresented ones.

---

## 402. Intermediate Layers Produce Redundant Representations Across Blocks

**Symptoms:**
- Activations from different layers are highly correlated; no feature hierarchy forms.

**Suggested Next Steps:**
- Apply layer-wise decorrelation loss or mutual information penalties.
- Use auxiliary heads to encourage specialization per depth.
- Train with depth dropouts or block shuffling to diversify layer behavior.

---

## 403. Augmentation Curriculum Breaks When Model Becomes Too Confident

**Symptoms:**
- Progressive augmentation strategy fails as model starts ignoring hard views.

**Suggested Next Steps:**
- Apply confidence-aware augmentation (increase strength based on prediction certainty).
- Introduce augmentation entropy regularization.
- Monitor loss gradient variance by view difficulty and adjust sampling.

---

## 404. Self-Supervised Objective Causes Sharp Minima in Loss Landscape

**Symptoms:**
- Small weight perturbations cause large drops in validation accuracy or latent alignment.

**Suggested Next Steps:**
- Add sharpness-aware minimization (SAM) or gradient smoothing.
- Use label-free mixup or representation interpolation to smooth landscape.
- Visualize loss curvature using eigenvalue approximation or flatness probes.

---

## 405. Attention Collapse Only Occurs for Tokens in a Specific Spatial Band

**Symptoms:**
- Tokens in horizontal/vertical slices receive no attention from other tokens.

**Suggested Next Steps:**
- Rotate or shift token grids during training (e.g., axial roll).
- Add attention map diversity loss across spatial regions.
- Track row/column attention entropy and apply dropout to dominant paths.

---

## 406. Projection Head Over-Compresses Features for Loss Minimization

**Symptoms:**
- Encoder output is informative, but projection loses most of it to minimize contrastive loss.

**Suggested Next Steps:**
- Detach encoder output and compare it to target using bypass loss.
- Add information bottleneck constraint to force minimal distortion.
- Introduce dual-head architecture (contrastive and reconstruction).

---

## 407. Cross-Entropy Loss Encourages Angular Collapse in Representation Space

**Symptoms:**
- All class vectors are tightly clustered along angular boundaries in hypersphere.

**Suggested Next Steps:**
- Replace dot-product classification with cosine-based loss (e.g., ArcFace, CosFace).
- Monitor angle histograms between class prototypes.
- Use margin-based supervision to space out class anchors.

---

## 408. LayerNorm Stabilizes Training but Hides Feature Collapse

**Symptoms:**
- Model trains without exploding/vanishing gradients, but latents become degenerate (e.g., rank-1).

**Suggested Next Steps:**
- Track covariance matrix eigenvalue spectrum, not just training loss.
- Apply post-norm decorrelation loss on raw (pre-norm) activations.
- Use alternative normalizations (GroupNorm, filter norm) for raw structure.

---

## 409. Encoder Output Becomes a Low-Energy Copy of the Input

**Symptoms:**
- Latents are highly correlated with raw input, especially in autoencoding setups.

**Suggested Next Steps:**
- Add input–latent mutual information penalty (encourage abstraction).
- Use invariant-target losses (e.g., clustering, contrast) alongside AE loss.
- Train with input corruption or adversarial masking.

---

## 410. Representations Collapse Along View-Specific Axes

**Symptoms:**
- Each view encodes information along different latent dimensions, causing misalignment.

**Suggested Next Steps:**
- Add axis alignment loss or PCA shape matching across views.
- Use shared projection subspace enforced by linear constraint layers.
- Penalize cross-view covariance mismatch in latent space.

---

## 411. Multitask Training Creates Conflicting Latent Geometry

**Symptoms:**
- Two tasks embed similar inputs in opposite regions of latent space.

**Suggested Next Steps:**
- Apply shared encoder + task-specific projector with orthogonal subspaces.
- Introduce geometry-aware loss (e.g., pairwise angle preservation).
- Log and visualize task-wise latent overlap using t-SNE or cosine histograms.

---

## 412. Semi-Supervised Setup Drifts Away From Labeled Anchor Points

**Symptoms:**
- SSL pretraining aligns views, but fine-tuning samples fail to map near labeled prototypes.

**Suggested Next Steps:**
- Include prototype alignment loss between SSL and supervised class anchors.
- Use label sharpening or teacher-student targets to retain supervision signal.
- Freeze encoder and retrain classifier head with constrained latent drift.

---

## 413. Multi-Head Architectures Converge to One Dominant Head

**Symptoms:**
- One head learns effectively, others stagnate or collapse.

**Suggested Next Steps:**
- Use independent dropout paths per head to promote specialization.
- Rotate head activation across epochs (e.g., round-robin or task-specific).
- Penalize cross-head similarity in outputs or parameter space.

---

## 414. Alignment Loss Becomes Incompatible With Downstream Task

**Symptoms:**
- Good alignment (e.g., cosine sim) between views reduces transfer accuracy.

**Suggested Next Steps:**
- Replace hard alignment with semantically aware agreement loss.
- Add downstream task feedback during SSL (e.g., online probe feedback loop).
- Use triangular loss: encoder-to-target, target-to-encoder, and decoder feedback.

---

## 415. Patch-Based Representations Alias Across Locations

**Symptoms:**
- Tokens from different spatial zones produce similar features, confusing decoder or classifier.

**Suggested Next Steps:**
- Use patch-relative positional encoding or absolute + delta mix.
- Train with patch permutation discrimination or masked reconstruction.
- Add token decorrelation constraint within samples.

---

## 416. Feature Norms Diverge Across Heads or Branches

**Symptoms:**
- One branch outputs unit-norm features; others shrink toward zero.

**Suggested Next Steps:**
- Apply norm normalization layers post-projection (F.normalize()).
- Add L2 norm balancing loss between branches.
- Regularize head norm variance across time and tasks.

---

## 417. Fine-Grained Features Are Lost After Global Pooling

**Symptoms:**
- Model ignores subtle differences between similar classes after averaging over space/time.

**Suggested Next Steps:**
- Use attention-based pooling or NetVLAD to retain discriminative tokens.
- Add token-aware prediction heads or per-token classification loss.
- Include local feature contrastive tasks alongside global objectives.

---

## 418. Masked Models Learn to Predict Mask Pattern Instead of Content

**Symptoms:**
- Decoder learns to guess layout of mask rather than reconstructing features.

**Suggested Next Steps:**
- Randomize mask shape, density, and noise across views.
- Penalize mask-predictive decoder activation (e.g., via adversarial auxiliary).
- Introduce mask consistency loss: different masks should yield similar output.

---

## 419. Early Training Oscillates Between Collapse and Divergence

**Symptoms:**
- Loss swings dramatically before stabilizing or collapsing entirely.

**Suggested Next Steps:**
- Warm-start encoder using gradient clipping + linear LR schedule.
- Use cosine LR with sharp restarts to smooth early optimization.
- Track gradient norms + output variance and inject noise adaptively.

---

## 420. Representations Become Too Similar Across Architectures in Ensembles

**Symptoms:**
- Different backbones yield highly redundant latent spaces.

**Suggested Next Steps:**
- Add ensemble-wise diversity loss on latent representations (e.g., CKA decorrelation).
- Use asymmetric augmentations per model in ensemble.
- Initialize with different random seeds + data bootstraps per architecture.

---

## 421. Architectural Reuse From Supervised Models Creates SSL Collapse

**Symptoms:**
- A supervised-trained architecture reused in SSL collapses quickly despite stable training.

**Suggested Next Steps:**
- Reset normalization layers and positional embeddings before SSL.
- Add early noise injection or stochastic reinitialization in final layers.
- Use architecture-specific SSL losses (e.g., SimSiam variants for ResNets, patch-based for ViTs).

---

## 422. Overpruned or Low-Capacity Encoders Fail to Maintain Representation Diversity

**Symptoms:**
- Shallow or thin models produce rank-deficient embeddings even early in training.

**Suggested Next Steps:**
- Increase projection head width or embedding dimension, even if encoder is small.
- Regularize latent spread with variance loss + entropy loss jointly.
- Replace deep pipeline with parallel lightweight heads and concatenate outputs.

---

## 423. Asymmetric Branches in SSL Fail to Align at Token-Level Granularity

**Symptoms:**
- Global features align, but token/patch features diverge across online and target branches.

**Suggested Next Steps:**
- Add cross-token alignment loss or token-wise contrastive head.
- Train with view mixing or patch-swap augmentation to create token variation.
- Freeze token encoder early and update only projection/pooling layers.

---

## 424. Feature Anchors Become Over-Specific and Resist Transfer

**Symptoms:**
- Class or cluster centers are so sharp they fail to generalize to near neighbors.

**Suggested Next Steps:**
- Replace hard anchors with center-of-mass features over multiple batches.
- Use soft prototypes with entropy-maximizing assignments.
- Penalize anchor–neighbor angular saturation to preserve class manifold smoothness.

---

## 425. Latent Representations Collapse to Hyper-Concentrated Regions

**Symptoms:**
- PCA or t-SNE reveals a very dense, compact core with few outliers.

**Suggested Next Steps:**
- Use volume-maximization loss (e.g., maximize pairwise L2 distance).
- Inject Gaussian or uniform latent noise with dropout-style schedules.
- Add repulsive regularization between all pairs of embeddings.

---

## 426. Strong Data Augmentation Reveals Memorization of Label-like Patterns

**Symptoms:**
- Model can still predict class after intense augmentation, suggesting overfitting.

**Suggested Next Steps:**
- Visualize prediction entropy under augmentation perturbations.
- Penalize low-entropy outputs when augmentation is high.
- Introduce augment-consistency loss only if prediction stability is semantically grounded.

---

## 427. Low-Level Features Leak into High-Level Task Heads

**Symptoms:**
- Final predictions depend on color, orientation, or border pixels even in semantic tasks.

**Suggested Next Steps:**
- Add texture suppression loss or style-removal contrastive pairs.
- Freeze shallow features and learn mid-level maps for decision branches.
- Train with color-shuffled or edge-erased augmentations.

---

## 428. Different Losses Compete to Align vs. Separate Similar Tokens

**Symptoms:**
- One loss pulls two tokens together, while another (e.g., class contrastive) pushes them apart.

**Suggested Next Steps:**
- Identify conflict using cosine angle of gradients between losses.
- Apply gradient surgery to remove conflicting components.
- Weight losses dynamically based on latent space curvature or cluster tightness.

---

## 429. Representation Learning Fails When Tokens Contain Mixed Semantics

**Symptoms:**
- One patch or token contains multiple objects, leading to noise in alignment.

**Suggested Next Steps:**
- Train with semantic token masking or object-aware segmentation (if available).
- Increase token granularity (smaller patches, overlapping crops).
- Use attention-weighted alignment instead of hard view matching.

---

## 430. Latent Drift Reverses After Checkpoint Reload Without Training Restart

**Symptoms:**
- Reloaded model outputs slowly move back toward pre-trained state despite same optimizer state.

**Suggested Next Steps:**
- Track optimizer and scheduler internal states (e.g., Adam’s moments).
- Store EMA-weighted models and validate from both raw and averaged checkpoints.
- Add re-anchor loss to preserve alignment with previous representation snapshots.

---

## 431. Representations Align Across Views But Lose Compositionality

**Symptoms:**

* Features match across augmentations but fail to generalize to new compositions (e.g., multiple objects, layered sounds).

**Suggested Next Steps:**

* Add compositional contrastive loss using pairs of object/object or phrase/phrase.
* Train with view mixing tasks (e.g., audio layering, CutMix).
* Use feature disentanglement losses to force substructure preservation.

---

## 432. Tokens Learn to Co-Adaptive Collapse Within a Single View

**Symptoms:**

* Tokens within the same sample encode redundant or correlated features.

**Suggested Next Steps:**

* Apply token-wise VC regularization (patch independence).
* Use attention head entropy regularization to enforce diversity.
* Add co-token decorrelation constraint in embedding space.

---

## 433. Representations Are Consistent Across Augmentations But Fragile to Time

**Symptoms:**

* SSL features hold under spatial views but change significantly across training epochs.

**Suggested Next Steps:**

* Track feature consistency over checkpoints using cosine similarity or kernel alignment.
* Use feature EMA loss or temporal consistency loss.
* Add check-in anchor loss every N epochs to preserve latent space structure.

---

## 434. Downstream Heads Forget Pretraining Alignment in Just a Few Batches

**Symptoms:**

* Finetuning a head on a pretrained encoder causes immediate drift or collapse.

**Suggested Next Steps:**

* Use warmup head training with frozen encoder for initial epochs.
* Add head-to-latent alignment loss (e.g., cosine sim with previous SSL features).
* Apply low learning rate or L2 penalty on head weights relative to identity.

---

## 435. Attention Maps Oscillate Between Tokens Without Converging

**Symptoms:**

* Token attention heads jump between unrelated anchors every few steps.

**Suggested Next Steps:**

* Apply attention map temporal smoothing (EMA attention or prediction-head averaging).
* Train with token dropout + identity-preserving regularization.
* Introduce attention consistency loss across epochs or views.

---

## 436. Contrastive Learning Over-Aligns Near Duplicates But Ignores Subtle In-Class Variation

**Symptoms:**

* Intra-class hard positives are aligned perfectly, but other variations are ignored.

**Suggested Next Steps:**

* Introduce view-aware angular margin loss to prevent total collapse.
* Sample intra-class negatives explicitly with metadata or pseudo-labels.
* Penalize over-alignment using feature variance or rank loss among positives.

---

## 437. Temporal Positional Encoding Drift Appears After Finetuning

**Symptoms:**

* Positional embeddings that work during pretraining misalign under supervised adaptation.

**Suggested Next Steps:**

* Replace absolute positions with rotary or offset-normalized encodings.
* Freeze temporal embeddings for first few downstream epochs.
* Evaluate position–class correlation drift after finetuning.

---

## 438. Pretraining Collapse is Triggered by Shortcut-Compatible View Combinations

**Symptoms:**

* Collapse only happens when certain combinations of augmentations are applied (e.g., grayscale + center crop).

**Suggested Next Steps:**

* Audit view combinations and add shortcut-aware sampling constraints.
* Penalize trivial similarity maps with low activation entropy.
* Add a view adversary network to detect and avoid shortcut reliance.

---

## 439. Head Overfitting Leads to Rotated Feature Spaces With Same Accuracy

**Symptoms:**

* Model achieves good classification accuracy, but embeddings are inconsistent or rotated unpredictably.

**Suggested Next Steps:**

* Track rotation-invariant metrics (e.g., kernel alignment, Procrustes distance).
* Apply latent alignment constraints across epochs.
* Penalize feature manifold drift even when logits remain stable.

---

## 440. Representations Show Modality-Specific Saturation in Multimodal Models

**Symptoms:**

* Audio-only or text-only inputs saturate latent space differently than combined views.

**Suggested Next Steps:**

* Normalize each modality branch with shared latent targets.
* Train with cross-modal reconstruction or prediction heads.
* Apply modality-wise latent variance equalization loss.

---

## 431. Representations Align Across Views But Lose Compositionality

**Symptoms:**

* Features match across augmentations but fail to generalize to new compositions (e.g., multiple objects, layered sounds).

**Suggested Next Steps:**

* Add compositional contrastive loss using pairs of object/object or phrase/phrase.
* Train with view mixing tasks (e.g., audio layering, CutMix).
* Use feature disentanglement losses to force substructure preservation.

---

## 432. Tokens Learn to Co-Adaptive Collapse Within a Single View

**Symptoms:**

* Tokens within the same sample encode redundant or correlated features.

**Suggested Next Steps:**

* Apply token-wise VC regularization (patch independence).
* Use attention head entropy regularization to enforce diversity.
* Add co-token decorrelation constraint in embedding space.

---

## 433. Representations Are Consistent Across Augmentations But Fragile to Time

**Symptoms:**

* SSL features hold under spatial views but change significantly across training epochs.

**Suggested Next Steps:**

* Track feature consistency over checkpoints using cosine similarity or kernel alignment.
* Use feature EMA loss or temporal consistency loss.
* Add check-in anchor loss every N epochs to preserve latent space structure.

---

## 434. Downstream Heads Forget Pretraining Alignment in Just a Few Batches

**Symptoms:**

* Finetuning a head on a pretrained encoder causes immediate drift or collapse.

**Suggested Next Steps:**

* Use warmup head training with frozen encoder for initial epochs.
* Add head-to-latent alignment loss (e.g., cosine sim with previous SSL features).
* Apply low learning rate or L2 penalty on head weights relative to identity.

---

## 435. Attention Maps Oscillate Between Tokens Without Converging

**Symptoms:**

* Token attention heads jump between unrelated anchors every few steps.

**Suggested Next Steps:**

* Apply attention map temporal smoothing (EMA attention or prediction-head averaging).
* Train with token dropout + identity-preserving regularization.
* Introduce attention consistency loss across epochs or views.

---

## 436. Contrastive Learning Over-Aligns Near Duplicates But Ignores Subtle In-Class Variation

**Symptoms:**

* Intra-class hard positives are aligned perfectly, but other variations are ignored.

**Suggested Next Steps:**

* Introduce view-aware angular margin loss to prevent total collapse.
* Sample intra-class negatives explicitly with metadata or pseudo-labels.
* Penalize over-alignment using feature variance or rank loss among positives.

---

## 437. Temporal Positional Encoding Drift Appears After Finetuning

**Symptoms:**

* Positional embeddings that work during pretraining misalign under supervised adaptation.

**Suggested Next Steps:**

* Replace absolute positions with rotary or offset-normalized encodings.
* Freeze temporal embeddings for first few downstream epochs.
* Evaluate position–class correlation drift after finetuning.

---

## 438. Pretraining Collapse is Triggered by Shortcut-Compatible View Combinations

**Symptoms:**

* Collapse only happens when certain combinations of augmentations are applied (e.g., grayscale + center crop).

**Suggested Next Steps:**

* Audit view combinations and add shortcut-aware sampling constraints.
* Penalize trivial similarity maps with low activation entropy.
* Add a view adversary network to detect and avoid shortcut reliance.

---

## 439. Head Overfitting Leads to Rotated Feature Spaces With Same Accuracy

**Symptoms:**

* Model achieves good classification accuracy, but embeddings are inconsistent or rotated unpredictably.

**Suggested Next Steps:**

* Track rotation-invariant metrics (e.g., kernel alignment, Procrustes distance).
* Apply latent alignment constraints across epochs.
* Penalize feature manifold drift even when logits remain stable.

---

## 440. Representations Show Modality-Specific Saturation in Multimodal Models

**Symptoms:**

* Audio-only or text-only inputs saturate latent space differently than combined views.

**Suggested Next Steps:**

* Normalize each modality branch with shared latent targets.
* Train with cross-modal reconstruction or prediction heads.
* Apply modality-wise latent variance equalization loss.

---

## 441. Latent Manifold Becomes Folded or Warped Under SSL Training

**Symptoms:**

* Nearby inputs map to distant representations, while some distant inputs are mapped close.

**Suggested Next Steps:**

* Add geodesic-preserving contrastive loss or local neighborhood alignment.
* Visualize latent space curvature with triplet margin violations.
* Encourage smoothness with manifold unfolding loss (e.g., Laplacian regularization).

---

## 442. Stochastic Training Noise Leaks Into Final Features

**Symptoms:**

* Dropout, data shuffle, or view randomness introduces volatile feature drift.

**Suggested Next Steps:**

* Apply EMA smoothing or batch-level latent averaging.
* Train with noise-aware stability loss between epochs or checkpoints.
* Evaluate with variance over seeds or dropout masks.

---

## 443. Backbone Choice Induces Architecture-Specific Collapse Modes

**Symptoms:**

* Same training config works on ResNet but fails on ViT, or vice versa.

**Suggested Next Steps:**

* Tailor augmentation + view pairing to backbone type (e.g., grid-aware for ViT).
* Replace projection heads with architecture-aware adapters.
* Run architecture sensitivity tests on stability metrics (rank, VC loss, cosine spread).

---

## 444. Fine-Grained Classes Collapse Together Under Global SSL Objectives

**Symptoms:**

* SSL fails to distinguish between subcategories with subtle semantic differences.

**Suggested Next Steps:**

* Add intra-class variation boosting loss (e.g., class-aware contrastive).
* Train with coarse + fine prototypes jointly.
* Introduce multi-scale contrastive samples from fine-to-coarse augmentations.

---

## 445. Multi-View Features Are Consistent at Global Scale But Diverge Locally

**Symptoms:**

* Global representations align, but patches or tokens don’t match across augmentations.

**Suggested Next Steps:**

* Add token-wise contrastive or jigsaw loss.
* Apply hierarchical contrastive training (e.g., align global, mid, and local levels).
* Penalize view divergence with multi-layer feature consistency loss.

---

## 446. Output Head Overfits to Early Samples, Ignoring Later Batches

**Symptoms:**

* Head performance saturates or forgets new classes rapidly.

**Suggested Next Steps:**

* Apply gradient balancing across mini-batches or class groups.
* Use logit smoothing or entropy-aware replay.
* Freeze classifier weights periodically to retain early structure.

---

## 447. Pretraining Objective Causes Non-Uniform Feature Use

**Symptoms:**

* Some embedding dimensions are active; others stay near zero across all batches.

**Suggested Next Steps:**

* Track feature usage histograms across training steps.
* Apply dimension-wise dropout or per-feature variance penalties.
* Encourage decorrelation with Barlow Twins-style whitening loss.

---

## 448. Representations from Parallel Branches (e.g., dual encoders) Drift Over Time

**Symptoms:**

* Encoders trained together produce aligned features initially, but diverge after many epochs.

**Suggested Next Steps:**

* Add drift penalty between encoder outputs over time.
* Use cyclical alignment checkpoints (e.g., sync every N steps).
* Introduce global projection head to unify both branches.

---

## 449. Projection Head Becomes a Bottleneck Under Optimization Pressure

**Symptoms:**

* Optimization focuses solely on projection weights, while encoder gradients vanish.

**Suggested Next Steps:**

* Add gradient balancing regularizer to equalize encoder/projection updates.
* Use residual projection heads or add skip connections from encoder.
* Periodically reset or randomize head weights to refresh encoder flow.

---

## 450. Latent Clusters Collapse Due to Uniform Sampling Noise

**Symptoms:**

* Noise in data (e.g., uniform crop jitter) causes class clusters to merge.

**Suggested Next Steps:**

* Replace uniform augmentation with task-aware view samplers.
* Add cluster-preserving regularization during noisy view training.
* Visualize latent shift per augmentation level to tune jitter/noise parameters.

---

## 451. Long-Tail Classes Collapse During SSL Pretraining

**Symptoms:**

* Rare or minority classes collapse to the same prototype or latent cluster.

**Suggested Next Steps:**

* Apply class-balanced sampling or synthetic augmentation for underrepresented classes.
* Add long-tail contrastive loss that reweights rare samples during alignment.
* Track class-wise cluster variance and apply adaptive prototype smoothing.

---

## 452. Initialization Bias Persists Into Mid or Late Training

**Symptoms:**

* Features remain close to initial weight directions despite large updates elsewhere.

**Suggested Next Steps:**

* Use orthogonal or normalized initialization for heads and projection layers.
* Track cosine sim to initial embeddings and penalize frozen dimensions.
* Inject early-stage noise-based loss to overcome initialization stickiness.

---

## 453. Temporal Contrastive Models Collapse on Short or Slow Sequences

**Symptoms:**

* Models trained on videos or audio with little temporal change learn near-constant representations.

**Suggested Next Steps:**

* Add temporal jigsaw prediction or delta token contrast as auxiliary losses.
* Increase temporal stride or sample spacing in contrastive view selection.
* Penalize low inter-frame distance in latent space during training.

---

## 454. Attention Heads Co-Adapt and Lose Specialization

**Symptoms:**

* All heads converge to similar attention maps regardless of token, layer, or task.

**Suggested Next Steps:**

* Apply drop-head regularization where only subsets of heads are used per batch.
* Visualize and audit attention similarity matrix across heads and time.

---

## 455. Latent Representations Reuse the Same Principal Axes Across Tasks

**Symptoms:**

* PCA shows consistent top-3 directions regardless of training phase or data.

**Suggested Next Steps:**

* Add axis decorrelation loss across task branches or checkpoints.
* Penalize repeated high-variance directions in latent covariance.
* Use random rotation augmentation in feature space during auxiliary tasks.

---

## 456. Mixed-View Models (e.g., multimodal) Collapse to a Single Dominant View

**Symptoms:**

* Audio-text or image-text encoders align fully with only one modality during training.

**Suggested Next Steps:**

* Use modality dropout and train view-specific projection heads.
* Add cross-modal contrastive loss with per-view balancing.
* Monitor view-specific feature norms and apply reweighting when imbalance appears.

---

## 457. Token Representations Saturate With Identical Values

**Symptoms:**

* All patch or word tokens collapse to the same feature vector even with masking and dropout.

**Suggested Next Steps:**

* Track intra-token cosine sim distributions.
* Add token variance loss or patch decorrelation regularizer.
* Replace pooling with attention-weighted summary tokens.

---

## 458. LayerNorm Masks Feature Collapse During Evaluation

**Symptoms:**

* Evaluation loss appears stable, but raw features are degenerate (e.g., low-rank or high cosine sim).

**Suggested Next Steps:**

* Track pre-LayerNorm statistics during validation (not just post-norm).
* Apply norm-free feature logging or raw-space contrastive metrics.
* Penalize hidden layer variance drop with intermediate variance loss.

---

## 459. Token Jitter Augmentations Induce Semantic Drift

**Symptoms:**

* Slight token shifts or spatial noise cause large changes in feature output.

**Suggested Next Steps:**

* Train with token jitter consistency loss.
* Use semantic-aware masking or region-based dropout to smooth token response.
* Add token attention averaging to reduce sharp boundaries in shift-sensitive heads.

---

## 460. Global Representation Is Stable, But Local Tokens Collapse

**Symptoms:**

* CLS or pooled feature is semantically meaningful, but token grid is nearly constant.

**Suggested Next Steps:**

* Add token contrastive loss to preserve spatial detail.
* Use token-wise heads for auxiliary tasks (e.g., segmentation or patch prediction).
* Penalize token covariance degeneracy alongside VC loss at the global level.

---

## 461. Inter-Token Binding Is Lost in Deep Transformer Layers

**Symptoms:**

* Tokens operate independently at deep layers, ignoring relational context.

**Suggested Next Steps:**

* Add relational contrastive loss or token pairwise prediction.
* Use residual token fusion (merge intermediate token states across depths).
* Penalize attention entropy collapse across token pairs.

---

## 462. Projection Head Learns to Memorize Identity Mapping

**Symptoms:**

* Projection output mirrors encoder output or target view directly.

**Suggested Next Steps:**

* Introduce predictor bottleneck with non-linear compression.
* Apply stop-gradient to both encoder and projector, and use a separate predictor.
* Penalize low-rank projection weights using Frobenius norm or eigenvalue diversity.

---

## 463. Modality Injection (e.g., textual prompts into image model) Causes Latent Drift

**Symptoms:**

* Adding auxiliary inputs causes features to move off the pretrained distribution manifold.

**Suggested Next Steps:**

* Use gated modality fusion to control auxiliary influence.
* Train with modality-aware normalization layers (e.g., FiLM).
* Apply feature space anchoring loss to retain pretrained embedding geometry.

---

## 464. Model Learns to Ignore Specific Patches or Tokens Over Time

**Symptoms:**

* Certain token positions become “dead zones” — minimal attention, gradient, or activation.

**Suggested Next Steps:**

* Randomly drop token positions to prevent over-adaptation.
* Visualize token attention heatmaps across time and location.
* Apply activation-level dropout to low-variance token outputs.

---

## 465. Loss Landscape Becomes Flat Yet Highly Localized (Sharp at Low Scale)

**Symptoms:**

* Training appears stable but responds poorly to minor parameter changes or retraining.

**Suggested Next Steps:**

* Apply Sharpness-Aware Minimization (SAM) or loss smoothing techniques.
* Perturb parameters periodically and monitor loss increase curvature.
* Track local Hessian approximation or use eigenvalue clipping.

---

## 466. View-Specific Semantic Corruption Appears Only After View Augmentations

**Symptoms:**

* Representations change meaning under specific crop, blur, or color jitter settings.

**Suggested Next Steps:**

* Detect and penalize semantic shift across augmentations using probing heads.
* Apply semantic consistency loss on pseudo-labels across views.
* Mask out trivial patterns (e.g., border hints) that induce shortcut paths.

---

## 467. View-Asymmetric Masking Causes Inconsistent Representations

**Symptoms:**

* Different masking patterns between two views distort alignment and cause latent inconsistency.

**Suggested Next Steps:**

* Use partially overlapping masking schedules with fixed anchors.
* Apply alignment loss only on shared visible tokens.
* Visualize masked view heatmaps and enforce consistency in unmasked token regions.

---

## 468. Representations Collapse Only When Training at Reduced Precision (e.g., FP16)

**Symptoms:**

* Model works in full precision but collapses or diverges in mixed-precision regimes.

**Suggested Next Steps:**

* Track grad norm magnitudes and underflow events in FP16 training.
* Use loss scaling and master weights to stabilize FP16 gradients.
* Apply precision-aware normalization layers (e.g., LayerNorm over float32 buffer).

---

## 469. Token-Level Features Show Class-Independent Drift

**Symptoms:**

* Tokens become detached from semantic classes, even when global representation is accurate.

**Suggested Next Steps:**

* Train with token-wise class anchors or semantic patch heads.
* Add local-global alignment loss to enforce consistency between pooled and token-level features.
* Penalize intra-class token feature variance across samples.

---

## 470. Mixed-View or Multi-Scale Augmentations Destroy Semantic Clusters

**Symptoms:**

* Introducing scale or crop variation scatters samples from the same class.

**Suggested Next Steps:**

* Apply scale-invariant latent alignment loss.
* Use progressive augmentation schedule, increasing view diversity over time.
* Introduce view-cluster anchoring — align augmented views to a shared semantic centroid.

---

## 471. Adaptive Schedulers Cause Late-Stage Collapse in SSL Models

**Symptoms:**

* Model performs well early, but collapses after LR or weight decay schedule shifts.

**Suggested Next Steps:**

* Freeze momentum encoder update rate after warmup.
* Add schedule-aware regularization (e.g., increase variance loss as LR decays).
* Delay final LR step or decay projection head LR separately.

---

## 472. Model Learns to Treat Some Tokens as Noise, Even When Informative

**Symptoms:**

* Certain positions are consistently ignored by attention or heads across all views.

**Suggested Next Steps:**

* Apply token reweighting or token dropout schedules.
* Add position entropy loss or token salience boosting loss.
* Use random token masking in informative positions to test impact.

---

## 473. Representations Drift Severely When Fine-Tuning on Noisy Labels

**Symptoms:**

* SSL representations become entangled with label noise during supervised adaptation.

**Suggested Next Steps:**

* Use tempered softmax, label smoothing, or co-teaching.
* Freeze encoder and finetune on high-confidence subsets first.
* Apply noise-robust contrastive loss (e.g., unsupervised sample agreement under labels).

---

## 474. Fine-Tuning on Small Supervised Sets Erases SSL Semantics

**Symptoms:**

* Probes or classifiers overwrite generalizable SSL structure immediately.

**Suggested Next Steps:**

* Use low-rank projection heads with orthogonal initialization.
* Add semantic retention loss based on checkpointed SSL embeddings.
* Fine-tune under feature-preserving L2 loss to resist drift.

---

## 475. Architecture Instability Emerges on Smaller Datasets (e.g., ViT on CIFAR-10)

**Symptoms:**

* Larger models collapse when trained on small-scale or low-resolution inputs.

**Suggested Next Steps:**

* Switch to patchless tokens (e.g., convolutions before ViT).
* Add local inductive bias via hybrid CNN-Transformer blocks.
* Scale embedding dim and head count to input size.

---

## 476. Class-Wise Representations Drift Toward Opposing Axes

**Symptoms:**

* Each class occupies a nearly orthogonal or reflected region in feature space, harming generalization.

**Suggested Next Steps:**

* Add center anchoring loss across adjacent classes.
* Use angular margin loss to align related classes near shared bases.
* Penalize representation polarity across symmetric classes.

---

## 477. Curriculum-Based View Difficulty Leads to Shortcut Overlearning

**Symptoms:**

* Easier views are memorized; harder views are skipped or ignored.

**Suggested Next Steps:**

* Apply uncertainty-weighted view balancing.
* Alternate hard/easy view ratios dynamically.
* Penalize view collapse entropy (i.e., same feature from all views).

---

## 478. Representations Diverge When Switching Optimizer Mid-Training

**Symptoms:**

* Changing from Adam to SGD (or vice versa) leads to collapse or loss spike.

**Suggested Next Steps:**

* Warm-start optimizer with matched momentum and LR scaling.
* Use optimizer interpolation over multiple steps.
* Freeze sensitive layers (e.g., batchnorm, projection) during optimizer transition.

---

## 479. Learned Representations Overfit to Patch Size

**Symptoms:**

* Change in patch resolution or stride causes large drop in accuracy or drift in latent space.

**Suggested Next Steps:**

* Train with variable patch sizes or multi-scale tokens.
* Normalize token size sensitivity with patch-aware dropout.
* Add token scale-invariance loss across patch size augmentations.

---

## 480. Self-Supervised Pretraining Leads to Consistent Under-Clustering

**Symptoms:**

* Representation space contains few tight clusters regardless of dataset complexity.

**Suggested Next Steps:**

* Increase latent dimensionality or number of prototypes.
* Add cluster sharpening loss or increase temperature in assignment.
* Use multi-stage contrastive pretraining, starting broad, ending fine-grained.

---

## 481. Representations Collapse Along a Few Overused Directions in Latent Space

**Symptoms:**

* Most features project strongly onto just a handful of axes, reducing expressiveness.

**Suggested Next Steps:**

* Penalize dominant eigenvalues in the latent covariance matrix.
* Use feature whitening losses (Barlow Twins, VICReg) to balance axis usage.
* Add random rotation in feature space during training.

---

## 482. Asymmetric View Collapse Only Happens in Online Encoder

**Symptoms:**

* Momentum encoder produces diverse outputs, but online encoder collapses to trivial states.

**Suggested Next Steps:**

* Reduce prediction head capacity or use nonlinear bottlenecks.
* Add consistency loss across branches at intermediate layers.
* Increase update rate of momentum encoder to stabilize learning signal.

---

## 483. Class Prototypes Converge to Class Mean But Lose Internal Variation

**Symptoms:**

* All class features hover around class mean vector with minimal spread.

**Suggested Next Steps:**

* Add intra-class diversity penalty (e.g., minimum pairwise distance).
* Use instance + class-level loss hybrid (e.g., supervised SimCLR).
* Visualize and optimize for per-class feature entropy.

---

## 484. Dropout Masks Induce Inconsistency Across Views

**Symptoms:**

* Stochastic dropout causes large feature drift even with same input and augmentation.

**Suggested Next Steps:**

* Freeze dropout masks for view pairs or use deterministic dropout during contrastive passes.
* Add dropout stability loss across stochastic samples.
* Average multiple passes per input to stabilize features under randomness.

---

## 485. Decoder Bottlenecks Create Latent Space Over-Compression

**Symptoms:**

* Latents collapse to low-norm, highly entangled states after passing through decoder during pretext tasks.

**Suggested Next Steps:**

* Widen decoder or increase activation magnitude via skip connections.
* Add detached decoder branch (no gradient) for stable upstream learning.
* Train with mutual information maximization between input and latent instead of only decoder loss.

---

## 486. Spatial Attention Heads Focus Only on Central or Salient Regions

**Symptoms:**

* Tokens outside central patches receive minimal attention across all layers.

**Suggested Next Steps:**

* Use spatial dropout or region-based dropout during training.
* Apply attention entropy regularization per token location.
* Penalize spatial attention bias using uniformity constraints.

---

## 487. Features Encode Data Order or Sampling Pattern

**Symptoms:**

* Batch ID or sample index becomes encoded in latent representation.

**Suggested Next Steps:**

* Shuffle batches more aggressively and test for batch consistency artifacts.
* Add batch decorrelation loss or shuffle-invariance regularization.
* Train with index permutation probes to audit latent encoding behavior.

---

## 488. Prototypes Attract Unrelated Samples in Early Training

**Symptoms:**

* Wrong class samples are frequently pulled toward early prototype vectors.

**Suggested Next Steps:**

* Warm up using instance-level contrast before introducing prototypes.
* Add confidence-aware assignment filtering (e.g., Sinkhorn, entropy threshold).
* Track prototype assignment stability over batches and delay update until stable.

---

## 489. Representation Space Rotates Periodically During Training

**Symptoms:**

* Latent directions drift in a cyclic pattern even though accuracy is stable.

**Suggested Next Steps:**

* Add temporal alignment loss between embeddings across checkpoints.
* Use EMA anchors or rotation-penalizing constraints on latent basis.
* Visualize cosine drift or Procrustes distance over time.

---

## 490. SSL Training Works on Original Data, Collapses on Transformed or Augmented Versions

**Symptoms:**

* Model collapses when pretraining on spectrograms, edge maps, depth images, etc.

**Suggested Next Steps:**

* Tailor augmentations to respect transformed modality (e.g., frequency masking for spectrograms).
* Use modality-specific pretext task (e.g., masked frequency modeling).
* Initialize with weights trained on similar modalities and gradually adapt.

---

## 491. Gradients Bypass the Encoder via Overpowered Prediction Heads

**Symptoms:**

* Most updates affect the head layers while the encoder’s parameters remain nearly static.

**Suggested Next Steps:**

* Apply gradient norm balancing across encoder and head.
* Introduce gradient gating or stop-grad inside prediction head.
* Reduce head capacity and add bottleneck layers with dropout.

---

## 492. Decoder Forces Representations to Encode Reconstruction Artifacts

**Symptoms:**

* Features contain irrelevant structure (e.g., texture, edges) due to decoder loss minimization.

**Suggested Next Steps:**

* Add auxiliary semantic head to enforce content-based structure.
* Train decoder and encoder with disjoint objectives (e.g., decoder for noise, encoder for contrast).
* Use adversarial decoder loss to penalize non-semantic reconstruction cues.

---

## 493. Class Separation Appears in Low-Dimensional Embedding Projections Only

**Symptoms:**

* t-SNE or PCA shows class clusters, but full representations lack separability.

**Suggested Next Steps:**

* Use contrastive probes on raw features instead of projection slices.
* Add class-wise margin loss in full-dimensional space.
* Penalize superficial clustering using curvature or class spread metrics.

---

## 494. Head Layers Learn to Memorize Temporal Order or Augmentation Identity

**Symptoms:**

* Classifier outputs differ based on sample time or augment metadata, not content.

**Suggested Next Steps:**

* Train with time-invariant loss or temporal shuffling tasks.
* Add augmentation-invariance head with a decorrelation objective.
* Penalize logit divergence across identical content under different views.

---

## 495. Early-Stage Over-Alignment Between Views Prevents Feature Diversity Later

**Symptoms:**

* Cosine similarity spikes early in training, followed by stagnation or collapse.

**Suggested Next Steps:**

* Use delayed alignment schedule (start with VC loss only).
* Add orthogonality constraint between view encodings during early epochs.
* Warm up with instance-level variance maximization before view alignment begins.

---

## 496. Global Features Dominate Over Local Tokens in Classification Tasks

**Symptoms:**

* Class decisions rely heavily on pooled vector while ignoring patch-level signals.

**Suggested Next Steps:**

* Add token-level auxiliary tasks (e.g., patch classification, patch-to-label attention).
* Use token mixing via attention dropout or NetVLAD pooling to increase locality.
* Regularize pooled vector variance relative to token variance.

---

## 497. Pretraining Fails on Small Datasets with Limited Augmentation Diversity

**Symptoms:**

* Self-supervised training collapses due to insufficient intra-class or inter-view variation.

**Suggested Next Steps:**

* Apply synthetic augmentation generation (e.g., augment pipelines, GANs, jittered views).
* Reduce reliance on contrastive alignment — use reconstruction or jigsaw tasks.
* Add per-class variance loss to encourage feature spread even on small data.

---

## 498. Prototypes or Cluster Centers Get Stuck in Unused Regions of Feature Space

**Symptoms:**

* Prototypes don’t update and remain far from active sample embeddings.

**Suggested Next Steps:**

* Prune or reinitialize inactive prototypes every N steps.
* Add EMA-based prototype updates rather than gradient-based only.
* Use center attraction and repulsion constraints to rebalance.

---

## 499. Sharp Transition in Loss Components Causes Collapse During Multi-Phase Training

**Symptoms:**

* Switching from one objective (e.g., MSE) to another (e.g., contrastive) causes instability.

**Suggested Next Steps:**

* Gradually anneal loss weights between phases.
* Use a composite hybrid loss with shared normalization and joint optimization.
* Re-initialize prediction head or projector between loss phases.

---

## 500. SSL Model Learns Trivial Identity Mappings When Augmentations Are Too Weak

**Symptoms:**

* Latent vectors match across views with no abstraction or compression.

**Suggested Next Steps:**

* Audit and increase augmentation strength or variety (blur, crop, noise, color jitter).
* Introduce view disagreement loss to penalize overly matched trivial views.
* Add semantic masking or token dropout to break shortcut paths.

---

## 501. View Pairs Have High Mutual Information But Encode Redundant Semantics

**Symptoms:**

* SSL model aligns views perfectly, but latents contain no novel or structured information.

**Suggested Next Steps:**

* Add conditional entropy loss to enforce information gain per view.
* Replace or augment contrastive loss with predictive coding or multi-task decoding.
* Introduce multi-view rotation or permutation tasks to push latent diversity.

---

## 502. Model Overfits to Augmentation Identity Instead of View-Invariant Semantics

**Symptoms:**

* Outputs encode the type of augmentation (e.g., rotation angle, blur intensity).

**Suggested Next Steps:**

* Train with augmentation-invariance loss or confusion loss.
* Penalize augmentation classification accuracy using an adversarial head.
* Add mix-augmented views (blur + crop + color jitter) to prevent isolation of signals.

---

## 503. Masked Tokens Leak Into Latent Representations During MAE Training

**Symptoms:**

* Masked areas influence encoder output despite masking being intended to block gradients.

**Suggested Next Steps:**

* Audit data pipeline for mask dropout or buffer reuse bugs.
* Add detached masking token insertion or zero-masked input patches.
* Visualize gradient maps to confirm true masking path.

---

## 504. Dual-Branch Losses Compete and Cause Representation Collapse

**Symptoms:**

* Contrastive + reconstruction or clustering loss cause instability.

**Suggested Next Steps:**

* Use gradient surgery or loss orthogonalization to prevent update interference.
* Train alternating batches per loss or apply weighted interpolation schedule.
* Introduce shared pre-head latent space, then branch separately.

---

## 505. Model Learns to Ignore Contrastive Targets in Presence of Reconstruction Decoder

**Symptoms:**

* Strong decoder leads to contrastive loss stagnation.

**Suggested Next Steps:**

* Freeze decoder or detach gradients from decoder targets during contrastive optimization.
* Normalize decoder path loss weight based on encoder update ratio.
* Add contrastive alignment head separate from decoder branch.

---

## 506. Temporal Sampling Drift Creates View Inconsistency in Sequential Data

**Symptoms:**

* Different time-sampling schemes produce unpredictable latent drift.

**Suggested Next Steps:**

* Use temporal contrastive prediction tasks with timestamp embeddings.
* Align across time using cross-sequence token similarity loss.
* Regularize with time-aware MSE loss across augment paths.

---

## 507. Optimizer Schedules Misalign Across Branches (e.g., momentum vs. online)

**Symptoms:**

* EMA branch updates slower or faster than online, leading to collapse or lag.

**Suggested Next Steps:**

* Use cosine ramped EMA decay (slow → fast over time).
* Sync projection heads across branches periodically.
* Apply drift monitoring with cosine sim checks between branches.

---

## 508. Gradients Oscillate in Sign for Critical Layers

**Symptoms:**

* Per-layer or per-feature gradients flip directions frequently without convergence.

**Suggested Next Steps:**

* Apply momentum clipping or cosine-decayed learning rates.
* Use gradient variance penalty across time for unstable layers.
* Train with second-order update approximations (e.g., AdaHessian).

---

## 509. SSL Model Retains Too Much Input Detail, Blocking Abstraction

**Symptoms:**

* Model reproduces input too well but fails to generalize across class or domain.

**Suggested Next Steps:**

* Apply semantic abstraction loss (contrastive + decoder disagreement).
* Add autoencoder noise injection or semantic shuffling in input space.
* Penalize token redundancy or L2 norm density in intermediate layers.

---

## 510. Token Similarity Graph Becomes Disconnected Mid-Training

**Symptoms:**

* Token-token similarity matrix becomes block-diagonal or fragmented across views.

**Suggested Next Steps:**

* Use token graph regularization (e.g., Laplacian smoothing).
* Apply inter-token KL divergence penalty across augmented views.
* Train with graph-based pooling or graph contrastive learning modules.

---

## 511. Pseudo-Labels Become Noisy and Unstable During Semi-Supervised Pretraining

**Symptoms:**

* Model’s self-generated labels fluctuate or flip repeatedly across epochs.

**Suggested Next Steps:**

* Add temporal label smoothing or confidence threshold filtering.
* Use EMA of logits to stabilize pseudo-label assignment.
* Penalize label flip rate with a disagreement loss across time.

---

## 512. View-Adaptive Branches Overfit to a Single Augmentation Strategy

**Symptoms:**

* Model performs well on certain augmentations but fails under combined or unseen ones.

**Suggested Next Steps:**

* Train with aug-mix views and a mixed-view contrastive loss.
* Add a view dropout mechanism (randomly remove views per sample).
* Introduce a meta-view regularizer to align representations across all augment classes.

---

## 513. Contrastive Head Inverts Features Relative to Semantic Content

**Symptoms:**

* The contrastive head maps similar samples to opposites (e.g., high cosine sim before head, low after).

**Suggested Next Steps:**

* Add pre- and post-head similarity agreement loss.
* Constrain head weights using spectral norm or orthogonality regularization.
* Use dual-path probes to visualize whether feature direction is preserved.

---

## 514. EMA Model Diverges After Many Epochs Due to Weight Drift

**Symptoms:**

* Momentum encoder begins producing uncorrelated features despite stable online encoder.

**Suggested Next Steps:**

* Apply momentum cap (e.g., upper bound < 0.9995).
* Add EMA branch reset logic every N epochs based on similarity metrics.
* Use dual EMA branches and pick best-aligned features dynamically.

---

## 515. Final Layers Accumulate Activation Saturation (Dead Units)

**Symptoms:**

* Final projection or classifier layers show near-zero activation in many channels.

**Suggested Next Steps:**

* Use ReLU leakage detection and leaky activations (GELU, Swish).
* Apply activation histogram monitoring and sparsity penalties.
* Add unit dropout or reinitialization schedule to refresh stagnant filters.

---

## 516. Representations Encode Batch Statistics Instead of Content

**Symptoms:**

* Latents correlate with batch mean or standard deviation patterns.

**Suggested Next Steps:**

* Train with instance normalization or GroupNorm instead of BatchNorm.
* Add batch decorrelation regularizer across views.
* Test for batch-specific drift using cosine sim by batch index.

---

## 517. Temporal Self-Supervised Models Fail to Reuse Representations Across Segments

**Symptoms:**

* Adjacent time segments generate orthogonal or inconsistent latents.

**Suggested Next Steps:**

* Train with segment reassembly loss or temporal contrastive prediction.
* Use context window encoders that fuse past and future segments.
* Apply cross-segment alignment probes to test drift.

---

## 518. Multiple Heads Collide in Feature Space, Causing Joint Collapse

**Symptoms:**

* Two or more parallel heads align to the same latent dimensions or class boundaries.

**Suggested Next Steps:**

* Add head disentanglement loss (e.g., negative cosine or KL divergence between heads).
* Use orthogonal head initialization and maintain norm separation.
* Freeze one head periodically to break symmetry.

---

## 519. Early Attention Heads Learn Only Positional Identity, Not Content

**Symptoms:**

* Attention weights are position-biased even with randomized content.

**Suggested Next Steps:**

* Train with position dropout or rotary embeddings to remove absolute bias.
* Add token-agnostic attention loss (e.g., uniform head entropy per position).
* Visualize positional weight bias and clip extremal tokens during warmup.

---

## 520. Latent Collapse Happens Only Under Specific Class Imbalance Ratios

**Symptoms:**

* SSL model collapses when class distribution is skewed (e.g., 90/10 or 95/5).

**Suggested Next Steps:**

* Simulate class imbalance and apply adaptive instance weighting.
* Add per-class diversity loss or conditional variance maximization.
* Use online class balancing memory bank to ensure balanced negatives.

---

## 521. Early Layers Lose Discriminative Power During Long SSL Training

**Symptoms:**

* First few encoder blocks produce increasingly uniform or low-norm outputs.

**Suggested Next Steps:**

* Add auxiliary shallow layer probes with variance loss or token contrast.
* Apply layer-wise dropout or periodic skip unfreezing to preserve diversity.
* Freeze later layers temporarily and re-train early ones with shallow supervision.

---

## 522. View-Specific Tokens Cause Inconsistent Global Representation

**Symptoms:**

* Different tokens dominate the pooled output across views (e.g., CLS in one view, average in another).

**Suggested Next Steps:**

* Standardize pooling strategy across branches (e.g., always use mean or learned token).
* Add token-pooling alignment loss across augmentations.
* Visualize token contribution via gradient attribution per view.

---

## 523. Representation Collapses on Unseen Dataset Splits (e.g., val/test)

**Symptoms:**

* Embeddings generalize well on train set but collapse to low-rank or clustered representations on val/test.

**Suggested Next Steps:**

* Add cross-split latent consistency loss or feature variance preservation.
* Regularize with domain-generalization heads during pretraining.
* Track rank and norm drift across splits in latent space.

---

## 524. Time-Based or Frame-Based Contrastive Learning Overfits to Step Size

**Symptoms:**

* Embeddings become tightly coupled to fixed time offset between views.

**Suggested Next Steps:**

* Train with variable temporal offsets or jittered time skips.
* Add offset confusion loss to suppress view-specific offset encodings.
* Penalize latent similarity patterns aligned with sampling stride.

---

## 525. Feature Norm Collapses When Using Lightweight Normalization (e.g., LN, GN)

**Symptoms:**

* Switching from BN to LN/GN causes features to collapse to small-norm latents.

**Suggested Next Steps:**

* Rescale normalized outputs with learnable post-norm scalars.
* Track feature norm histograms by normalization scheme.
* Add feature norm preservation loss across layers.

---

## 526. Multi-Head Attention Layers Learn Redundant Key/Query Projections

**Symptoms:**

* K/Q matrices become nearly identical across heads, causing attention collapse.

**Suggested Next Steps:**

* Track QK projection similarity heatmaps.
* Add diversity-aware dropout to independently regularize heads.

---

## 527. Latent Features Drift Between Source and Target Domains Even After Alignment

**Symptoms:**

* Pretrained representations remain unstable or distorted when transferred to a new domain.

**Suggested Next Steps:**

* Use domain-invariant contrastive alignment (e.g., contrast across domains).
* Add feature-space entropy regularization across domains.
* Re-center and rescale representations using domain-wise normalization heads.

---

## 528. Loss Plateaus But Feature Space Keeps Rotating

**Symptoms:**

* No change in training loss, but latent features shift over epochs (cosine or PCA).

**Suggested Next Steps:**

* Introduce temporal consistency loss across checkpoints.
* Freeze projector and track representation movement per layer.
* Add eigenvector alignment loss on top latent axes over time.

---

## 529. Classifier Finetuning Causes Representation Collapse in Deep Blocks

**Symptoms:**

* Classifier head updates cause encoder’s deep layers to reduce diversity.

**Suggested Next Steps:**

* Train with head detachment warm-up phase (frozen encoder for first N epochs).
* Apply feature similarity tracking per block, and reinitialize collapsed ones.
* Penalize gradient flow imbalance between classifier and encoder.

---

## 530. Cross-Modal Contrastive Training Collapses on Narrow Modalities (e.g., audio only)

**Symptoms:**

* Representations collapse when one modality has low variance or diversity.

**Suggested Next Steps:**

* Balance modality information content via entropy-aware loss reweighting.
* Add modality dropout or shuffling during pretraining.
* Use modality-specific projections and combine only at shared latent stage.

---

## 531. Decoder Entangles Representations with Anchor Tokens or Positional Bias

**Symptoms:**

* Features near the start/end of input consistently dominate decoder outputs.

**Suggested Next Steps:**

* Add positional dropout or jitter before decoding.
* Penalize anchor token over-attention in decoder with entropy loss.
* Use content-only pooling (e.g., CLS-token-free decoding) to reduce positional bias.

---

## 532. Token Pooling Creates Layer-to-Head Representation Misalignment

**Symptoms:**

* Different layers generate high-quality features, but pooling strategies erase this signal.

**Suggested Next Steps:**

* Use multi-layer pooling or attention-weighted fusion across depths.
* Add consistency loss between pooled and unpooled tokens.
* Regularize per-layer token norm variance to balance contributions.

---

## 533. Contrastive Models Collapse When Negative Sampling Becomes Too Easy

**Symptoms:**

* Model quickly distinguishes all negatives, losing training signal.

**Suggested Next Steps:**

* Increase negative difficulty using hard negative mining or semantic negatives.
* Use temperature annealing to keep logits sharp and discriminative.
* Switch to relative contrastive losses (e.g., triplet, N-pair) for smoother learning.

---

## 534. Class-Specific Shortcuts Arise During Long Contrastive Training

**Symptoms:**

* Some classes are learned via trivial artifacts (e.g., texture, border, patch location).

**Suggested Next Steps:**

* Add shortcut suppression loss or texture-randomizing augmentations.
* Penalize attention maps concentrated on non-semantic zones.
* Train with class confusion augmentation — sample-mixed or blurred views.

---

## 535. Feature Norms Misalign Between Branches After BatchNorm Removal

**Symptoms:**

* Removing BN causes projection head outputs to diverge in magnitude across views.

**Suggested Next Steps:**

* Apply explicit feature norm constraint or normalize outputs before loss.
* Use LayerNorm with affine scale to reintroduce normalization without statistics.
* Track inter-view norm divergence and rescale dynamically.

---

## 536. Early Stop-Free Training Overfits Latent Covariance Geometry

**Symptoms:**

* Long training leads to tightly shaped (e.g., low-rank ellipse) feature distributions.

**Suggested Next Steps:**

* Use early stopping or checkpoint entropy tracking.
* Introduce latent space noise or dropout after N epochs.
* Penalize eigenvalue variance in covariance matrix (flat spectrum encouragement).

---

## 537. Layer Skipping During Pretraining Results in Frozen Mid-Level Features

**Symptoms:**

* Skipped layers (e.g., residuals or unused blocks) produce no gradient and no activation.

**Suggested Next Steps:**

* Track layer activity via activation norms or gradient stats.
* Add periodic forced routing through skipped paths during training.
* Re-initialize or boost unused blocks using auxiliary shallow loss.

---

## 538. Multi-View Alignment Loss Prevents Local Feature Specialization

**Symptoms:**

* Token-level features collapse to global direction after strong view matching loss.

**Suggested Next Steps:**

* Add local contrastive or patch-to-patch alignment head.
* Reduce global alignment weight when token similarity exceeds threshold.
* Train with masked region alignment, leaving some tokens free to diverge.

---

## 539. Representations Encode Label Distribution Statistics Instead of Class Semantics

**Symptoms:**

* Rare classes share features; common ones dominate vector space.

**Suggested Next Steps:**

* Apply label distribution decorrelation loss or sample reweighting.
* Penalize centroid drift caused by class imbalance.
* Introduce label-conditioned contrastive noise to dilute distributional artifacts.

---

## 540. Mixed-Resolution Training Leads to Feature Collapse in Low-Res Batches

**Symptoms:**

* Model collapses only when input resolution is small or inconsistently sized.

**Suggested Next Steps:**

* Train with resolution-aware projection heads or per-resolution losses.
* Normalize patch size impact using token scaling or adaptive stride.
* Add res-scale contrastive alignment loss between large and small inputs.

---

## 541. Momentum Decay Rate Drifts Too Slowly, Preventing Representation Stability

**Symptoms:**

* Momentum encoder produces unstable or lagging features late in training.

**Suggested Next Steps:**

* Use cosine or sigmoid EMA schedule: slow start, faster decay.
* Freeze momentum encoder updates after convergence checkpoint.
* Add momentum-target divergence penalty based on latent similarity.

---

## 542. Hierarchical Attention Models Collapse to Shallow Pathways

**Symptoms:**

* Attention concentrates only on the first few tokens or shallow branches, ignoring deeper hierarchy.

**Suggested Next Steps:**

* Penalize depth-skewed attention scores using entropy or KL to uniform.
* Use layer-wise supervision heads to enforce deep feature utility.
* Train with hierarchy dropout — disable upper/lower layers randomly per batch.

---

## 543. Local Token Contrastiveness Degrades Global Feature Structure

**Symptoms:**

* Strong token-level contrastive loss causes pooled/global representation drift.

**Suggested Next Steps:**

* Add global-to-local alignment loss to preserve semantic direction.
* Reduce patch contrastive weight over time or based on entropy thresholds.
* Use region-based pooling to retain mid-level context in local heads.

---

## 544. Latent Feature Collapse Occurs When Loss Scaling is Too Aggressive (e.g., FP16)

**Symptoms:**

* Features flatten or explode when using large dynamic loss scales in mixed-precision training.

**Suggested Next Steps:**

* Clamp or anneal loss scale factor based on gradient norms.
* Monitor per-feature gradient variance to detect underflow or overflow.
* Switch to adaptive loss scaling libraries (e.g., Apex, GradScaler) with stabilization checks.

---

## 545. View-Aware Tokens Collapse to a Shared Vector in All Views

**Symptoms:**

* Tokens trained under multiple augmentations all converge to a universal embedding.

**Suggested Next Steps:**

* Add view decorrelation loss or token-specific dropout masking.
* Use view embedding conditioning on projection heads to preserve identity.
* Penalize cross-view token cosine similarity beyond alignment threshold.

---

## 546. Pretraining with Excessive Regularization Flattens Useful Directions

**Symptoms:**

* Strong dropout, VC loss, or norm penalties remove task-relevant variance.

**Suggested Next Steps:**

* Use scheduled regularization: high early, low late.
* Monitor per-class variance curves — stop regularization when flattening begins.
* Add task-conditional feature gating (e.g., modulation by supervised signal during late epochs).

---

## 547. Early Training Instability Causes Disjoint Latent Geometry (No Consistent Basis)

**Symptoms:**

* Embedding directions change completely from epoch to epoch despite low loss.

**Suggested Next Steps:**

* Warm start with alignment anchor loss over first few epochs.
* Use procrustes regularization to preserve latent manifold continuity.
* Visualize basis drift using principal direction cosine similarity.

---

## 548. Auxiliary Decoder Loss Dominates Multi-Task SSL Objective

**Symptoms:**

* Strong decoder head causes encoder to optimize reconstruction-only, ignoring contrast or VC terms.

**Suggested Next Steps:**

* Delay decoder loss activation (e.g., turn on after N epochs).
* Apply decoder detach or zero-grad masking to reduce feedback.
* Use joint head balancing based on loss variance or learning progress.

---

## 549. Token Attention Becomes Uniform Across All Tokens (Attention Saturation)

**Symptoms:**

* Attention maps are flat; no selectivity across tokens.

**Suggested Next Steps:**

* Add token-wise attention entropy penalty to increase sharpness.
* Train with attention competition loss (e.g., token sparsity).
* Introduce content-based token masking to break symmetry.

---

## 550. Inter-View Representations Share Structure But Differ in Scale/Norm

**Symptoms:**

* Cosine sim is high across views, but L2 distance or norm is inconsistent.

**Suggested Next Steps:**

* Normalize all latent vectors before projection with F.normalize(z).
* Add norm-matching loss across views alongside similarity loss.
* Use view-agnostic batch normalization or rescale targets explicitly.

---

## 551. Representations Align Across Views But Lose Compositionality

**Symptoms:**

* Features match across augmentations but fail to generalize to new compositions (e.g., multiple objects, layered sounds).

**Suggested Next Steps:**

* Add compositional contrastive loss using pairs of object/object or phrase/phrase.
* Train with view mixing tasks (e.g., audio layering, CutMix).
* Use feature disentanglement losses to force substructure preservation.

---

## 552. Tokens Learn to Co-Adaptive Collapse Within a Single View

**Symptoms:**

* Tokens within the same sample encode redundant or correlated features.

**Suggested Next Steps:**

* Apply token-wise VC regularization (patch independence).
* Use attention head entropy regularization to enforce diversity.
* Add co-token decorrelation constraint in embedding space.

---

## 553. Representations Are Consistent Across Augmentations But Fragile to Time

**Symptoms:**

* SSL features hold under spatial views but change significantly across training epochs.

**Suggested Next Steps:**

* Track feature consistency over checkpoints using cosine similarity or kernel alignment.
* Use feature EMA loss or temporal consistency loss.
* Add check-in anchor loss every N epochs to preserve latent space structure.

---

## 554. Downstream Heads Forget Pretraining Alignment in Just a Few Batches

**Symptoms:**

* Finetuning a head on a pretrained encoder causes immediate drift or collapse.

**Suggested Next Steps:**

* Use warmup head training with frozen encoder for initial epochs.
* Add head-to-latent alignment loss (e.g., cosine sim with previous SSL features).
* Apply low learning rate or L2 penalty on head weights relative to identity.

---

## 555. Attention Maps Oscillate Between Tokens Without Converging

**Symptoms:**

* Token attention heads jump between unrelated anchors every few steps.

**Suggested Next Steps:**

* Apply attention map temporal smoothing (EMA attention or prediction-head averaging).
* Train with token dropout + identity-preserving regularization.
* Introduce attention consistency loss across epochs or views.

---

## 556. Contrastive Learning Over-Aligns Near Duplicates But Ignores Subtle In-Class Variation

**Symptoms:**

* Intra-class hard positives are aligned perfectly, but other variations are ignored.

**Suggested Next Steps:**

* Introduce view-aware angular margin loss to prevent total collapse.
* Sample intra-class negatives explicitly with metadata or pseudo-labels.
* Penalize over-alignment using feature variance or rank loss among positives.

---

## 557. Temporal Positional Encoding Drift Appears After Finetuning

**Symptoms:**

* Positional embeddings that work during pretraining misalign under supervised adaptation.

**Suggested Next Steps:**

* Replace absolute positions with rotary or offset-normalized encodings.
* Freeze temporal embeddings for first few downstream epochs.
* Evaluate position–class correlation drift after finetuning.

---

## 558. Pretraining Collapse is Triggered by Shortcut-Compatible View Combinations

**Symptoms:**

* Collapse only happens when certain combinations of augmentations are applied (e.g., grayscale + center crop).

**Suggested Next Steps:**

* Audit view combinations and add shortcut-aware sampling constraints.
* Penalize trivial similarity maps with low activation entropy.
* Add a view adversary network to detect and avoid shortcut reliance.

---

## 559. Head Overfitting Leads to Rotated Feature Spaces With Same Accuracy

**Symptoms:**

* Model achieves good classification accuracy, but embeddings are inconsistent or rotated unpredictably.

**Suggested Next Steps:**

* Track rotation-invariant metrics (e.g., kernel alignment, Procrustes distance).
* Apply latent alignment constraints across epochs.
* Penalize feature manifold drift even when logits remain stable.

---

## 560. Representations Show Modality-Specific Saturation in Multimodal Models

**Symptoms:**

* Audio-only or text-only inputs saturate latent space differently than combined views.

**Suggested Next Steps:**

* Normalize each modality branch with shared latent targets.
* Train with cross-modal reconstruction or prediction heads.
* Apply modality-wise latent variance equalization loss.

---

## 561. High-Frequency Features Are Suppressed During Multi-View Pretraining

**Symptoms:**

* Fine details (e.g., edges, pitch modulations) disappear in late-stage representations.

**Suggested Next Steps:**

* Add frequency-aware loss using high-pass filtered views.
* Train with multi-band contrastive heads (e.g., low + high frequency separately).
* Apply reconstruction of frequency residuals as auxiliary target.

---

## 562. View-Specific Representations Are Forgotten During Later Epochs

**Symptoms:**

* Early augmentations lose alignment or contribution over time.

**Suggested Next Steps:**

* Log view-specific alignment score (e.g., cosine sim across epochs).
* Freeze augment pairings periodically and enforce historical view anchors.
* Reintroduce early views with curriculum view replay toward the end.

---

## 563. Over-Regularization Suppresses Class Boundary Sharpness in Semi-Supervised Learning

**Symptoms:**

* Features become smooth but fail to separate noisy or overlapping classes.

**Suggested Next Steps:**

* Apply margin-aware contrastive loss to sharpen ambiguous class boundaries.
* Temporarily reduce variance or VC loss during late supervised training.
* Introduce label uncertainty margin head to adjust boundary strength dynamically.

---

## 564. Hybrid SSL Architectures Leak Features Across Incompatible Heads

**Symptoms:**

* One head’s optimization degrades another’s task-specific representation.

**Suggested Next Steps:**

* Use gradient isolation (e.g., detach() or stop\_grad) between heads.
* Apply feature routing layers to isolate feature paths.
* Monitor head-specific feature norm and drift to detect early collision.

---

## 565. Representation Collapse Is Driven by Global Pooling Only

**Symptoms:**

* Model appears fine until pooling; collapse appears only in pooled features.

**Suggested Next Steps:**

* Apply multi-token pooling strategies (e.g., attention, NetVLAD).
* Add contrastive loss directly on unpooled token features.
* Penalize pooled output flatness (e.g., low norm or similarity saturation).

---

## 566. Probe Head Accuracy Plateaus While Representations Still Evolve

**Symptoms:**

* Linear probes give misleading plateau, even though encoder keeps changing.

**Suggested Next Steps:**

* Track probe-to-feature drift (cosine sim or alignment vs. epoch).
* Use nonlinear probe with frozen encoder to detect deeper structure.
* Retrain probe periodically or average across epochs.

---

## 567. Fine-Tuning on Downstream Task Causes Shallow Layers to Overwrite General Features

**Symptoms:**

* Early layers adapt to task-specific patterns and lose generality.

**Suggested Next Steps:**

* Apply shallow layer freezing or gradual unfreezing strategy.
* Add shallow feature preservation penalty (e.g., cosine sim with pretrained checkpoint).
* Log activation pattern overlap with pretraining stages.

---

## 568. Multi-Modal Representations Collapse When One Modality Is Dominant

**Symptoms:**

* Joint representations reflect only one modality (e.g., image > text).

**Suggested Next Steps:**

* Use modality-balancing contrastive loss or modality dropout.
* Add cross-modal decoder tasks to force bidirectional utility.
* Track modality-specific latent centroids and their drift.

---

## 569. Positional Encoding Interferes with Semantic Structure Under Long Inputs

**Symptoms:**

* Sequence-level representations degrade as input length increases.

**Suggested Next Steps:**

* Replace with rotary, relative, or learned sinusoidal encodings.
* Train with variable-length segments and apply length-aware normalization.
* Use position-invariant contrastive tasks as sanity checks.

---

## 570. SSL Representations Collapse When Pretraining on Repetitive or Redundant Datasets

**Symptoms:**

* Representations saturate quickly with low diversity datasets.

**Suggested Next Steps:**

* Increase augmentation diversity or synthetic noise injection.
* Add instance-aware contrastive loss to emphasize rare patterns.
* Penalize representation re-use across similar samples with diversity loss.

---

## 571. Semantic Meaning Shifts Across Epochs Without Accuracy Loss

**Symptoms:**

* Features retain classification performance but their internal structure changes drastically.

**Suggestions:**

* Track semantic direction alignment using CKA or Procrustes distance.
* Add semantic drift penalty between current and checkpointed latents.
* Use slow-moving centroids or anchor prototypes across epochs.

---

## 572. Model Relies on a Small Subset of Tokens to Encode All Information

**Symptoms:**

* Most tokens contribute negligibly to output or classification.

**Suggestions:**

* Introduce token-wise diversity loss or patch dropout.
* Train with learnable token masks to dynamically suppress dominant ones.
* Visualize token contribution heatmaps and track entropy across heads.

---

## 573. Frozen Layers Fail to Reactivate Even After Unfreezing

**Symptoms:**

* Once frozen, layers remain unchanged despite resumed training.

**Suggestions:**

* Apply activation boosting (e.g., skip connections or layer rescaling).
* Use higher LR or gradient amplification on unfrozen layers.
* Inject intermediate supervision to force shallow feature updates.

---

## 574. Contrastive Representations Saturate and Resist Further Optimization

**Symptoms:**

* Features converge quickly, then plateau in rank and diversity despite ongoing training.

**Suggestions:**

* Add orthogonal regularization between positive pairs (not just similarity).
* Use noise-injected contrastive views to increase difficulty.
* Apply feature decorrelation loss to refresh latent axes.

---

## 575. Projections Become Invariant to Sample Identity (High Uniformity)

**Symptoms:**

* All samples collapse to the same point in latent space.

**Suggestions:**

* Add instance separation loss (e.g., nearest-neighbor push-away).
* Increase temperature in softmax-based objectives to sharpen pairwise contrast.
* Introduce sample-conditioned augmentations (e.g., instance-specific color jitter).

---

## 576. Representation Collapse Happens Only During Joint Multi-Task Training

**Symptoms:**

* Individual tasks are stable; collapse occurs only when trained together.

**Suggestions:**

* Isolate each task and monitor gradient conflict via cosine angle.
* Use PCGrad or task-weighted loss scaling.
* Train with per-head feature adapters and merge only at global stage.

---

## 577. Positional Encoding Causes Collapsed Feature Space at Long Sequence Lengths

**Symptoms:**

* Token positions near sequence end collapse to indistinguishable vectors.

**Suggestions:**

* Switch to relative position encodings or learnable offset encodings.
* Train with reversed and shuffled token positions to desaturate learned bias.
* Monitor per-position feature entropy for collapse indicators.

---

## 578. View-Specific Features Become Non-Linearly Entangled Across Branches

**Symptoms:**

* Different augmentations produce features that align globally but differ in semantic meaning.

**Suggestions:**

* Add semantic probing loss to detect entanglement drift.
* Use predictive consistency loss at class or concept level, not just vector similarity.
* Introduce semantic-aware alignment heads (e.g., attribute-level contrast).

---

## 579. Encoder Activations Collapse Even With Well-Spaced Projections

**Symptoms:**

* Projections look good (diverse, aligned), but encoder output itself is flat or low-rank.

**Suggestions:**

* Monitor encoder-only covariance rank independent of head.
* Add intermediate layer VC losses (not just final head).
* Train with layer-wise contrastive probes.

---

## 580. Model Creates Fragmented Representations Across Tasks or Batches

**Symptoms:**

* Latent space splits into disconnected manifolds by task or batch ID.

**Suggestions:**

* Apply manifold stitching loss (e.g., global prototype alignment across tasks).
* Add batch-normalization smoothing or per-task feature norm equalization.
* Penalize cluster-to-cluster alignment distance across heads or tasks.

---

## 581. Encoder Uses Redundant Paths Instead of Learning Semantics

**Symptoms:**

* Features are constructed by shortcut paths (e.g., early skip connections) that avoid learning meaningful patterns.

**Suggestions:**

* Prune redundant skip connections during pretraining.
* Add path dropout to force reliance on deeper features.
* Monitor activation ratios per path to find underused layers.

---

## 582. Decoder Learns to Reconstruct Its Own Features Rather Than From Encoder

**Symptoms:**

* Decoder becomes self-sufficient, ignoring encoder representation.

**Suggestions:**

* Apply stop-gradient from decoder to encoder input.
* Add target perturbation to prevent memorized reconstructions.
* Track decoder output similarity to raw input vs. encoder features.

---

## 583. Latent Anchors Drift to Low-Density Regions, Leading to Unused Prototypes

**Symptoms:**

* Certain cluster centers or prototypes attract no samples over time.

**Suggestions:**

* Periodically reinitialize dormant prototypes.
* Use EMA-based updates to shift anchors toward active regions.
* Add minimum attraction loss: all prototypes must serve a minimum number of samples.

---

## 584. Model Forgets Token Spatial Location During Repeated Augmentations

**Symptoms:**

* Tokens become positionally invariant even when location matters (e.g., segmentation).

**Suggestions:**

* Use location-aware contrastive loss or positional embeddings in latent space.
* Penalize positional entropy collapse using attention heatmaps.
* Add token patch prediction task as an auxiliary head.

---

## 585. Positive Pair Representations Align but Remain Separable from Class Clusters

**Symptoms:**

* Paired views are close, but their latent structure doesn’t match class distributions.

**Suggestions:**

* Add semantic alignment loss across paired and class-based prototypes.
* Use triangular alignment objectives: view–view–class center.
* Introduce label-aware view anchors to fuse instance and class identity.

---

## 586. Multi-Loss Training Creates Competing Feature Compression Patterns

**Symptoms:**

* One loss prefers low-rank embedding; another promotes spread features.

**Suggestions:**

* Monitor feature rank per loss head using PCA or condition number.
* Apply loss-specific bottlenecks to prevent cross-interference.
* Alternate losses per step or batch (cyclical schedule) to decouple objectives.

---

## 587. Representations Become Dominated by Early Training Signal

**Symptoms:**

* Features become stuck near early prototypes or shortcut tokens.

**Suggestions:**

* Add time-weighted contrastive loss to prioritize recent learning.
* Use representation anchoring refresh (recompute cluster centers over time).
* Introduce gradient forgetting penalty to counter inertia.

---

## 588. Alignment Loss Flattens Class-Conditional Boundaries

**Symptoms:**

* Intra-class separation is preserved, but between-class distinctions are blurred.

**Suggestions:**

* Apply class-separating push loss alongside alignment (e.g., cosine margin).
* Use class-conditional contrastive views or explicit cross-class negatives.
* Track inter-class centroid separation and apply variance scaling loss.

---

## 589. Representations Collapse Differently by Augmentation Type

**Symptoms:**

* Different augment classes (e.g., crop vs. blur) cause different latent saturation modes.

**Suggestions:**

* Add per-augmentation regularization curves (VC, entropy, rank).
* Train with augmentation-aware projection heads.
* Penalize augmentation-wise covariance matrix collapse.

---

## 590. Token-Level Representations Vary Too Much Across Epochs (No Convergence)

**Symptoms:**

* Token features oscillate significantly even after global loss stabilizes.

**Suggestions:**

* Add token-level EMA loss to stabilize patch representations.
* Track token cosine similarity over checkpoints for convergence monitoring.
* Freeze token positional embeddings mid-training to lock structure.

---

## 591. Representations Become Brittle to Minor Input Changes Late in Training

**Symptoms:**

* Small input perturbations cause large latent or prediction shifts despite stable training loss.

**Suggestions:**

* Add robustness-aware regularization (e.g., latent smoothness or Jacobian norm penalty).
* Train with input mixup or noise injection to enforce continuity.
* Track feature sensitivity metrics like L2 or cosine delta under noise.

---

## 592. Shift in Input Distribution Causes Feature Collapse Only in Target Subset

**Symptoms:**

* Features collapse only on new domain or rare classes after distribution shift.

**Suggestions:**

* Add domain-adaptive normalization (e.g., AdaBN, domain-specific LN).
* Train with entropy balancing loss to equalize latent variance across domains.
* Monitor per-class or per-domain latent rank during training.

---

## 593. Shared Head Across Tasks Causes Representational Cross-Talk

**Symptoms:**

* Head parameters leak across tasks and degrade task-specific alignment.

**Suggestions:**

* Use task-specific projection heads with shared encoder.
* Add head disentanglement loss to prevent gradient correlation.
* Alternate tasks per epoch or optimize with task-gradient projection.

---

## 594. Positional Token Representations Flatten to Uniform Across Input

**Symptoms:**

* Tokens forget their location, collapsing to same representation regardless of position.

**Suggestions:**

* Reinforce position-awareness via relative encoding or contrastive position prediction.
* Penalize token-position cosine similarity in later layers.
* Add auxiliary task to reconstruct token position.

---

## 595. Optimizer Hyperparameters Cause Gradual Semantic Drift

**Symptoms:**

* Changing optimizer (e.g., SGD to AdamW) leads to semantic shift despite similar accuracy.

**Suggestions:**

* Align momentum schedules and LR scaling when switching optimizers.
* Freeze encoder for N steps post-switch to stabilize downstream drift.
* Monitor semantic drift with class prototype cosine sim.

---

## 596. Semantic Collapse Appears Only in Representations of “Easy” Classes

**Symptoms:**

* High-confidence or frequently occurring classes lose intra-class diversity.

**Suggestions:**

* Apply variance preservation loss only on overconfident predictions.
* Downscale similarity-based loss for confident class samples.
* Introduce difficulty-aware loss modulation.

---

## 597. Mixed-Token Models (e.g., patch + CLS) Collapse to CLS-Dominated Latents

**Symptoms:**

* CLS token absorbs all information, patch tokens become irrelevant.

**Suggestions:**

* Add token dropout targeting CLS to force distributed representation.
* Train with auxiliary heads on patch tokens (e.g., patch classification).
* Penalize CLS-token attention dominance in early layers.

---

## 598. Representation Evolves Toward Class Centroids Too Early

**Symptoms:**

* Pretraining forces class-like structure prematurely, before instance-level diversity is learned.

**Suggestions:**

* Start with instance-level contrastive loss, introduce prototypes later.
* Delay class-wise regularization until diversity stabilizes.
* Apply prototype sharpening gradually, tied to feature rank schedule.

---

## 599. LayerNorm Re-centers Feature Space and Obscures Collapse

**Symptoms:**

* Feature vectors appear stable, but collapse is hidden due to normalization masking drift.

**Suggestions:**

* Log pre-LayerNorm representations and monitor their norm/spread.
* Add layer-norm-free latent probes for raw signal health.
* Use variance-aware loss before normalization layers.

---

## 600. EMA Targets Collapse When Paired With Rapid Augmentation Dynamics

**Symptoms:**

* Target encoder cannot keep up with aggressive view augmentations.

**Suggestions:**

* Use adaptive EMA decay based on cosine sim between branches.
* Stabilize augmentations via curriculum scheduling.
* Clip or smooth latent updates during early aggressive augment phases.

---




More is in progress! Stay tuned!


